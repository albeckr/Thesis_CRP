{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f838d2e",
   "metadata": {},
   "source": [
    "# MultiAgentEnvironment: simple Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a170d6",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful\n"
     ]
    }
   ],
   "source": [
    "#%autosave 30\n",
    "import glob\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms.shortest_paths.generic import shortest_path_length\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "#from ray.tune.logger import pretty_print\n",
    "from ray.rllib.policy.policy import Policy, PolicySpec\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.fcnet import FullyConnectedNetwork\n",
    "\n",
    "#from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.env import MultiAgentEnv\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "\n",
    "from gym.spaces import Discrete, Box, Tuple, MultiDiscrete, Dict, MultiBinary\n",
    "from ray.rllib.utils.spaces.repeated import Repeated\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf1, tf, tfv = try_import_tf()  # prefered TF import for Ray.io\n",
    "\n",
    "from threading import Thread, Event\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10fdf817",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "######## Utility Classes ########\n",
    "class BoolTimer(Thread):\n",
    "    \"\"\"A boolean value that toggles after a specified number of seconds:\n",
    "    \n",
    "    Example:\n",
    "        bt = BoolTimer(30.0, False)\n",
    "        bt.start()\n",
    "\n",
    "    Used in the Centrality Baseline to limit the computation time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, interval, initial_state=True):\n",
    "        Thread.__init__(self)\n",
    "        self.interval = interval\n",
    "        self.state = initial_state\n",
    "        self.finished = Event()\n",
    "\n",
    "    def __bool__(self):\n",
    "        return bool(self.state)\n",
    "\n",
    "    def run(self):\n",
    "        self.finished.wait(self.interval)\n",
    "        if not self.finished.is_set():\n",
    "            self.state = not self.state\n",
    "        self.finished.set()\n",
    "\n",
    "\n",
    "######## Static helper functions ########\n",
    "def shuffle_actions(action_dict, check_space = False):\n",
    "    \"\"\"\n",
    "        Used to shuffle the action dict to ensure that agents with a lower id are not always preferred\n",
    "        when picking up parcels over other agents that chose the same action.\n",
    "        For debugging: Can also be used to check if all actions are in the action_space.\n",
    "    \"\"\"\n",
    "    keys = list(action_dict)\n",
    "    random.shuffle(keys)\n",
    "    shuffled = {}\n",
    "\n",
    "    for agent in keys:\n",
    "        if check_space:  #assert actions are in action space -> disable for later trainig, extra check while development\n",
    "            assert self.action_space.contains(action_dict[agent]),f\"Action {action_dict[agent]} taken by agent {agent} not in action space\"\n",
    "        shuffled[agent] = action_dict[agent]\n",
    "\n",
    "    return shuffled\n",
    "\n",
    "\n",
    "\n",
    "def load_graph(data):\n",
    "    \"\"\"Loads topology (map) from json file into a networkX Graph and returns the graph\"\"\"\n",
    "\n",
    "    nodes = data[\"nodes\"]\n",
    "    edges = data[\"edges\"]\n",
    "\n",
    "    g = nx.DiGraph()  # directed graph\n",
    "    g.add_nodes_from(nodes)\n",
    "    g.edges(data=True)\n",
    "\n",
    "   \n",
    "    for node in nodes:  # add attribute values\n",
    "        g.nodes[node][\"id\"] = nodes[node][\"id\"]\n",
    "        g.nodes[node][\"type\"] = nodes[node][\"type\"]\n",
    "\n",
    "    for edge in edges:  # add edges with attributes\n",
    "        f = edges[edge][\"from\"]\n",
    "        t = edges[edge][\"to\"]\n",
    "        \n",
    "        weight_road, weight_air, _type = sys.maxsize, sys.maxsize, None\n",
    "        \n",
    "        if edges[edge][\"road\"] >= 0:\n",
    "            weight_road = edges[edge][\"road\"]\n",
    "            _type = 'road'\n",
    "        if edges[edge][\"air\"] >= 0:\n",
    "            weight_air = edges[edge][\"air\"]\n",
    "            _type = 'both' if _type == 'road' else 'air'\n",
    "\n",
    "        weight = min(weight_road, weight_air)  # needed for optimality baseline\n",
    "        \n",
    "        g.add_edge(f, t, type=_type, road= weight_road, air=weight_air, weight=weight)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663b334",
   "metadata": {},
   "source": [
    "## Environment Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f835bd0",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Map definition\n",
    "topology = {\n",
    "    'nodes': {\n",
    "            0: {'id': 1, 'type': 'parking'},\n",
    "            1: {'id': 1, 'type': 'parking'},\n",
    "            2: {'id': 2, 'type': 'parking'},\n",
    "            3: {'id': 3, 'type': 'parking'},\n",
    "            4: {'id': 4, 'type': 'parking'},\n",
    "            5: {'id': 5, 'type': 'air'},\n",
    "            6: {'id': 6, 'type': 'parking'},\n",
    "            7: {'id': 7, 'type': 'air'},\n",
    "            8: {'id': 8, 'type': 'parking'},\n",
    "            9: {'id': 9, 'type': 'parking'},\n",
    "            10: {'id': 10, 'type': 'parking'},\n",
    "            11: {'id': 11, 'type': 'air'}\n",
    "    },\n",
    "    'edges': {\n",
    "    ## Outer Ring    --> Road much (!) faster than drones\n",
    "    \"e01\":{\"from\": 0,\"to\": 1,\"road\": 10, \"air\": 40},\n",
    "    \"e02\":{\"from\": 1,\"to\": 0,\"road\": 10, \"air\": 40},\n",
    "    \"e03\":{\"from\": 1,\"to\": 4,\"road\": 8, \"air\": 6},        \n",
    "    \"e04\":{\"from\": 4,\"to\": 1,\"road\": 8, \"air\": 6},\n",
    "    \"e03\":{\"from\": 4,\"to\": 2,\"road\": 8, \"air\": 6},        \n",
    "    \"e04\":{\"from\": 2,\"to\": 4,\"road\": 8, \"air\": 6},\n",
    "    \"e05\":{\"from\": 2,\"to\": 3,\"road\": 10, \"air\": 30},\n",
    "    \"e06\":{\"from\": 3,\"to\": 2,\"road\": 10, \"air\": 30},\n",
    "    \"e07\":{\"from\": 3,\"to\": 0,\"road\": 10, \"air\": 40},\n",
    "    \"e08\":{\"from\": 0,\"to\": 3,\"road\": 8, \"air\": 35}, \n",
    "    ## Inner Nodes  --> Reinfahren langsamer als rausfahren\n",
    "    ##              --> \n",
    "    \"e11\":{\"from\": 4,\"to\": 6,\"road\": 2, \"air\": 5},        \n",
    "    \"e12\":{\"from\": 6,\"to\": 4,\"road\": 2, \"air\": 5},\n",
    "    \"e13\":{\"from\": 0,\"to\": 6,\"road\": 6, \"air\": -1},\n",
    "    \"e14\":{\"from\": 6,\"to\": 0,\"road\": 6, \"air\": -1},\n",
    "    \"e15\":{\"from\": 3,\"to\": 6,\"road\": 6, \"air\": -1},\n",
    "    \"e16\":{\"from\": 6,\"to\": 3,\"road\": 6, \"air\": -1},   \n",
    "    ## Outliers   --> Distance about equal if both exist!\n",
    "    \"e17\":{\"from\": 4,\"to\": 5,\"road\": 2, \"air\": 2},        \n",
    "    \"e18\":{\"from\": 5,\"to\": 4,\"road\": 2, \"air\": 2},\n",
    "    \"e19\":{\"from\": 6,\"to\": 7,\"road\": -1, \"air\": 2},\n",
    "    \"e20\":{\"from\": 7,\"to\": 6,\"road\": -1, \"air\": 2},\n",
    "    \"e21\":{\"from\": 3,\"to\": 11,\"road\": -1, \"air\": 3},\n",
    "    \"e22\":{\"from\": 11,\"to\": 3,\"road\": -1, \"air\": 3},     \n",
    "    ## Upper left square  --> Hier besser mit den Drohnen agieren, Falls Sie da sind!!\n",
    "    \"e23\":{\"from\": 0,\"to\": 8,\"road\": -1, \"air\": 4},        \n",
    "    \"e24\":{\"from\": 8,\"to\": 0,\"road\": -1, \"air\": 4},\n",
    "    \"e25\":{\"from\": 8,\"to\": 9,\"road\": 3, \"air\": 3},\n",
    "    \"e26\":{\"from\": 9,\"to\": 8,\"road\": 3, \"air\": 3},\n",
    "    \"e27\":{\"from\": 9,\"to\": 10,\"road\": 3, \"air\": 3},\n",
    "    \"e28\":{\"from\": 10,\"to\": 9,\"road\": 3, \"air\": 3},\n",
    "    \"e29\":{\"from\": 0,\"to\": 10,\"road\": 3, \"air\": 3},\n",
    "    \"e30\":{\"from\": 10,\"to\": 0,\"road\": 3, \"air\": 3}  \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6ccbe98",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class Map_Environment(MultiAgentEnv):\n",
    "    \n",
    "    def __init__(self, env_config: dict = {}):\n",
    "        # ensure config file includes all necessary settings\n",
    "        assert 'NUMBER_STEPS_PER_EPISODE' in env_config\n",
    "        assert 'NUMBER_OF_DRONES' in env_config\n",
    "        assert 'NUMBER_OF_CARS' in env_config\n",
    "        assert 'INIT_NUMBER_OF_PARCELS' in env_config\n",
    "        assert 'TOPOLOGY' in env_config\n",
    "        assert 'MAX_NUMBER_OF_PARCELS' in env_config\n",
    "        assert 'THRESHOLD_ADD_NEW_PARCEL' in env_config\n",
    "        assert 'BASELINE_FLAG' in env_config\n",
    "        assert 'BASELINE_TIME_CONSTRAINT' in env_config\n",
    "        assert 'BASELINE_OPT_CONSTANT' in env_config\n",
    "        assert 'CHARGING_STATION_NODES' in env_config\n",
    "        assert 'REWARDS' in env_config\n",
    "        \n",
    "        self.graph = load_graph(topology)\n",
    "        \n",
    "\n",
    "        # Map config\n",
    "        self.NUMBER_OF_DRONES = env_config['NUMBER_OF_DRONES']\n",
    "        self.NUMBER_OF_CARS = env_config['NUMBER_OF_CARS']\n",
    "        self.NUMBER_OF_EDGES = self.graph.number_of_edges()\n",
    "        self.NUMBER_OF_NODES = self.graph.number_of_nodes()\n",
    "        self.CHARGING_STATION_NODES = env_config['CHARGING_STATION_NODES']           \n",
    "        self.MAX_BATTERY_POWER = env_config['MAX_BATTERY_POWER']\n",
    "       \n",
    "\n",
    "        # Simulation config\n",
    "        self.NUMBER_STEPS_PER_EPISODE = env_config['NUMBER_STEPS_PER_EPISODE']\n",
    "        self.INIT_NUMBER_OF_PARCELS = env_config['INIT_NUMBER_OF_PARCELS']\n",
    "        self.RANDOM_SEED = env_config.get('RANDOM_SEED', None)\n",
    "        self.MAX_NUMBER_OF_PARCELS = env_config['MAX_NUMBER_OF_PARCELS']\n",
    "        self.THRESHOLD_ADD_NEW_PARCEL = env_config['THRESHOLD_ADD_NEW_PARCEL']\n",
    "        self.BASELINE_FLAG = env_config['BASELINE_FLAG']\n",
    "        self.BASELINE_TIME_CONSTRAINT = env_config['BASELINE_TIME_CONSTRAINT']\n",
    "        self.BASELINE_OPT_CONSTANT = env_config['BASELINE_OPT_CONSTANT']\n",
    " \n",
    "        self.DEBUG_LOG = env_config.get('DEBUG_LOGS', False)\n",
    "        # Some Sanity Checks on the settings\n",
    "        if self.DEBUG_LOG: assert self.MAX_NUMBER_OF_PARCELS >= self.INIT_NUMBER_OF_PARCELS, \"Number of initial parcels exceeds max parcel limit\"\n",
    "\n",
    "        #Reward constants\n",
    "        self.STEP_PENALTY = env_config['REWARDS']['STEP_PENALTY']\n",
    "        self.PARCEL_DELIVERED = env_config['REWARDS']['PARCEL_DELIVERED']\n",
    "        # compute other rewards\n",
    "        self.BATTERY_DIED = self.STEP_PENALTY * self.NUMBER_STEPS_PER_EPISODE\n",
    "        self.BATTERY_DIED_WITH_PARCEL = self.BATTERY_DIED * 2\n",
    "        #self.DELIVERY_CONTRIBUTION: depends on active agents --> computed when given in prepare_global_reward()\n",
    "        #self.ALL_DELIVERED_CONTRIB: depends on active agents --> computed when given in prepare_global_reward(episode_success=True)\n",
    "\n",
    "        # Computed constants\n",
    "        self.NUMBER_OF_AGENTS = self.NUMBER_OF_DRONES + self.NUMBER_OF_CARS\n",
    "        self.PARCEL_STATE_DELIVERED = self.NUMBER_OF_AGENTS + self.NUMBER_OF_NODES\n",
    "        self.NUMBER_OF_ACTIONS = 1 + self.NUMBER_OF_NODES + 1 + self.MAX_NUMBER_OF_PARCELS\n",
    "        self.ACTION_DROPOFF = 1 + self.NUMBER_OF_NODES     # First Action NOOP is 0\n",
    "    \n",
    "        # seed RNGs\n",
    "        self.seed(self.RANDOM_SEED) \n",
    "        \n",
    "        self.state = None  \n",
    "        self.current_step = None\n",
    "        self.blocked_agents = None\n",
    "        self.parcels_delivered = None\n",
    "        self.done_agents = None\n",
    "        self.all_done = None  \n",
    "        self.allowed_actions = None\n",
    "        \n",
    "        # baseline related \n",
    "        self.baseline_missions = None\n",
    "        self.agents_base = None\n",
    "        self.o_employed = None\n",
    "        \n",
    "        # metrics for the evaluation\n",
    "        self.parcel_delivered_steps = None # --> {p1: 20, p2: 240, p:140}\n",
    "        self.parcel_added_steps = None     # --> {p1: 0, p2: 0, p: 50}\n",
    "        self.agents_crashed = None         # --> {c_2: 120, d_0: 242} \n",
    "        self.metrics = None\n",
    "        \n",
    "        self.agents = [*[\"d_\" + str(i) for i in range(self.NUMBER_OF_DRONES)],*[\"c_\" + str(i) for i in range(self.NUMBER_OF_DRONES, self.NUMBER_OF_DRONES + self.NUMBER_OF_CARS)]]\n",
    "        \n",
    "        # Define observation and action spaces for individual agents\n",
    "        self.action_space = Discrete(self.NUMBER_OF_ACTIONS)\n",
    "\n",
    "        #---- Repeated Obs Space: Represents a parcel with (id, location, destination)  --> # parcel_id starts at 1 \n",
    "        parcel_space = Dict({'id': Discrete(self.MAX_NUMBER_OF_PARCELS+1),\n",
    "                             'location': Discrete(self.NUMBER_OF_NODES + self.NUMBER_OF_AGENTS + 1),\n",
    "                             'destination': Discrete(self.NUMBER_OF_NODES)\n",
    "                            })\n",
    "        self.observation_space = Dict({'obs': Dict({'state': \n",
    "                                                 Dict({ 'position': Discrete(self.NUMBER_OF_NODES),\n",
    "                                                        'battery': Discrete(self.MAX_BATTERY_POWER + 1), #[0-100]\n",
    "                                                        'has_parcel': Discrete(self.MAX_NUMBER_OF_PARCELS + 1),\n",
    "                                                        'current_step': Discrete(self.NUMBER_STEPS_PER_EPISODE + 1)}), \n",
    "                                                 'parcels': Repeated(parcel_space, max_len=self.MAX_NUMBER_OF_PARCELS)\n",
    "                                                }),\n",
    "                                        'allowed_actions': MultiBinary(self.NUMBER_OF_ACTIONS)\n",
    "                                      }) \n",
    "        #TODO: why is reset() not called by env? \n",
    "        self.reset() \n",
    "        \n",
    "    def step(self, action_dict):\n",
    "        \"\"\"conduct the state transitions caused by actions in action_dict\n",
    "        :returns:\n",
    "            - observation_dict: observations for agents that need to act in the next round\n",
    "            - rewards_dict: rewards for agents following their chosen actions\n",
    "            - done_dict: indicates end of episode if max_steps reached or all parcels delivered\n",
    "            - info_dict: pass data to custom logger\n",
    "        \"\"\"\n",
    "\n",
    "        if self.DEBUG_LOG: print(f\"Debug log flag set to {self.DEBUG_LOG}\")\n",
    "        \n",
    "        # ensure no disadvantage for agents with higher IDs if action conflicts with that taken by other agent\n",
    "        action_dict = shuffle_actions(action_dict)\n",
    "    \n",
    "        self.current_step += 1\n",
    "    \n",
    "        # grant step penalty reward    \n",
    "        agent_rewards = {agent: self.STEP_PENALTY for agent in self.agents}  \n",
    "        \n",
    "        # setting an agent done twice might cause crash when used with tune... -> https://github.com/ray-project/ray/issues/10761\n",
    "        dones = {}\n",
    "        \n",
    "        self.metrics['step'] = self.current_step\n",
    "\n",
    "        # dynamically add parcel\n",
    "        if random.random() <= self.THRESHOLD_ADD_NEW_PARCEL and len(self.state['parcels']) < self.MAX_NUMBER_OF_PARCELS:\n",
    " \n",
    "            p_id, parcel = self.generate_parcel()\n",
    "            assert p_id not in self.state['parcels'], \"Duplicate parcel ID generated\"\n",
    "            self.state['parcels'][p_id] = parcel\n",
    "\n",
    "            if self.BASELINE_FLAG:\n",
    "                self.metrics[\"optimal\"] = self.compute_optimality_baseline(p_id, extra_charge=self.BASELINE_OPT_CONSTANT)\n",
    "                \n",
    "            for agent in self.agents:\n",
    "                self.allowed_actions[agent][self.ACTION_DROPOFF + p_id] = np.array([1]).astype(bool)\n",
    "            \n",
    "        if self.BASELINE_FLAG:\n",
    "            old_actions = action_dict\n",
    "            action_dict = {}\n",
    "            # Replace actions with actions recommended by the central baseline\n",
    "            for agent, action in old_actions.items():\n",
    "                if len(self.baseline_missions[agent]) > 0:\n",
    "                    new_action = self.baseline_missions[agent][0]\n",
    "\n",
    "                    if type(new_action) is tuple: # dropoff action with minimal time\n",
    "\n",
    "                        if self.current_step >= new_action[1]:\n",
    "                            new_action = new_action[0]\n",
    "                            self.baseline_missions[agent].pop(0)\n",
    "                        else: #agent has to wait for previous subroute agent\n",
    "                            new_action = 0\n",
    "                    else: # move or pickup or charge\n",
    "                        self.baseline_missions[agent].pop(0)\n",
    "                            \n",
    "                    action_dict[agent] = new_action\n",
    "\n",
    "                else: # agent has no baseline mission -> Noop\n",
    "                    action_dict[agent] = 0\n",
    "                \n",
    "        # carry out State Transition\n",
    "        \n",
    "        # handel NOP actions: -> action == 0 \n",
    "        noop_agents =  {agent: action for agent, action in action_dict.items() if action == 0}\n",
    "        effectual_agents_items = {agent: action for agent, action in action_dict.items() if action > 0}.items()\n",
    "        \n",
    "        # now: transaction between agents modelled as pickup of just offloaded (=dropped) parcel --> handle dropoff first\n",
    "        moving_agents = {agent: action for agent, action in effectual_agents_items if 0 < action and action <= self.NUMBER_OF_NODES}\n",
    "        dropoff_agents = {agent: action for agent, action in effectual_agents_items if action == self.ACTION_DROPOFF}\n",
    "        pickup_agents = {agent: action for agent, action in effectual_agents_items if action > self.ACTION_DROPOFF}\n",
    "\n",
    "        # handle noop / charge decisions:\n",
    "        for agent, action in noop_agents.items():\n",
    "            # check if recharge is possible\n",
    "            current_pos = self.state['agents'][agent]['position']\n",
    "            if current_pos in self.CHARGING_STATION_NODES:\n",
    "                self.state['agents'][agent]['battery'] = self.MAX_BATTERY_POWER\n",
    "\n",
    "        # handle Movement actions:\n",
    "        for agent, action in moving_agents.items():\n",
    "            # get Current agent position from state\n",
    "            self.state['agents'][agent]['battery'] += -1\n",
    "            current_pos = self.state['agents'][agent]['position']\n",
    "            \n",
    "            # networkX: use node instead of edge:\n",
    "            destination = action - 1\n",
    "            \n",
    "            if self.graph.has_edge(current_pos, destination):\n",
    "                # Agent chose existing edge! -> check if type is suitable\n",
    "              \n",
    "                agent_type = 'road' if agent[:1] == 'c' else 'air'\n",
    "                if self.graph[current_pos][destination][\"type\"] in [agent_type, 'both']:\n",
    "                    # Edge has correct type\n",
    "                    self.state['agents'][agent]['position'] = destination\n",
    "                    self.state['agents'][agent]['battery'] += -(self.graph[current_pos][destination][agent_type] +1)\n",
    "                    if self.state['agents'][agent]['battery'] < 0:        # ensure negative battery value does not break obs_space\n",
    "                        #Battery below 0 --> reset to 0 (stay in obs space)\n",
    "                        self.state['agents'][agent]['battery'] = 0\n",
    "                    \n",
    "                    self.blocked_agents[agent] = self.graph[current_pos][destination][agent_type]\n",
    "                    self.update_allowed_actions_nodes(agent)\n",
    "\n",
    "                    \n",
    "        # handle Dropoff Decision: -> action == self.NUMBER_OF_NODES + 2\n",
    "        for agent, action in dropoff_agents.items():\n",
    "            self.state['agents'][agent]['battery'] += -1\n",
    "\n",
    "            if self.state['agents'][agent]['has_parcel'] > 0:   # agent has parcel\n",
    "                parcel_id = self.state['agents'][agent]['has_parcel']\n",
    "                self.state['agents'][agent]['has_parcel'] = 0\n",
    "                self.state['parcels'][parcel_id][0] = self.state['agents'][agent]['position']\n",
    "                if self.state['parcels'][parcel_id][0] ==  self.state['parcels'][parcel_id][1]:\n",
    "                    # Delivery successful\n",
    "                    \n",
    "                    agent_rewards[agent] += self.PARCEL_DELIVERED  # local reward\n",
    "                    # global contribution rewards\n",
    "                    active_agents, reward = self.prepare_global_reward()\n",
    "                    for a_id in active_agents: agent_rewards[a_id] += reward\n",
    "                    \n",
    "                    self.state['parcels'][parcel_id][0] = self.PARCEL_STATE_DELIVERED\n",
    "                    self.parcels_delivered[int(parcel_id) -1] = True                    # Parcel_ids start at 1\n",
    "\n",
    "                    self.metrics['delivered'].update({\"p_\" + str(parcel_id): self.current_step})\n",
    "\n",
    "                self.update_allowed_actions_parcels(agent)\n",
    "          \n",
    "        \n",
    "        # handle Pickup Decision:\n",
    "        for agent, action in pickup_agents.items():\n",
    "            self.state['agents'][agent]['battery'] += -1 \n",
    "    \n",
    "            # agent has free capacity\n",
    "            if self.state['agents'][agent]['has_parcel'] == 0:  #free parcel capacity\n",
    "                # convert action_id to parcel_id\n",
    "                parcel_id = action - self.ACTION_DROPOFF\n",
    "\n",
    "                if self.DEBUG_LOG: assert parcel_id in self.state['parcels'] # parcel {parcel_id} already in ENV ??\n",
    "                    \n",
    "                elif self.state['parcels'][parcel_id][0] == self.state['agents'][agent]['position']:\n",
    "                    # Successful pickup operation\n",
    "                    self.state['parcels'][parcel_id][0] = self.NUMBER_OF_NODES + int(agent[2:])\n",
    "                    self.state['agents'][agent]['has_parcel'] = int(parcel_id)\n",
    "                    \n",
    "                    self.update_allowed_actions_parcels(agent)\n",
    "                        \n",
    "        # unblock agents for next round \n",
    "        self.blocked_agents = {agent: remaining_steps -1 for agent, remaining_steps in self.blocked_agents.items() if remaining_steps > 1}\n",
    "\n",
    "        # handle dones - out of battery or max_steps or goal reached\n",
    "        for agent in action_dict.keys():\n",
    "            if agent not in self.done_agents and self.state['agents'][agent]['battery'] <= 0:\n",
    "                agent_rewards[agent] = self.BATTERY_DIED_WITH_PARCEL if self.state['agents'][agent]['has_parcel'] != 0 else self.BATTERY_DIED\n",
    "                dones[agent] = True\n",
    "                self.done_agents.append(agent)\n",
    "                \n",
    "                self.metrics['crashed'].update({agent: self.current_step})\n",
    "                \n",
    "                if len(self.done_agents) == self.NUMBER_OF_AGENTS:\n",
    "                    # all agents dead\n",
    "                    self.all_done = True\n",
    "        \n",
    "        # check if episode terminated because of goal reached or all agents crashed -> avoid setting done twice\n",
    "        if self.current_step >= self.NUMBER_STEPS_PER_EPISODE or (all(self.parcels_delivered) and len(self.parcels_delivered) == self.MAX_NUMBER_OF_PARCELS):\n",
    "            # check if episode success:\n",
    "            if self.current_step < self.NUMBER_STEPS_PER_EPISODE:\n",
    "                    # grant global reward for all parcels delivered \n",
    "                    active_agents, reward = self.prepare_global_reward(_episode_success=True)\n",
    "                    for a_id in active_agents: agent_rewards[a_id] += reward\n",
    "            \n",
    "            self.all_done = True\n",
    "\n",
    "        dones['__all__'] = self.all_done\n",
    "\n",
    "        \n",
    "        parcel_obs = self.get_parcel_obs()\n",
    "    \n",
    "        # obs \\ rewards \\ dones\\ info\n",
    "        return  {agent: { 'obs': {'state': {'position': self.state['agents'][agent]['position'], 'battery': self.state['agents'][agent]['battery'],\n",
    "                                    'has_parcel': self.state['agents'][agent]['has_parcel'],'current_step': self.current_step},\n",
    "                                    'parcels': parcel_obs},\n",
    "                         'allowed_actions': self.allowed_actions[agent]} for agent in self.agents if agent not in self.blocked_agents and agent not in self.done_agents}, \\\n",
    "                { agent: agent_rewards[agent] for agent in self.agents}, \\\n",
    "                dones, \\\n",
    "                {}\n",
    "\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)  \n",
    "\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"resets variables; returns dict with observations, keys are agent_ids\"\"\"\n",
    "        self.current_step = 0\n",
    "        self.blocked_agents = {}\n",
    "        self.parcels_delivered = [False for _ in range(self.MAX_NUMBER_OF_PARCELS)]\n",
    "        self.done_agents = []\n",
    "        self.all_done = False\n",
    "        \n",
    "        self.metrics = { \"step\": self.current_step,\n",
    "            \"delivered\": {},\n",
    "            \"crashed\": {},\n",
    "            \"added\": {},\n",
    "            \"optimal\": None\n",
    "        }\n",
    "\n",
    "        #Baseline\n",
    "        self.baseline_missions = {agent: [] for agent in self.agents}\n",
    "        self.o_employed = [0 for _ in range(self.NUMBER_OF_AGENTS)]\n",
    "\n",
    "        self.agents_base = None  # env.agents in the pseudocode\n",
    "        self.allowed_actions = { agent: np.array([1 for act in range(self.NUMBER_OF_ACTIONS)]) for agent in self.agents}\n",
    " \n",
    "        #Reset State\n",
    "        self.state = {'agents': {},\n",
    "                      'parcels': {}}\n",
    "        #generate initial parcels\n",
    "        for _ in range(self.INIT_NUMBER_OF_PARCELS):\n",
    "            p_id, parcel = self.generate_parcel()\n",
    "            self.state['parcels'][p_id] = parcel            \n",
    "\n",
    "        parcel_obs = self.get_parcel_obs()\n",
    "        \n",
    "        # init agents\n",
    "        self.state['agents'] = {agent: {'position': self._random_feasible_agent_position(agent),\n",
    "                                        'battery': self.MAX_BATTERY_POWER, 'has_parcel': 0} for agent in self.agents}\n",
    "        \n",
    "        if self.BASELINE_FLAG:\n",
    "            for parcel in self.state['parcels']:\n",
    "                self.compute_central_delivery(parcel, debug_log=False)\n",
    "                \n",
    "                # TODO really return something here??\n",
    "                self.metrics['optimal'] = self.compute_optimality_baseline(parcel, extra_charge=self.BASELINE_OPT_CONSTANT, debug_log=False)\n",
    "        \n",
    "        # compute allowed actions per agent --> move to function\n",
    "        for agent in self.agents:\n",
    "            self.update_allowed_actions_nodes(agent)\n",
    "            self.update_allowed_actions_parcels(agent)    \n",
    "            \n",
    "        agent_obs = {agent: {'obs': {'state':  {'position': state['position'], 'battery': state['battery'],\n",
    "                                        'has_parcel': state['has_parcel'],'current_step': self.current_step},\n",
    "                            'parcels': parcel_obs\n",
    "                            },\n",
    "                            'allowed_actions': self.allowed_actions[agent]\n",
    "                            } for agent, state in self.state['agents'].items()\n",
    "                    } \n",
    "        \n",
    "        return {**agent_obs}\n",
    "    \n",
    "    \n",
    "    def _random_feasible_agent_position(self, agent_id):\n",
    "        \"\"\"Needed to avoid car agents being initialized at nodes only reachable by drones\n",
    "        and thus trapped from the beginning. Ensures that car agents start at a node of type 'road' or 'parking'.\n",
    "        \"\"\"\n",
    "        position = random.randrange(self.NUMBER_OF_NODES)\n",
    "        if agent_id[0] == 'c':    # agent is car \n",
    "            while self.graph.nodes[position]['type'] == 'air':      # position not reachable by car\n",
    "                position = random.randrange(self.NUMBER_OF_NODES)\n",
    "        return position\n",
    "    \n",
    "    \n",
    "    def update_allowed_actions_nodes(self, agent):\n",
    "        new_pos = self.state['agents'][agent]['position']\n",
    "        next_steps = list(self.graph.neighbors(new_pos))\n",
    "        agent_type = 'air' if agent[0]=='d' else 'road'\n",
    "        \n",
    "        allowed_nodes = np.zeros(self.NUMBER_OF_NODES)\n",
    "        for neighbor in next_steps:\n",
    "            if self.graph[new_pos][neighbor]['type'] in [agent_type, 'both']:\n",
    "                allowed_nodes[neighbor] = 1\n",
    "        self.allowed_actions[agent][1:self.NUMBER_OF_NODES+1] = np.array(allowed_nodes).astype(bool)\n",
    "                 \n",
    "            \n",
    "    def update_allowed_actions_parcels(self, agent):\n",
    "        \"\"\" Allow only the Dropoff or Pickup actions, depending on the has_parcel value of the agent.\n",
    "        Pickup is not concerned with the parcel actually being at the agents current location, only with free capacity\n",
    "        and the parcel already being added to the ENV\"\"\"\n",
    "        num_parcels = len(self.state['parcels'])\n",
    "                          \n",
    "        allowed_parcels = np.zeros(self.MAX_NUMBER_OF_PARCELS)\n",
    "        dropoff = 1\n",
    "        if self.state['agents'][agent]['has_parcel'] == 0:\n",
    "            dropoff = 0\n",
    "            allowed_parcels = np.concatenate([np.ones(num_parcels), np.zeros(self.MAX_NUMBER_OF_PARCELS - num_parcels)])\n",
    "                          \n",
    "        self.allowed_actions[agent][self.NUMBER_OF_NODES+1:] = np.array([dropoff, *allowed_parcels]).astype(bool)\n",
    "    \n",
    "    \n",
    "    def get_parcel_obs(self):\n",
    "        parcel_obs = [{'id':pid, 'location': parcel[0], 'destination':parcel[1]} for (pid, parcel) in self.state['parcels'].items()]\n",
    "        return parcel_obs\n",
    "    \n",
    "    def generate_parcel(self):\n",
    "        \"\"\"generate new parcel id and new parcel with random nodes for location and destination.\n",
    "            p_ids (int) start at 1, later nodes for parcel will be sampled to avoid parcels that already spawn at their destination\"\"\"\n",
    "        p_id = len(self.state['parcels']) + 1\n",
    "\n",
    "        parcel = random.sample(range(self.NUMBER_OF_NODES), 2)  # => initial location != destination\n",
    "        self.metrics['added'].update({p_id: self.current_step})\n",
    "        \n",
    "        return p_id, parcel\n",
    "    \n",
    "    def prepare_global_reward(self, _episode_success=False):\n",
    "        \"\"\" computes a global reward for all active agents still in the environment.\n",
    "            If _episode_success is set to True, all parcels have been delivered an the ALL_DELIVERED reward is granted.\n",
    "            :Returns: list of active agents and the reward value\n",
    "        \"\"\"\n",
    "        agents_alive = list(set(self.agents).difference(set(self.done_agents)))\n",
    "        if self.DEBUG_LOG: assert len(agents_alive) > 0\n",
    "        reward = self.PARCEL_DELIVERED * (self.NUMBER_STEPS_PER_EPISODE - self.current_step) / self.NUMBER_STEPS_PER_EPISODE if _episode_success else self.PARCEL_DELIVERED / len(agents_alive)\n",
    "\n",
    "        return agents_alive, reward\n",
    "    \n",
    "    \n",
    "    #------ BASELINE related methods ------###         \n",
    "    def compute_optimality_baseline(self, parcel_id, extra_charge=2.5, debug_log=False):\n",
    "        \"\"\"Used in the optimality baseline\n",
    "            Input: parcel_id, (extra_charge)\n",
    "            Output: new total delivery rounds needed for all parcels\n",
    "        \"\"\"\n",
    "        parcel = self.state['parcels'][parcel_id]\n",
    "        path_time = 2 + shortest_path_length(self.graph, parcel[0], parcel[1], 'weight')\n",
    "        \n",
    "        _time = math.ceil(path_time * extra_charge) # round to next higher integer\n",
    "        \n",
    "        min_index = self.o_employed.index(min(self.o_employed))\n",
    "        self.o_employed[min_index] += _time\n",
    "                                    \n",
    "        return max(self.o_employed)\n",
    "        \n",
    "    \n",
    "    def compute_central_delivery(self, p_id, debug_log = False):\n",
    "        \"\"\"Used in the central baseline, iteratively tries to find a good delivery route \n",
    "        with the available agents in the time specified in BASELINE_TIME_CONSTRAINT\n",
    "            Input: parcel_id\n",
    "            Output: Dict: {agent_id: [actions], ...}  --> update that dict! (merge in this function with prev actions!) \n",
    "        \"\"\"\n",
    "\n",
    "        if self.agents_base is None:\n",
    "            self.agents_base = {a_id: (a['position'], 0) for (a_id, a) in self.state[\"agents\"].items()}  # last instructed pos + its step count\n",
    "\n",
    "        min_time = None\n",
    "        new_missions = {}   # key: agent, value: [actions]\n",
    "\n",
    "        source = self.state[\"parcels\"][p_id][0]\n",
    "        target = self.state[\"parcels\"][p_id][1]\n",
    "        shortest_paths_generator = nx.shortest_simple_paths(self.graph, source, target, weight='weight')\n",
    "\n",
    "        running = BoolTimer(self.BASELINE_TIME_CONSTRAINT)\n",
    "        running.start()             \n",
    "        \n",
    "        while running:\n",
    "            # Default --> assign full route to nearest drone\n",
    "            if min_time is None:   \n",
    "\n",
    "                air_route = nx.shortest_path(self.graph, source= source, target= target, weight=\"air\")\n",
    "                air_route_time = nx.shortest_path_length(self.graph, source= source, target= target, weight=\"air\")\n",
    "                air_route.pop(0)  # remove source node from path\n",
    "               \n",
    "                # Assign most suitable Drone\n",
    "                best_drone = None\n",
    "                for (a_id, a_tp) in self.agents_base.items():\n",
    "                    if a_id[0] != 'd':                      # filter for correct agent type\n",
    "                        continue\n",
    "                        \n",
    "                    journey_time = nx.shortest_path_length(self.graph, source=a_tp[0], target=source, weight=\"air\") + a_tp[1]\n",
    "                    if min_time is None or journey_time < min_time:\n",
    "                        min_time = journey_time\n",
    "                        best_drone = (a_id, a_tp)\n",
    "  \n",
    "                # construct path for agent\n",
    "                drone_route = nx.shortest_path(self.graph, source= best_drone[1][0], target= source, weight=\"air\")\n",
    "\n",
    "                drone_route_actions = [x+1 for x in drone_route[1:]] + [(self.ACTION_DROPOFF + p_id, 0)] + [x+1 for x in air_route[1:]] + [self.ACTION_DROPOFF]   # increment node_ids by one to get corresponding action\n",
    "                min_time += air_route_time + 2 # add 2 steps for pick & drop\n",
    "                \n",
    "                self._add_charging_stops_to_route(drone_route_actions, debug_log=debug_log)              \n",
    "                \n",
    "                new_missions[best_drone[0]] = (drone_route_actions, min_time) \n",
    "                \n",
    "            else:   # try to improve the existing base mission  \n",
    "                try:\n",
    "                    shortest_route = next(shortest_paths_generator)\n",
    "                except StopIteration:\n",
    "                    break               # all existing shortest paths already tried\n",
    "                    \n",
    "                subroutes = self._path_to_subroutes(shortest_route, debug_log=debug_log)\n",
    "                duration, min_agents = self._find_best_agents(subroutes, min_time, debug_log=debug_log)\n",
    "\n",
    "                if duration < min_time:\n",
    "                    # faster delivery route found!          \n",
    "                    assert duration < min_time, \"Central Baseline prefered a longer route...\"\n",
    "                    \n",
    "                    # update min_time\n",
    "                    min_time = duration                    \n",
    "                    new_missions = self._build_missions(min_agents, subroutes, p_id, debug_log=debug_log) \n",
    "\n",
    "        #---- end while = Timer\n",
    "        # now save the best mission found in the ENV\n",
    "        for agent in new_missions.keys():\n",
    "            # retrieve target node from mission, depends on charging stops and case no move necessary    \n",
    "            target = None     \n",
    "            if isinstance(new_missions[agent][0][-2], int):    \n",
    "                target = new_missions[agent][0][-2] \n",
    "                if target == 0: target = new_missions[agent][0][-3]    # charging action was added before dropoff\n",
    "                target -= 1                                            # action-1 = node_id\n",
    "            else:  # handle case no move necessary -> pick action before dropoff\n",
    "                target = self.agents_base[agent][0]\n",
    "\n",
    "            self.agents_base[agent] = (target, self.agents_base[agent][1] + new_missions[agent][1])  \n",
    "            self.baseline_missions[agent].extend(new_missions[agent][0])\n",
    "            \n",
    "        return new_missions\n",
    "        \n",
    "        \n",
    "    def _find_best_agents(self, subroutes, min_time, debug_log=False):\n",
    "        \"\"\" For use in centrality_baseline. Finds best available agents for traversing a set of subroutes \n",
    "            and returns these with the total duration for doing so.\n",
    "            Input: subroutes = [ edge_type, [actions]]\n",
    "            \"\"\"\n",
    "        \n",
    "        min_agents = {}\n",
    "        temp_agents_base = {k:v for k,v in self.agents_base.items()}  # deep copy for temporary planning transfer or stick with agent??\n",
    "\n",
    "        for i,r in enumerate(subroutes):\n",
    "\n",
    "            # init some helper vars\n",
    "            a_type = \"d\" if r[0] == 'air' else 'c' \n",
    "            min_time_sub = None    # best time (min) for this subroute (closest agent)\n",
    "            best_agent_sub = None\n",
    "\n",
    "            # iterate over agents of correct type!  --> later: Busy / unbusy\n",
    "            for (a_id, a_tp) in temp_agents_base.items():\n",
    "                #reminder: a_tp is tuple of latest future position (node, timestep)\n",
    "\n",
    "                # filter for correct agent type - even if type is 'both' one agent can still take only its edge type\n",
    "                weight_agent = r[0]\n",
    "                if r[0] == 'both':\n",
    "                    weight_agent = 'road' if a_id[0] == 'c' else 'air'\n",
    "                else:                     # wrong agent type\n",
    "                    if a_id[0] != a_type:  # todo replace with parameter variable in function!\n",
    "                        continue \n",
    "                        \n",
    "                journey_time = nx.shortest_path_length(self.graph, source=a_tp[0], target=r[1][0], weight=weight_agent) + a_tp[1] # earliest time agent can be there\n",
    "                if min_time_sub is None or journey_time < min_time_sub:\n",
    "                    min_time_sub = journey_time\n",
    "                    best_agent_sub = (a_id, a_tp)    # a_id, a_tp = (latest location, timestep)\n",
    "\n",
    "            # closest available agent found        \n",
    "            best_agent_weight = weight_agent = 'road' if best_agent_sub[0] == 'c' else 'air'\n",
    "            duration_sub = min_time_sub + nx.shortest_path_length(self.graph, source=r[1][0], target=r[1][-1], weight=best_agent_weight) + 1\n",
    "\n",
    "            # update agent state in temporary planning\n",
    "            temp_agents_base[best_agent_sub[0]] = (r[1][-1], duration_sub)\n",
    "\n",
    "            # agent_tuple, duration_subroutes_until_then\n",
    "            min_agents[i] = (best_agent_sub, duration_sub)\n",
    "            \n",
    "            if debug_log: assert duration_sub < sys.maxsize, \"Non existent edge taken somewhere...\"\n",
    "\n",
    "            # check if current subroute already longer than the min one\n",
    "            if duration_sub > min_time:\n",
    "                break # already worse, check next simple path\n",
    "                \n",
    "        return duration_sub, min_agents\n",
    "    \n",
    "    \n",
    "    def _build_missions(self, min_agents, subroutes, parcel_id, debug_log = False):\n",
    "        \"\"\"For use in centrality_baseline. Computes list of actions for delivery of parcel parcel_id \n",
    "        and necessary duration for execution from list of agents, subroutes\"\"\"\n",
    "        new_mission = {}\n",
    "\n",
    "        for i, s in enumerate(subroutes):\n",
    "            best_agent_pos = min_agents[i][0][1][0]\n",
    "            #earliest time to start that action\n",
    "            time_pickup = min_agents[i-1][1] if i > 0 else 0 # First subroute pickup as soon as possible\n",
    "            \n",
    "            # construct the actual delivery path to pickup\n",
    "            delivery_route = nx.shortest_path(self.graph, source=best_agent_pos, target=s[1][0], weight=s[0])\n",
    "            delivery_route_actions = [x+1 for x in delivery_route[1:]] + [(self.ACTION_DROPOFF + parcel_id, time_pickup)] + [x+1 for x in s[1][1:]] + [self.ACTION_DROPOFF]\n",
    "\n",
    "            if debug_log: assert min_agents[i][0] not in new_mission  #I see no preferable case where agent picks-up 1 parcel 2 times --> holding always better than following\n",
    "\n",
    "            self._add_charging_stops_to_route(delivery_route_actions, debug_log=debug_log)\n",
    "            new_mission[min_agents[i][0][0]] = (delivery_route_actions, min_agents[i][1])\n",
    "            \n",
    "        return new_mission\n",
    "    \n",
    "    \n",
    "    def _add_charging_stops_to_route(self, route_actions, debug_log=False):\n",
    "        \"\"\"For use in centrality_baseline. Iterates through a list of actions and inserts a charging action\n",
    "        after every move action to a node with a charging station. \n",
    "        Tuples representing Dropoff actions with minimal executions time are updated to account for eventual delays. \"\"\"\n",
    "        \n",
    "        delay = 0\n",
    "        for i, n in enumerate(route_actions):\n",
    "            if type(n) is tuple:\n",
    "                if delay > 0: route_actions[i] = (n[0], n[1] + delay)\n",
    "            else: \n",
    "                if n-1 in self.CHARGING_STATION_NODES:\n",
    "                    delay += 1\n",
    "                    route_actions.insert(i+1, 0)\n",
    "\n",
    "\n",
    "    def _path_to_subroutes(self, path, debug_log= False):\n",
    "        \"\"\"For use in centrality_baseline. Takes path in the graph as input and returns list of subroutes \n",
    "        split at changes of the edge types with the type\"\"\"\n",
    "\n",
    "        # get subroutes by their edge_types\n",
    "        e_type_prev = None\n",
    "        e_type_changes = [] # save indices of source nodes before new edge type\n",
    "        subroutes = []\n",
    "        _subroute = [path[0]] \n",
    "\n",
    "        if len(path) > 1:\n",
    "            e_type_prev = self.graph.edges[path[0], path[1]]['type']\n",
    "    \n",
    "            for i, node in enumerate(path[1:-1], start=1):\n",
    "    \n",
    "                e_type_next = self.graph.edges[node, path[i+1]]['type']\n",
    "    \n",
    "                _subroute.append(node)\n",
    "                if e_type_next != e_type_prev:   \n",
    "                    subroutes.append((e_type_prev, _subroute))\n",
    "                    _subroute = [node]\n",
    "                    e_type_prev = e_type_next\n",
    "    \n",
    "            _subroute.append(path[-1])\n",
    "        subroutes.append((e_type_prev, _subroute))   # don't forget last subroute           \n",
    "    \n",
    "        return subroutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6698c6",
   "metadata": {},
   "source": [
    "## Agent Model and Experiment Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14b96755",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete, Box, Tuple, MultiDiscrete, Dict, MultiBinary\n",
    "#from ray.rllib.utils.spaces.space_utils import flatten_space\n",
    "#from ray.rllib.models.preprocessors import DictFlatteningPreprocessor\n",
    "\n",
    "# Parametric-action agent model  --> apply Action Masking!\n",
    "class ParametricAgentModel(TFModelV2):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name, *args, **kwargs):\n",
    "        super(ParametricAgentModel, self).__init__(obs_space, action_space, num_outputs, model_config, name, *args, **kwargs)\n",
    "\n",
    "        assert isinstance(action_space, Discrete), f'action_space is a {type(action_space)}, but should be Discrete!'\n",
    "        \n",
    "        # Adjust for number of agents/parcels/Nodes!! -> Simply copy found shape from the thrown exception\n",
    "        true_obs_shape = (2370, )\n",
    "        action_embed_size = action_space.n\n",
    "        \n",
    "        self.action_embed_model = FullyConnectedNetwork(\n",
    "            Box(0, 1, shape=true_obs_shape),  # TODO hier nochmal die 1 anpassen?? --> muss das hier der obs entsprechen??\n",
    "            action_space,\n",
    "            action_embed_size,\n",
    "            model_config,\n",
    "            name + '_action_embedding')\n",
    "        \n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        \n",
    "        action_mask = input_dict['obs']['allowed_actions']\n",
    "        action_embedding, _ = self.action_embed_model.forward({'obs_flat': input_dict[\"obs_flat\"]}, state, seq_lens)\n",
    "        intent_vector = tf.expand_dims(action_embedding, 1)\n",
    "        action_logits = tf.reduce_sum(intent_vector, axis=1)\n",
    "        inf_mask = tf.maximum(tf.math.log(action_mask), tf.float32.min)\n",
    "\n",
    "        return action_logits + inf_mask, state\n",
    "    \n",
    "    def value_function(self):\n",
    "        return self.action_embed_model.value_function()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2017a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Proposed way to train / evaluate MARL policy from github Issues: --> https://github.com/ray-project/ray/issues/9123 and  https://github.com/ray-project/ray/issues/9208\n",
    "\n",
    "def train(config, name, save_dir, stop_criteria, num_samples, verbosity=1):\n",
    "    \"\"\"\n",
    "    Train an RLlib PPO agent using tune until any of the configured stopping criteria is met.\n",
    "    :param stop_criteria: Dict with stopping criteria.\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run\n",
    "    :return: Return the path to the saved agent (checkpoint) and tune's ExperimentAnalysis object\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/analysis.html#experimentanalysis-tune-experimentanalysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Start training\")\n",
    "    analysis = ray.tune.run(PPOTrainer, verbose=verbosity, config=config, local_dir=save_dir, \n",
    "                            stop=stop_criteria, name=name, num_samples=num_samples,\n",
    "                            checkpoint_at_end=True, resume=True)\n",
    "    \n",
    "    # list of lists: one list per checkpoint; each checkpoint list contains 1st the path, 2nd the metric value\n",
    "    checkpoints = analysis.get_trial_checkpoints_paths(trial=analysis.get_best_trial('episode_reward_mean', mode='max'),\n",
    "                                                       metric='episode_reward_mean')\n",
    "    # retriev the checkpoint path; we only have a single checkpoint, so take the first one\n",
    "    checkpoint_path = checkpoints[0][0]\n",
    "    print(f\"Saved trained model in checkpoint {checkpoint_path} - achieved episode_reward_mean: {checkpoints[0][1]}\")\n",
    "    return checkpoint_path, analysis\n",
    "\n",
    "\n",
    "def load(config, path):\n",
    "    \"\"\"\n",
    "    Load a trained RLlib agent from the specified path. Call this before testing the trained agent.\n",
    "    \"\"\"\n",
    "    agent = PPOTrainer(config=config) #, env=env_class)\n",
    "    agent.restore(path)\n",
    "    return agent\n",
    "    \n",
    "    \n",
    "def test(env_class, env_config, policy_mapping_fcn, agent):\n",
    "    \"\"\"Test trained agent for a single episode. Return the retrieved env metrics for this episode and the episode reward\"\"\"\n",
    "    # instantiate env class\n",
    "    env = env_class(env_config)\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    \n",
    "    while not done: # run until episode ends\n",
    "        actions = {}\n",
    "        for agent_id, agent_obs in obs.items():\n",
    "            # Here: policy_id == agent_id - added this to avoid confusion for other policy mappings \n",
    "            policy_id = policy_mapping_fcn(agent_id, episode=None, worker=None) \n",
    "            actions[agent_id] = agent.compute_action(agent_obs, policy_id=policy_id)\n",
    "        obs, reward, done, info = env.step(actions)\n",
    "        done = done['__all__']\n",
    "        \n",
    "        # sum up reward for all agents\n",
    "        episode_reward += sum(reward.values())\n",
    "        \n",
    "    # Retrieve custom metrics from ENV\n",
    "    return env.metrics, episode_reward \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14c5ec6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_and_test_scenarios(config, seeds=None):\n",
    "    \"\"\" Trains for a single scenario indicated by a seed \"\"\"\n",
    "    # TODO how to distinguish between the different algos ? \n",
    "    print(\"Starte: run_function_trainer!\")\n",
    "    \n",
    "    # prepare the config dicts\n",
    "    NAME = config['NAME']\n",
    "    SAVE_DIR = config['SAVE_DIR']\n",
    "    ENVIRONMENT = config['ENV']\n",
    "    \n",
    "    # Simulations\n",
    "    NUMBER_STEPS_PER_EPISODE = config['NUMBER_STEPS_PER_EPISODE']\n",
    "    STOP_CRITERIA = config['STOP_CRITERIA']\n",
    "    NUMBER_OF_SAMPLES = config['NUMBER_OF_SAMPLES']\n",
    "    \n",
    "    #MAP / ENV\n",
    "    NUMBER_OF_DRONES = config['NUMBER_OF_DRONES']\n",
    "    NUMBER_OF_CARS = config['NUMBER_OF_CARS']\n",
    "    NUMBER_OF_AGENTS = NUMBER_OF_DRONES + NUMBER_OF_CARS\n",
    "    MAX_NUMBER_OF_PARCELS = config['MAX_NUMBER_OF_PARCELS']\n",
    "    \n",
    "    # TESTING\n",
    "    SEEDS = config['SEEDS']\n",
    "    \n",
    "    env_config = {\n",
    "            'DEBUG_LOGS':False,\n",
    "            'TOPOLOGY': config['TOPOLOGY'],\n",
    "            # Simulation config\n",
    "            'NUMBER_STEPS_PER_EPISODE': NUMBER_STEPS_PER_EPISODE,\n",
    "            #'NUMBER_OF_TIMESTEPS': NUMBER_OF_TIMESTEPS,\n",
    "            'RANDOM_SEED': None, # 42\n",
    "            # Map\n",
    "            'CHARGING_STATION_NODES': config['CHARGING_STATION_NODES'],\n",
    "            # Entities\n",
    "            'NUMBER_OF_DRONES': NUMBER_OF_DRONES,\n",
    "            'NUMBER_OF_CARS': NUMBER_OF_CARS,\n",
    "            'MAX_BATTERY_POWER': config['MAX_BATTERY_POWER'],  # TODO split this for drone and car??\n",
    "            'INIT_NUMBER_OF_PARCELS': config['INIT_NUMBER_OF_PARCELS'],\n",
    "            'MAX_NUMBER_OF_PARCELS': config['MAX_NUMBER_OF_PARCELS'],\n",
    "            'THRESHOLD_ADD_NEW_PARCEL': config['THRESHOLD_ADD_NEW_PARCEL'],\n",
    "            # Baseline settings\n",
    "            'BASELINE_FLAG': False,  # is set True in the test function when needed\n",
    "            'BASELINE_OPT_CONSTANT': config['BASELINE_OPT_CONSTANT'],\n",
    "            'BASELINE_TIME_CONSTRAINT': config['BASELINE_TIME_CONSTRAINT'],\n",
    "            # TODO \n",
    "            #Rewards\n",
    "            'REWARDS': config['REWARDS'] \n",
    "        }\n",
    "        \n",
    "    run_config = {\n",
    "        'num_gpus': config['NUM_GPUS'],\n",
    "        'num_workers': config['NUM_WORKERS'],\n",
    "        'env': ENVIRONMENT,\n",
    "        'env_config': env_config,\n",
    "        'multiagent': {\n",
    "            'policies': {\n",
    "                # tuple values: policy, obs_space, action_space, config\n",
    "                **{a: (None, None, None, { 'model': {'custom_model': ParametricAgentModel }, 'framework': 'tf'}) for a in ['d_'+ str(j) for j in range(NUMBER_OF_DRONES)] + ['c_'+ str(i) for i in range(NUMBER_OF_DRONES, NUMBER_OF_CARS + NUMBER_OF_DRONES)]}\n",
    "            },\n",
    "            'policy_mapping_fn': policy_mapping_fn,\n",
    "            'policies_to_train': ['d_'+ str(i) for i in range(NUMBER_OF_DRONES)] + ['c_'+ str(i) for i in range(NUMBER_OF_DRONES, NUMBER_OF_CARS + NUMBER_OF_DRONES)]\n",
    "        },\n",
    "        #'log_level': \"INFO\",\n",
    "        #\"hiddens\": [],     # For DQN\n",
    "        #\"dueling\": False,  # For DQN\n",
    "    }\n",
    "    \n",
    "    # Train and Evaluate the agents !\n",
    "    checkpoint, analysis = train(run_config, NAME, SAVE_DIR, STOP_CRITERIA, NUMBER_OF_SAMPLES)\n",
    "    print(\"Training finished - Checkpoint: \", checkpoint)\n",
    "    \n",
    "    env_class = ENVIRONMENT\n",
    "    # Restore trained policies for evaluation\n",
    "    agent = load(run_config, checkpoint)\n",
    "    print(\"Agent loaded - Agent: \", agent)\n",
    "    \n",
    "    # Run the test cases for the specified seeds\n",
    "    runs = {'max_steps': NUMBER_STEPS_PER_EPISODE, 'max_parcels': MAX_NUMBER_OF_PARCELS, 'max_agents': NUMBER_OF_AGENTS}\n",
    "    \n",
    "    for seed in SEEDS:\n",
    "        print(seed)\n",
    "        env_config['RANDOM_SEED'] = seed\n",
    "        # TODO check if \n",
    "        assert run_config['env_config']['RANDOM_SEED'] == seed\n",
    "        result = test_scenario(run_config, agent)\n",
    "        runs.update(result)\n",
    "        \n",
    "    return runs\n",
    "\n",
    "\n",
    "def test_scenario(config, agent):\n",
    "    \"\"\"\n",
    "    Loads a pretrained agent, initializes an environment from the seed \n",
    "    and then evaluates it over one episode with the Marl agents and the central baseline.\n",
    "\n",
    "    Returns: dict with results for graph creation for both evaluation / inference runs\n",
    "    \"\"\"\n",
    "    #TODO unterscheidung run_config > env_config\n",
    "    # Wie die beiden Runs abspeichern --> {Seed + [marl / base] : result }\n",
    "\n",
    "    env_class = config['env']  \n",
    "    env_config = config['env_config']\n",
    "    seed = env_config['RANDOM_SEED']\n",
    "    policy_mapping_fn = config['multiagent']['policy_mapping_fn']\n",
    "    \n",
    "    # Test with MARL\n",
    "\n",
    "    metrics_marl, reward_marl = test(env_class, env_config, policy_mapping_fn, agent)\n",
    "    \n",
    "    # Test with CentralBase\n",
    "    env_config['BASELINE_FLAG'] = True\n",
    "    metrics_central, reward_central = test(env_class, env_config, policy_mapping_fn, agent)\n",
    "    env_config['BASELINE_FLAG'] = False\n",
    "    \n",
    "    # ASSERT that both optimal values are equal\n",
    "    #assert metrics_marl['optimal'] == metrics_central['optimal']\n",
    "    \n",
    "    return {\"M_\" + str(seed): metrics_marl, \"C_\" + str(seed): metrics_central}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86f07bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-03-01 08:27:25 (running for 00:00:00.22)<br>Memory usage on this node: 616.1/1007.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/96 CPUs, 0/8 GPUs, 0.0/276.27 GiB heap, 0.0/122.39 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /work-ceph/albecker/Exp_thesis/Exp_environment/small_yes<br>Number of trials: 2/2 (2 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-01 08:27:25,805\tINFO tune.py:636 -- Total run time: 0.57 seconds (0.00 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model in checkpoint /work-ceph/albecker/Exp_thesis/Exp_environment/small_yes/PPOTrainer_Map_Environment_b02e4_00000_0_2022-02-21_00-06-09/checkpoint_006000/checkpoint-6000 - achieved episode_reward_mean: 84.4710000000029\n",
      "Training finished - Checkpoint:  /work-ceph/albecker/Exp_thesis/Exp_environment/small_yes/PPOTrainer_Map_Environment_b02e4_00000_0_2022-02-21_00-06-09/checkpoint_006000/checkpoint-6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-01 08:27:31,009\tINFO trainer.py:2054 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2022-03-01 08:27:31,012\tWARNING ppo.py:223 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=32 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 125.\n",
      "2022-03-01 08:27:31,014\tINFO ppo.py:249 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-03-01 08:27:31,015\tINFO trainer.py:790 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358337)\u001b[0m 2022-03-01 08:27:48,403\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358319)\u001b[0m 2022-03-01 08:27:48,506\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358322)\u001b[0m 2022-03-01 08:27:48,755\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358332)\u001b[0m 2022-03-01 08:27:48,951\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358323)\u001b[0m 2022-03-01 08:27:49,079\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358316)\u001b[0m 2022-03-01 08:27:49,210\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358325)\u001b[0m 2022-03-01 08:27:49,273\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358333)\u001b[0m 2022-03-01 08:27:49,381\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358310)\u001b[0m 2022-03-01 08:27:49,531\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358317)\u001b[0m 2022-03-01 08:27:49,566\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358312)\u001b[0m 2022-03-01 08:27:49,880\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358339)\u001b[0m 2022-03-01 08:27:50,021\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358313)\u001b[0m 2022-03-01 08:27:49,996\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358243)\u001b[0m 2022-03-01 08:27:50,067\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358327)\u001b[0m 2022-03-01 08:27:50,095\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358315)\u001b[0m 2022-03-01 08:27:50,303\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358318)\u001b[0m 2022-03-01 08:27:50,242\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358336)\u001b[0m 2022-03-01 08:27:50,612\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358236)\u001b[0m 2022-03-01 08:27:50,622\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358331)\u001b[0m 2022-03-01 08:27:50,919\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358286)\u001b[0m 2022-03-01 08:27:51,018\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358233)\u001b[0m 2022-03-01 08:27:51,042\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358238)\u001b[0m 2022-03-01 08:27:51,104\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358320)\u001b[0m 2022-03-01 08:27:51,114\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358311)\u001b[0m 2022-03-01 08:27:51,143\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358321)\u001b[0m 2022-03-01 08:27:51,356\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358306)\u001b[0m 2022-03-01 08:27:51,423\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358287)\u001b[0m 2022-03-01 08:27:51,466\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358314)\u001b[0m 2022-03-01 08:27:51,709\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358278)\u001b[0m 2022-03-01 08:27:51,809\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358335)\u001b[0m 2022-03-01 08:27:51,949\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3358232)\u001b[0m 2022-03-01 08:27:52,098\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-03-01 08:28:28,880\tINFO trainable.py:125 -- Trainable.setup took 57.872 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-03-01 08:28:28,897\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-03-01 08:28:29,398\tINFO trainable.py:472 -- Restored on 134.155.89.136 from checkpoint: /work-ceph/albecker/Exp_thesis/Exp_environment/small_yes/PPOTrainer_Map_Environment_b02e4_00000_0_2022-02-21_00-06-09/checkpoint_006000/checkpoint-6000\n",
      "2022-03-01 08:28:29,399\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 6000, '_timesteps_total': 24000000, '_time_total': 123913.14825487137, '_episodes_total': 20207}\n",
      "2022-03-01 08:28:29,404\tWARNING deprecation.py:45 -- DeprecationWarning: `compute_action` has been deprecated. Use `Trainer.compute_single_action()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent loaded - Agent:  PPOTrainer\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    return agent_id\n",
    "\n",
    "basic_config = {\n",
    "        # experiment\n",
    "        'NAME': 'small_yes',\n",
    "        'SAVE_DIR': 'Exp_environment',\n",
    "        'ALGO': PPOTrainer,\n",
    "        'ENV': Map_Environment,\n",
    "        'DEBUG_LOGS':False,\n",
    "        'NUM_GPUS': 2,\n",
    "        'NUM_WORKERS': 32,\n",
    "        'NUMBER_OF_SAMPLES': 2,\n",
    "        # Simulation config\n",
    "        'NUMBER_STEPS_PER_EPISODE': 1200,\n",
    "        'RANDOM_SEED': None, # 42\n",
    "        # Map\n",
    "        'TOPOLOGY': topology,\n",
    "        'CHARGING_STATION_NODES': [0,1,2,3,4,6,9],\n",
    "        # Entities\n",
    "        'NUMBER_OF_DRONES': 2,\n",
    "        'NUMBER_OF_CARS': 2,\n",
    "        'INIT_NUMBER_OF_PARCELS': 10,\n",
    "        'MAX_NUMBER_OF_PARCELS': 20,\n",
    "        'THRESHOLD_ADD_NEW_PARCEL': 0.1, # 10% chance\n",
    "        'MAX_BATTERY_POWER': 100, \n",
    "        #Baseline\n",
    "        'BASELINE_TIME_CONSTRAINT': 10,\n",
    "        'BASELINE_OPT_CONSTANT': 2.5,\n",
    "        #TESTING\n",
    "        'SEEDS': [72, 21, 44, 66, 86, 14],\n",
    "        #Rewards\n",
    "        'REWARDS': {\n",
    "            'PARCEL_DELIVERED': 200,\n",
    "            'STEP_PENALTY': -0.1,\n",
    "        }, \n",
    "        'STOP_CRITERIA': {\n",
    "            'timesteps_total': 24_000_000,\n",
    "        }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "print(\"Test evaluation functions\")\n",
    "experiment_results = train_and_test_scenarios(basic_config)\n",
    "print(\"Test evaluation finished ;)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c123aef",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Checkpoint-path:  /work-ceph/albecker/Exp_thesis/Exp_environment/small_yes/PPOTrainer_Map_Environment_b02e4_00000_0_2022-02-21_00-06-09/checkpoint_006000/checkpoint-6000\n",
    "experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86161a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "015fb616",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick \n",
    "    \n",
    "def create_chart_bars(results_dict):\n",
    "    \"\"\" Function that plots a bar graph with the duration of one episode \n",
    "        run with MARL agents/ Centrality Baseline/ Optimality Baseline, recorded in the :param results_dict:.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Design choices    \n",
    "    colors = {\n",
    "        \"marl\": 'blue',\n",
    "        \"central\": 'green',\n",
    "        \"optimal\": 'red'\n",
    "    }\n",
    "    \n",
    "    # Retrieve settings values from results dict\n",
    "    max_steps = results_dict['max_steps']\n",
    "    max_parcels = results_dict['max_parcels'] \n",
    "    max_agents = results_dict['max_agents']\n",
    "\n",
    "    # Deep copy for further computations\n",
    "    scenario_results = {k:v for k,v in results_dict.items() if k[0:3] != 'max'}\n",
    "    \n",
    "    # filter\n",
    "    runs = scenario_results.keys()\n",
    "    values = scenario_results.values()\n",
    "    \n",
    "    # TODO statt all dead => einfach not all delivered marker\n",
    "    merged = {}  # key is seed as str, value is a dict with [marl, central, optimal, all_dead_marl, all_dead_cent]\n",
    "  \n",
    "    # Retrieve the data\n",
    "    for run_id, res in scenario_results.items():\n",
    "        \n",
    "        split_id = run_id.split('_')  # --> type, seed\n",
    "        key_type, key_seed = split_id[0], split_id[1]\n",
    "        # merge data from the runs with same seed (marl + baselines)\n",
    "        \n",
    "        # add new dict if seed not encountered yet\n",
    "        if key_seed not in merged:\n",
    "            merged[key_seed] = {} \n",
    "        \n",
    "        _key_delivered = 'marl'\n",
    "        #_key_crashed = 'all_dead_marl'\n",
    "        if key_type == 'C':\n",
    "            # Baseline run\n",
    "            merged[key_seed]['optimal'] = res['optimal']\n",
    "            _key_delivered = 'central'\n",
    "            #_key_crashed = 'all_dead_central'\n",
    "\n",
    "        # Retrieve number of steps in run\n",
    "        last_step = res['step']\n",
    " \n",
    "        # old code for plotting the number of steps needed in the episode\n",
    "        merged[key_seed][_key_delivered] = last_step \n",
    "        merged[key_seed][_key_delivered + '_all'] = len(res['delivered'])\n",
    "        \n",
    "#        all_parcels_delivered = len(res['delivered']) == max_parcels     # were all parcels delivered\n",
    "#        merged[key_seed][_key_delivered + '_all'] = all_parcels_delivered\n",
    "#        if not all_parcels_delivered:\n",
    "#            merged[key_seed][_key_delivered] = max_steps \n",
    "            \n",
    "    print(\"Merged: \", merged)\n",
    "    \n",
    "    \n",
    "    #  example data = [[30, 25, 50, 20],\n",
    "#    [40, 23, 51, 17],\n",
    "#    [35, 22, 45, 19]]\n",
    "\n",
    "    data = [[],[],[],[], []]\n",
    "    labels = []\n",
    "    \n",
    "    # split data into type of run    \n",
    "    for seed,values in merged.items():\n",
    "        labels.append('S_'+seed)\n",
    "        data[0].append(values['marl'])\n",
    "        data[1].append(values['central'])\n",
    "        data[2].append(values['optimal'])\n",
    "        data[3].append(values['marl_all'])\n",
    "        data[4].append(values['central_all'])\n",
    "    \n",
    "    print(\"Data: \", data)\n",
    "    \n",
    "    X = np.arange(len(labels))\n",
    "    #print(X)\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "    #ax.bar(X + 0.00, data[2], color = colors['optimal'], label=\"Optimality Baseline\", width = 0.25)\n",
    "    ax.bar(X + 0.25, data[3], color = colors['marl'], label=\"MARL System\", width = 0.25, alpha=0.8)\n",
    "    ax.bar(X + 0.50, data[4], color = colors['central'], label=\"Centrality Baseline\", width = 0.25)\n",
    "    \n",
    "\n",
    "    plt.xlabel(\"Experiments\")\n",
    "    plt.ylabel(\"Parcels_delivered\")\n",
    "    plt.ylim(bottom=0, top=max_parcels)\n",
    "    \n",
    "    # y axis as percentage\n",
    "    yticks = mtick.PercentFormatter(max_parcels)\n",
    "    ax.yaxis.set_major_formatter(yticks)\n",
    "    \n",
    "    # Add experiment identifiers x-Axis\n",
    "    plt.xticks(X + 0.37, labels)\n",
    "    # Add legend\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5be69355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  [[1200, 1200, 1200, 1200, 1200], [1200, 1200, 1200, 1200, 1200], [178, 186, 173, 193, 191], [False, False, False, False, False], [False, False, False, False, False]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot duration bar graphs from the results\n",
    "create_chart_bars(experiment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3f82cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "\n",
    "def create_chart_episode_events(results_dict, draw_crashes = False):\n",
    "    \"\"\" Function that plots a graph with either the deliveries of parcels or the crashes of agents\n",
    "        over the course of an episode, recorded in the :param results_dict:.\n",
    "        Set the :param draw_crashes: flag for plotting crashes, default are deliveries.\n",
    "    \"\"\"\n",
    "    # TODO Graphen auch abspeichern oder nur hier anzeigen ??\n",
    "    # TODO - Parcel additions ??\n",
    "    # TODO - overthink fill with max_value for mean computation \n",
    "    \n",
    "    # Design choices    \n",
    "    colors = {\n",
    "        \"marl\": 'blue',\n",
    "        \"central\": 'green',\n",
    "        \"optimal\": 'red'\n",
    "    }\n",
    "    alpha = 0.6  # Opacity of individual value lines\n",
    "    alpha_opt = 0.2\n",
    "    opt_marker_size = 15\n",
    "    line_width_mean = 5\n",
    "    line_width_optimal = 2\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # Retrieve settings values from results dict\n",
    "    max_steps = results_dict['max_steps']\n",
    "    max_parcels = results_dict['max_parcels'] \n",
    "    max_agents = results_dict['max_agents']\n",
    "    \n",
    "    _len = max_agents if draw_crashes else max_parcels\n",
    "    _len += 1     # start plot at origin\n",
    "\n",
    "    _key = 'crashed' if draw_crashes else 'delivered'\n",
    "    \n",
    "    # Deep copy for further computations\n",
    "    scenario_results = {k:v for k,v in results_dict.items() if k[0:3] != 'max'}\n",
    "    \n",
    "    # for computation of mean\n",
    "    m_values, c_values, o_values = [], [], []\n",
    "    Y = [str(i) for i in range(0, _len)]\n",
    "\n",
    "    # iterate over configs\n",
    "    for scenario, results in scenario_results.items():\n",
    "\n",
    "        # Default settings -> MARL run\n",
    "        color = colors[\"marl\"]\n",
    "        _type = m_values\n",
    "        label= \"marl_system\"\n",
    "\n",
    "        if scenario[0] == 'C':\n",
    "            # Baseline run\n",
    "            color = colors[\"central\"]\n",
    "            _type = c_values\n",
    "            label = \"centrality_baseline\"\n",
    "            \n",
    "            # Retrieve and plot optimality baseline\n",
    "            optimal_time = results['optimal']\n",
    "            assert optimal_time is not None\n",
    "            if not draw_crashes: ax.plot(optimal_time, 0, \"*\", color = colors[\"optimal\"], label=\"optimality_baseline\", markersize=opt_marker_size, alpha= alpha_opt, clip_on=False)   \n",
    "            o_values.append(optimal_time)\n",
    "            \n",
    "        _num_steps = results['step']\n",
    "        \n",
    "        X = [0] + list(results[_key].values())\n",
    "        \n",
    "                \n",
    "        X = X + [max_steps]*(_len - len(X))           # Fill X up with max_step values for not delivered parcels / not crashed agents\n",
    " \n",
    "        _type.append(X)  # add X to the respective mean list\n",
    "        #Y = [str(i) for i in range(0, len(results[_key].values())+1)]\n",
    "        \n",
    "        #print(\"Data: \", results[_key].values())\n",
    "        #print(\"new X: \", X)\n",
    "        #print(\"new Y: \", Y)\n",
    "    \n",
    "        ax.step(X, Y, label=label, where='post', color=color, alpha=alpha)\n",
    "        \n",
    "        \n",
    "        # Attempt to improve the filling mess in the plot...         \n",
    "        #X = X + [max_steps]*(_len - len(X))           # Fill X up with max_step values for not delivered parcels / not crashed agents\n",
    " \n",
    "        #_type.append(X)  # add X to the respective mean list\n",
    "\n",
    "        \n",
    "    # compute mean values\n",
    "    m_mean = np.mean(np.array(m_values), axis=0)\n",
    "    c_mean = np.mean(np.array(c_values), axis=0)\n",
    "    o_mean = np.mean(np.array(o_values), axis=0)\n",
    "\n",
    "    \n",
    "    ax.step(m_mean, Y, label=\"marl_system\", where='post', color=colors[\"marl\"], linewidth=line_width_mean)\n",
    "    ax.step(c_mean, Y, label=\"centrality_baseline\", where='post', color=colors[\"central\"], linewidth=line_width_mean)\n",
    "\n",
    "    # star for opt:    if not draw_crashes: plt.plot(o_mean, 0, \"*\", label=\"optimality_baseline\", color=\"r\", markersize=opt_marker_size, clip_on=False)\n",
    "    # better?: vertical line for opt\n",
    "    if not draw_crashes: ax.axvline(o_mean, label=\"optimality_baseline\", color=colors[\"optimal\"], linewidth=2, alpha=alpha_opt+0.3)\n",
    "    \n",
    "    # y axis as percentage\n",
    "    max_percent = max_agents if draw_crashes else max_parcels\n",
    "    xticks = mtick.PercentFormatter(max_percent)\n",
    "    ax.yaxis.set_major_formatter(xticks)\n",
    "    \n",
    "    \n",
    "    # Lables and Legend\n",
    "    plt.xlabel(\"steps\")\n",
    "    ylabel = \"% of crashed agents\" if draw_crashes else \"% of delivered parcels\"\n",
    "    plt.ylabel(ylabel)\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys())\n",
    "    \n",
    "    # Margins\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.xlim()\n",
    "    plt.margins(x=0, y=0)\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9b300cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot delivery graphs from the results\n",
    "create_chart_episode_events(experiment_results, draw_crashes=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd51171e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot crash graphs from the results\n",
    "create_chart_episode_events(experiment_results, draw_crashes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e29c509",
   "metadata": {},
   "source": [
    "### Manual Actions for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efebf2d2",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "##################\n",
    "env_config = {\n",
    "            'DEBUG_LOGS':False,\n",
    "            'TOPOLOGY': topology,\n",
    "            # Simulation config\n",
    "            'NUMBER_STEPS_PER_EPISODE': 1000,\n",
    "            #'NUMBER_OF_TIMESTEPS': NUMBER_OF_TIMESTEPS,\n",
    "            'RANDOM_SEED': None, # 42\n",
    "            # Map\n",
    "            'CHARGING_STATION_NODES': [0,1,2,3,4],\n",
    "            # Entities\n",
    "            'NUMBER_OF_DRONES': 2,\n",
    "            'NUMBER_OF_CARS': 2,\n",
    "            'MAX_BATTERY_POWER': 100,  # TODO split this for drone and car??\n",
    "            'INIT_NUMBER_OF_PARCELS': 3,\n",
    "            'MAX_NUMBER_OF_PARCELS': 3,\n",
    "            'THRESHOLD_ADD_NEW_PARCEL': 0.01,\n",
    "            # Baseline settings\n",
    "            'BASELINE_FLAG': False,  # is set True in the test function when needed\n",
    "            'BASELINE_OPT_CONSTANT': 2.5,\n",
    "            'BASELINE_TIME_CONSTRAINT': 5,\n",
    "            # TODO \n",
    "            #Rewards\n",
    "            'REWARDS': {\n",
    "                'PARCEL_DELIVERED': 200,\n",
    "                'STEP_PENALTY': -0.1,\n",
    "            },  \n",
    "        }\n",
    "        \n",
    "\n",
    "\n",
    "env = Map_Environment(env_config)\n",
    "env.state\n",
    "#env.ACTION_DROPOFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5165d3bd",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO select actions to give agent 0 reward!!\n",
    "#print(env.action_space)\n",
    "\n",
    "\n",
    "actions_1 = {'d_0': 6, 'd_1': 2, 'c_2': 3, 'c_3': 4}\n",
    "#actions_1 = {'d_0': 0, 'd_1': 0, 'c_2': 5, 'c_3': 5}\n",
    "actions_2 = {'d_0': 2, 'd_1': 8, 'c_2': 7, 'c_3':0}\n",
    "actions_3 = {'d_0': 5, 'd_1': 5, 'c_2':2 }\n",
    "\n",
    "new_obs, rewards, dones, infos = env.step(actions_1)\n",
    "print(infos)\n",
    "print(f\"New Obs are: {new_obs}\")\n",
    "print(rewards)\n",
    "print(\"------------------\")\n",
    "new_obs2, rewards2, dones2, infos2 = env.step(actions_2)\n",
    "print(f\"New Obs are: {new_obs2}\")\n",
    "print(rewards2)\n",
    "print(\"------------------\")\n",
    "new_obs3, rewards3, dones3, infos3 = env.step(actions_3)\n",
    "print(f\"New Obs are: {new_obs3}\")\n",
    "print(rewards3)\n",
    "\n",
    "actions_4 = {'d_0': 0, 'd_1': 0, 'c_2': 1, 'c_3': 0}\n",
    "actions_5 = {'d_0': 0, 'd_1': 0, 'c_2': 0, 'c_3': 0}\n",
    "actions_6 = {'d_0': 0, 'd_1': 0, 'c_2': 5, 'c_3': 0}\n",
    "\n",
    "new_obs, rewards, dones, infos = env.step(actions_4)\n",
    "print(f\"New Obs are: {new_obs}\")\n",
    "print(rewards)\n",
    "print(\"------------------\")\n",
    "new_obs2, rewards2, dones2, infos2 = env.step(actions_5)\n",
    "print(f\"New Obs are: {new_obs2}\")\n",
    "print(rewards2)\n",
    "print(\"------------------\")\n",
    "new_obs3, rewards3, dones3, infos3 = env.step(actions_6)\n",
    "print(f\"New Obs are: {new_obs3}\")\n",
    "print(rewards3)\n",
    "\n",
    "print(\"------------------\")\n",
    "print(dones3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a143594",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TENSORBOARD\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "#Start tensorboard below the notebook\n",
    "#%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
