{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f838d2e",
   "metadata": {},
   "source": [
    "# MultiAgentEnvironment: simple Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a170d6",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful\n"
     ]
    }
   ],
   "source": [
    "#%autosave 30\n",
    "import glob\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms.shortest_paths.generic import shortest_path_length\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "#from ray.tune.logger import pretty_print\n",
    "from ray.rllib.policy.policy import Policy, PolicySpec\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.fcnet import FullyConnectedNetwork\n",
    "\n",
    "#from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.env import MultiAgentEnv\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "\n",
    "from gym.spaces import Discrete, Box, Tuple, MultiDiscrete, Dict, MultiBinary\n",
    "from ray.rllib.utils.spaces.repeated import Repeated\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf1, tf, tfv = try_import_tf()  # prefered TF import for Ray.io\n",
    "\n",
    "from threading import Thread, Event\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10fdf817",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "######## Utility Classes ########\n",
    "class BoolTimer(Thread):\n",
    "    \"\"\"A boolean value that toggles after a specified number of seconds:\n",
    "    \n",
    "    Example:\n",
    "        bt = BoolTimer(30.0, False)\n",
    "        bt.start()\n",
    "\n",
    "    Used in the Centrality Baseline to limit the computation time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, interval, initial_state=True):\n",
    "        Thread.__init__(self)\n",
    "        self.interval = interval\n",
    "        self.state = initial_state\n",
    "        self.finished = Event()\n",
    "\n",
    "    def __bool__(self):\n",
    "        return bool(self.state)\n",
    "\n",
    "    def run(self):\n",
    "        self.finished.wait(self.interval)\n",
    "        if not self.finished.is_set():\n",
    "            self.state = not self.state\n",
    "        self.finished.set()\n",
    "\n",
    "\n",
    "######## Static helper functions ########\n",
    "def shuffle_actions(action_dict, check_space = False):\n",
    "    \"\"\"\n",
    "        Used to shuffle the action dict to ensure that agents with a lower id are not always preferred\n",
    "        when picking up parcels over other agents that chose the same action.\n",
    "        For debugging: Can also be used to check if all actions are in the action_space.\n",
    "    \"\"\"\n",
    "    keys = list(action_dict)\n",
    "    random.shuffle(keys)\n",
    "    shuffled = {}\n",
    "\n",
    "    for agent in keys:\n",
    "        if check_space:  #assert actions are in action space -> disable for later trainig, extra check while development\n",
    "            assert self.action_space.contains(action_dict[agent]),f\"Action {action_dict[agent]} taken by agent {agent} not in action space\"\n",
    "        shuffled[agent] = action_dict[agent]\n",
    "\n",
    "    return shuffled\n",
    "\n",
    "\n",
    "\n",
    "def load_graph(data):\n",
    "    \"\"\"Loads topology (map) from json file into a networkX Graph and returns the graph\"\"\"\n",
    "\n",
    "    nodes = data[\"nodes\"]\n",
    "    edges = data[\"edges\"]\n",
    "\n",
    "    g = nx.DiGraph()  # directed graph\n",
    "    g.add_nodes_from(nodes)\n",
    "    g.edges(data=True)\n",
    "\n",
    "   \n",
    "    for node in nodes:  # add attribute values\n",
    "        g.nodes[node][\"id\"] = nodes[node][\"id\"]\n",
    "        g.nodes[node][\"type\"] = nodes[node][\"type\"]\n",
    "\n",
    "    for edge in edges:  # add edges with attributes\n",
    "        f = edges[edge][\"from\"]\n",
    "        t = edges[edge][\"to\"]\n",
    "        \n",
    "        weight_road, weight_air, _type = sys.maxsize, sys.maxsize, None\n",
    "        \n",
    "        if edges[edge][\"road\"] >= 0:\n",
    "            weight_road = edges[edge][\"road\"]\n",
    "            _type = 'road'\n",
    "        if edges[edge][\"air\"] >= 0:\n",
    "            weight_air = edges[edge][\"air\"]\n",
    "            _type = 'both' if _type == 'road' else 'air'\n",
    "\n",
    "        weight = min(weight_road, weight_air)  # needed for optimality baseline\n",
    "        \n",
    "        g.add_edge(f, t, type=_type, road= weight_road, air=weight_air, weight=weight)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663b334",
   "metadata": {},
   "source": [
    "## Environment Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f835bd0",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Map definition\n",
    "topology = {\n",
    "    'nodes': {\n",
    "            0: {'id': 1, 'type': 'parking'},\n",
    "            1: {'id': 1, 'type': 'parking'},\n",
    "            2: {'id': 2, 'type': 'parking'},\n",
    "            3: {'id': 3, 'type': 'parking'},\n",
    "            4: {'id': 4, 'type': 'parking'},\n",
    "            5: {'id': 5, 'type': 'air'},\n",
    "            6: {'id': 6, 'type': 'parking'},\n",
    "            7: {'id': 7, 'type': 'air'},\n",
    "            8: {'id': 8, 'type': 'parking'},\n",
    "            9: {'id': 9, 'type': 'parking'},\n",
    "            10: {'id': 10, 'type': 'parking'},\n",
    "            11: {'id': 11, 'type': 'air'}\n",
    "    },\n",
    "    'edges': {\n",
    "    ## Outer Ring    --> Road much (!) faster than drones\n",
    "    \"e01\":{\"from\": 0,\"to\": 1,\"road\": 10, \"air\": 40},\n",
    "    \"e02\":{\"from\": 1,\"to\": 0,\"road\": 10, \"air\": 40},\n",
    "    \"e03\":{\"from\": 1,\"to\": 4,\"road\": 8, \"air\": 6},        \n",
    "    \"e04\":{\"from\": 4,\"to\": 1,\"road\": 8, \"air\": 6},\n",
    "    \"e03\":{\"from\": 4,\"to\": 2,\"road\": 8, \"air\": 6},        \n",
    "    \"e04\":{\"from\": 2,\"to\": 4,\"road\": 8, \"air\": 6},\n",
    "    \"e05\":{\"from\": 2,\"to\": 3,\"road\": 10, \"air\": 30},\n",
    "    \"e06\":{\"from\": 3,\"to\": 2,\"road\": 10, \"air\": 30},\n",
    "    \"e07\":{\"from\": 3,\"to\": 0,\"road\": 10, \"air\": 40},\n",
    "    \"e08\":{\"from\": 0,\"to\": 3,\"road\": 8, \"air\": 35}, \n",
    "    ## Inner Nodes  --> Reinfahren langsamer als rausfahren\n",
    "    ##              --> \n",
    "    \"e11\":{\"from\": 4,\"to\": 6,\"road\": 2, \"air\": 5},        \n",
    "    \"e12\":{\"from\": 6,\"to\": 4,\"road\": 2, \"air\": 5},\n",
    "    \"e13\":{\"from\": 0,\"to\": 6,\"road\": 6, \"air\": -1},\n",
    "    \"e14\":{\"from\": 6,\"to\": 0,\"road\": 6, \"air\": -1},\n",
    "    \"e15\":{\"from\": 3,\"to\": 6,\"road\": 6, \"air\": -1},\n",
    "    \"e16\":{\"from\": 6,\"to\": 3,\"road\": 6, \"air\": -1},   \n",
    "    ## Outliers   --> Distance about equal if both exist!\n",
    "    \"e17\":{\"from\": 4,\"to\": 5,\"road\": 2, \"air\": 2},        \n",
    "    \"e18\":{\"from\": 5,\"to\": 4,\"road\": 2, \"air\": 2},\n",
    "    \"e19\":{\"from\": 6,\"to\": 7,\"road\": -1, \"air\": 2},\n",
    "    \"e20\":{\"from\": 7,\"to\": 6,\"road\": -1, \"air\": 2},\n",
    "    \"e21\":{\"from\": 3,\"to\": 11,\"road\": -1, \"air\": 3},\n",
    "    \"e22\":{\"from\": 11,\"to\": 3,\"road\": -1, \"air\": 3},     \n",
    "    ## Upper left square  --> Hier besser mit den Drohnen agieren, Falls Sie da sind!!\n",
    "    \"e23\":{\"from\": 0,\"to\": 8,\"road\": -1, \"air\": 4},        \n",
    "    \"e24\":{\"from\": 8,\"to\": 0,\"road\": -1, \"air\": 4},\n",
    "    \"e25\":{\"from\": 8,\"to\": 9,\"road\": 3, \"air\": 3},\n",
    "    \"e26\":{\"from\": 9,\"to\": 8,\"road\": 3, \"air\": 3},\n",
    "    \"e27\":{\"from\": 9,\"to\": 10,\"road\": 3, \"air\": 3},\n",
    "    \"e28\":{\"from\": 10,\"to\": 9,\"road\": 3, \"air\": 3},\n",
    "    \"e29\":{\"from\": 0,\"to\": 10,\"road\": 3, \"air\": 3},\n",
    "    \"e30\":{\"from\": 10,\"to\": 0,\"road\": 3, \"air\": 3}  \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6ccbe98",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class Map_Environment(MultiAgentEnv):\n",
    "    \n",
    "    def __init__(self, env_config: dict = {}):\n",
    "        # ensure config file includes all necessary settings\n",
    "        assert 'NUMBER_STEPS_PER_EPISODE' in env_config\n",
    "        assert 'NUMBER_OF_DRONES' in env_config\n",
    "        assert 'NUMBER_OF_CARS' in env_config\n",
    "        assert 'INIT_NUMBER_OF_PARCELS' in env_config\n",
    "        assert 'TOPOLOGY' in env_config\n",
    "        assert 'MAX_NUMBER_OF_PARCELS' in env_config\n",
    "        assert 'THRESHOLD_ADD_NEW_PARCEL' in env_config\n",
    "        assert 'BASELINE_FLAG' in env_config\n",
    "        assert 'BASELINE_TIME_CONSTRAINT' in env_config\n",
    "        assert 'BASELINE_OPT_CONSTANT' in env_config\n",
    "        assert 'CHARGING_STATION_NODES' in env_config\n",
    "        assert 'REWARDS' in env_config\n",
    "        \n",
    "        self.graph = load_graph(topology)\n",
    "        \n",
    "\n",
    "        # Map config\n",
    "        self.NUMBER_OF_DRONES = env_config['NUMBER_OF_DRONES']\n",
    "        self.NUMBER_OF_CARS = env_config['NUMBER_OF_CARS']\n",
    "        self.NUMBER_OF_EDGES = self.graph.number_of_edges()\n",
    "        self.NUMBER_OF_NODES = self.graph.number_of_nodes()\n",
    "        self.CHARGING_STATION_NODES = env_config['CHARGING_STATION_NODES']           \n",
    "        self.MAX_BATTERY_POWER = env_config['MAX_BATTERY_POWER']\n",
    "       \n",
    "\n",
    "        # Simulation config\n",
    "        self.NUMBER_STEPS_PER_EPISODE = env_config['NUMBER_STEPS_PER_EPISODE']\n",
    "        self.INIT_NUMBER_OF_PARCELS = env_config['INIT_NUMBER_OF_PARCELS']\n",
    "        self.RANDOM_SEED = env_config.get('RANDOM_SEED', None)\n",
    "        self.MAX_NUMBER_OF_PARCELS = env_config['MAX_NUMBER_OF_PARCELS']\n",
    "        self.THRESHOLD_ADD_NEW_PARCEL = env_config['THRESHOLD_ADD_NEW_PARCEL']\n",
    "        self.BASELINE_FLAG = env_config['BASELINE_FLAG']\n",
    "        self.BASELINE_TIME_CONSTRAINT = env_config['BASELINE_TIME_CONSTRAINT']\n",
    "        self.BASELINE_OPT_CONSTANT = env_config['BASELINE_OPT_CONSTANT']\n",
    " \n",
    "        self.DEBUG_LOG = env_config.get('DEBUG_LOGS', False)\n",
    "        # Some Sanity Checks on the settings\n",
    "        if self.DEBUG_LOG: assert self.MAX_NUMBER_OF_PARCELS >= self.INIT_NUMBER_OF_PARCELS, \"Number of initial parcels exceeds max parcel limit\"\n",
    "\n",
    "        #Reward constants\n",
    "        self.STEP_PENALTY = env_config['REWARDS']['STEP_PENALTY']\n",
    "        self.PARCEL_DELIVERED = env_config['REWARDS']['PARCEL_DELIVERED']\n",
    "        # compute other rewards\n",
    "        self.BATTERY_DIED = self.STEP_PENALTY * self.NUMBER_STEPS_PER_EPISODE\n",
    "        self.BATTERY_DIED_WITH_PARCEL = self.BATTERY_DIED * 2\n",
    "        #self.DELIVERY_CONTRIBUTION: depends on active agents --> computed when given in prepare_global_reward()\n",
    "        #self.ALL_DELIVERED_CONTRIB: depends on active agents --> computed when given in prepare_global_reward(episode_success=True)\n",
    "\n",
    "        # Computed constants\n",
    "        self.NUMBER_OF_AGENTS = self.NUMBER_OF_DRONES + self.NUMBER_OF_CARS\n",
    "        self.PARCEL_STATE_DELIVERED = self.NUMBER_OF_AGENTS + self.NUMBER_OF_NODES\n",
    "        self.NUMBER_OF_ACTIONS = 1 + self.NUMBER_OF_NODES + 1 + self.MAX_NUMBER_OF_PARCELS\n",
    "        self.ACTION_DROPOFF = 1 + self.NUMBER_OF_NODES     # First Action NOOP is 0\n",
    "    \n",
    "        # seed RNGs\n",
    "        self.seed(self.RANDOM_SEED) \n",
    "        \n",
    "        self.state = None  \n",
    "        self.current_step = None\n",
    "        self.blocked_agents = None\n",
    "        self.parcels_delivered = None\n",
    "        self.done_agents = None\n",
    "        self.all_done = None  \n",
    "        self.allowed_actions = None\n",
    "        \n",
    "        # baseline related \n",
    "        self.baseline_missions = None\n",
    "        self.agents_base = None\n",
    "        self.o_employed = None\n",
    "        \n",
    "        # metrics for the evaluation\n",
    "        self.parcel_delivered_steps = None # --> {p1: 20, p2: 240, p:140}\n",
    "        self.parcel_added_steps = None     # --> {p1: 0, p2: 0, p: 50}\n",
    "        self.agents_crashed = None         # --> {c_2: 120, d_0: 242} \n",
    "        self.metrics = None\n",
    "        \n",
    "        self.agents = [*[\"d_\" + str(i) for i in range(self.NUMBER_OF_DRONES)],*[\"c_\" + str(i) for i in range(self.NUMBER_OF_DRONES, self.NUMBER_OF_DRONES + self.NUMBER_OF_CARS)]]\n",
    "        \n",
    "        # Define observation and action spaces for individual agents\n",
    "        self.action_space = Discrete(self.NUMBER_OF_ACTIONS)\n",
    "\n",
    "        #---- Repeated Obs Space: Represents a parcel with (id, location, destination)  --> # parcel_id starts at 1 \n",
    "        parcel_space = Dict({'id': Discrete(self.MAX_NUMBER_OF_PARCELS+1),\n",
    "                             'location': Discrete(self.NUMBER_OF_NODES + self.NUMBER_OF_AGENTS + 1),\n",
    "                             'destination': Discrete(self.NUMBER_OF_NODES)\n",
    "                            })\n",
    "        self.observation_space = Dict({'obs': Dict({'state': \n",
    "                                                 Dict({ 'position': Discrete(self.NUMBER_OF_NODES),\n",
    "                                                        'battery': Discrete(self.MAX_BATTERY_POWER + 1), #[0-100]\n",
    "                                                        'has_parcel': Discrete(self.MAX_NUMBER_OF_PARCELS + 1),\n",
    "                                                        'current_step': Discrete(self.NUMBER_STEPS_PER_EPISODE + 1)}), \n",
    "                                                 'parcels': Repeated(parcel_space, max_len=self.MAX_NUMBER_OF_PARCELS)\n",
    "                                                }),\n",
    "                                        'allowed_actions': MultiBinary(self.NUMBER_OF_ACTIONS)\n",
    "                                      }) \n",
    "        #TODO: why is reset() not called by env? \n",
    "        self.reset() \n",
    "        \n",
    "    def step(self, action_dict):\n",
    "        \"\"\"conduct the state transitions caused by actions in action_dict\n",
    "        :returns:\n",
    "            - observation_dict: observations for agents that need to act in the next round\n",
    "            - rewards_dict: rewards for agents following their chosen actions\n",
    "            - done_dict: indicates end of episode if max_steps reached or all parcels delivered\n",
    "            - info_dict: pass data to custom logger\n",
    "        \"\"\"\n",
    "\n",
    "        if self.DEBUG_LOG: print(f\"Debug log flag set to {self.DEBUG_LOG}\")\n",
    "        \n",
    "        # ensure no disadvantage for agents with higher IDs if action conflicts with that taken by other agent\n",
    "        action_dict = shuffle_actions(action_dict)\n",
    "    \n",
    "        self.current_step += 1\n",
    "    \n",
    "        # grant step penalty reward    \n",
    "        agent_rewards = {agent: self.STEP_PENALTY for agent in self.agents}  \n",
    "        \n",
    "        # setting an agent done twice might cause crash when used with tune... -> https://github.com/ray-project/ray/issues/10761\n",
    "        dones = {}\n",
    "        \n",
    "        self.metrics['step'] = self.current_step\n",
    "\n",
    "        # dynamically add parcel\n",
    "        if random.random() <= self.THRESHOLD_ADD_NEW_PARCEL and len(self.state['parcels']) < self.MAX_NUMBER_OF_PARCELS:\n",
    " \n",
    "            p_id, parcel = self.generate_parcel()\n",
    "            assert p_id not in self.state['parcels'], \"Duplicate parcel ID generated\"\n",
    "            self.state['parcels'][p_id] = parcel\n",
    "\n",
    "            if self.BASELINE_FLAG:\n",
    "                self.metrics[\"optimal\"] = self.compute_optimality_baseline(p_id, extra_charge=self.BASELINE_OPT_CONSTANT)\n",
    "                \n",
    "            for agent in self.agents:\n",
    "                self.allowed_actions[agent][self.ACTION_DROPOFF + p_id] = np.array([1]).astype(bool)\n",
    "            \n",
    "        if self.BASELINE_FLAG:\n",
    "            old_actions = action_dict\n",
    "            action_dict = {}\n",
    "            # Replace actions with actions recommended by the central baseline\n",
    "            for agent, action in old_actions.items():\n",
    "                if len(self.baseline_missions[agent]) > 0:\n",
    "                    new_action = self.baseline_missions[agent][0]\n",
    "\n",
    "                    if type(new_action) is tuple: # dropoff action with minimal time\n",
    "\n",
    "                        if self.current_step >= new_action[1]:\n",
    "                            new_action = new_action[0]\n",
    "                            self.baseline_missions[agent].pop(0)\n",
    "                        else: #agent has to wait for previous subroute agent\n",
    "                            new_action = 0\n",
    "                    else: # move or pickup or charge\n",
    "                        self.baseline_missions[agent].pop(0)\n",
    "                            \n",
    "                    action_dict[agent] = new_action\n",
    "\n",
    "                else: # agent has no baseline mission -> Noop\n",
    "                    action_dict[agent] = 0\n",
    "                \n",
    "        # carry out State Transition\n",
    "        \n",
    "        # handel NOP actions: -> action == 0 \n",
    "        noop_agents =  {agent: action for agent, action in action_dict.items() if action == 0}\n",
    "        effectual_agents_items = {agent: action for agent, action in action_dict.items() if action > 0}.items()\n",
    "        \n",
    "        # now: transaction between agents modelled as pickup of just offloaded (=dropped) parcel --> handle dropoff first\n",
    "        moving_agents = {agent: action for agent, action in effectual_agents_items if 0 < action and action <= self.NUMBER_OF_NODES}\n",
    "        dropoff_agents = {agent: action for agent, action in effectual_agents_items if action == self.ACTION_DROPOFF}\n",
    "        pickup_agents = {agent: action for agent, action in effectual_agents_items if action > self.ACTION_DROPOFF}\n",
    "\n",
    "        # handle noop / charge decisions:\n",
    "        for agent, action in noop_agents.items():\n",
    "            # check if recharge is possible\n",
    "            current_pos = self.state['agents'][agent]['position']\n",
    "            if current_pos in self.CHARGING_STATION_NODES:\n",
    "                self.state['agents'][agent]['battery'] = self.MAX_BATTERY_POWER\n",
    "\n",
    "        # handle Movement actions:\n",
    "        for agent, action in moving_agents.items():\n",
    "            # get Current agent position from state\n",
    "            self.state['agents'][agent]['battery'] += -1\n",
    "            current_pos = self.state['agents'][agent]['position']\n",
    "            \n",
    "            # networkX: use node instead of edge:\n",
    "            destination = action - 1\n",
    "            \n",
    "            if self.graph.has_edge(current_pos, destination):\n",
    "                # Agent chose existing edge! -> check if type is suitable\n",
    "              \n",
    "                agent_type = 'road' if agent[:1] == 'c' else 'air'\n",
    "                if self.graph[current_pos][destination][\"type\"] in [agent_type, 'both']:\n",
    "                    # Edge has correct type\n",
    "                    self.state['agents'][agent]['position'] = destination\n",
    "                    self.state['agents'][agent]['battery'] += -(self.graph[current_pos][destination][agent_type] +1)\n",
    "                    if self.state['agents'][agent]['battery'] < 0:        # ensure negative battery value does not break obs_space\n",
    "                        #Battery below 0 --> reset to 0 (stay in obs space)\n",
    "                        self.state['agents'][agent]['battery'] = 0\n",
    "                    \n",
    "                    self.blocked_agents[agent] = self.graph[current_pos][destination][agent_type]\n",
    "                    self.update_allowed_actions_nodes(agent)\n",
    "\n",
    "                    \n",
    "        # handle Dropoff Decision: -> action == self.NUMBER_OF_NODES + 2\n",
    "        for agent, action in dropoff_agents.items():\n",
    "            self.state['agents'][agent]['battery'] += -1\n",
    "\n",
    "            if self.state['agents'][agent]['has_parcel'] > 0:   # agent has parcel\n",
    "                parcel_id = self.state['agents'][agent]['has_parcel']\n",
    "                self.state['agents'][agent]['has_parcel'] = 0\n",
    "                self.state['parcels'][parcel_id][0] = self.state['agents'][agent]['position']\n",
    "                if self.state['parcels'][parcel_id][0] ==  self.state['parcels'][parcel_id][1]:\n",
    "                    # Delivery successful\n",
    "                    \n",
    "                    agent_rewards[agent] += self.PARCEL_DELIVERED  # local reward\n",
    "                    # global contribution rewards\n",
    "                    active_agents, reward = self.prepare_global_reward()\n",
    "                    for a_id in active_agents: agent_rewards[a_id] += reward\n",
    "                    \n",
    "                    self.state['parcels'][parcel_id][0] = self.PARCEL_STATE_DELIVERED\n",
    "                    self.parcels_delivered[int(parcel_id) -1] = True                    # Parcel_ids start at 1\n",
    "\n",
    "                    self.metrics['delivered'].update({\"p_\" + str(parcel_id): self.current_step})\n",
    "\n",
    "                self.update_allowed_actions_parcels(agent)\n",
    "          \n",
    "        \n",
    "        # handle Pickup Decision:\n",
    "        for agent, action in pickup_agents.items():\n",
    "            self.state['agents'][agent]['battery'] += -1 \n",
    "    \n",
    "            # agent has free capacity\n",
    "            if self.state['agents'][agent]['has_parcel'] == 0:  #free parcel capacity\n",
    "                # convert action_id to parcel_id\n",
    "                parcel_id = action - self.ACTION_DROPOFF\n",
    "\n",
    "                if self.DEBUG_LOG: assert parcel_id in self.state['parcels'] # parcel {parcel_id} already in ENV ??\n",
    "                    \n",
    "                elif self.state['parcels'][parcel_id][0] == self.state['agents'][agent]['position']:\n",
    "                    # Successful pickup operation\n",
    "                    self.state['parcels'][parcel_id][0] = self.NUMBER_OF_NODES + int(agent[2:])\n",
    "                    self.state['agents'][agent]['has_parcel'] = int(parcel_id)\n",
    "                    \n",
    "                    self.update_allowed_actions_parcels(agent)\n",
    "                        \n",
    "        # unblock agents for next round \n",
    "        self.blocked_agents = {agent: remaining_steps -1 for agent, remaining_steps in self.blocked_agents.items() if remaining_steps > 1}\n",
    "\n",
    "        # handle dones - out of battery or max_steps or goal reached\n",
    "        for agent in action_dict.keys():\n",
    "            if agent not in self.done_agents and self.state['agents'][agent]['battery'] <= 0:\n",
    "                agent_rewards[agent] = self.BATTERY_DIED_WITH_PARCEL if self.state['agents'][agent]['has_parcel'] != 0 else self.BATTERY_DIED\n",
    "                dones[agent] = True\n",
    "                self.done_agents.append(agent)\n",
    "                \n",
    "                self.metrics['crashed'].update({agent: self.current_step})\n",
    "                \n",
    "                if len(self.done_agents) == self.NUMBER_OF_AGENTS:\n",
    "                    # all agents dead\n",
    "                    self.all_done = True\n",
    "        \n",
    "        # check if episode terminated because of goal reached or all agents crashed -> avoid setting done twice\n",
    "        if self.current_step >= self.NUMBER_STEPS_PER_EPISODE or (all(self.parcels_delivered) and len(self.parcels_delivered) == self.MAX_NUMBER_OF_PARCELS):\n",
    "            # check if episode success:\n",
    "            if self.current_step < self.NUMBER_STEPS_PER_EPISODE:\n",
    "                    # grant global reward for all parcels delivered \n",
    "                    active_agents, reward = self.prepare_global_reward(_episode_success=True)\n",
    "                    for a_id in active_agents: agent_rewards[a_id] += reward\n",
    "            \n",
    "            self.all_done = True\n",
    "\n",
    "        dones['__all__'] = self.all_done\n",
    "\n",
    "        \n",
    "        parcel_obs = self.get_parcel_obs()\n",
    "    \n",
    "        # obs \\ rewards \\ dones\\ info\n",
    "        return  {agent: { 'obs': {'state': {'position': self.state['agents'][agent]['position'], 'battery': self.state['agents'][agent]['battery'],\n",
    "                                    'has_parcel': self.state['agents'][agent]['has_parcel'],'current_step': self.current_step},\n",
    "                                    'parcels': parcel_obs},\n",
    "                         'allowed_actions': self.allowed_actions[agent]} for agent in self.agents if agent not in self.blocked_agents and agent not in self.done_agents}, \\\n",
    "                { agent: agent_rewards[agent] for agent in self.agents}, \\\n",
    "                dones, \\\n",
    "                {}\n",
    "\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)  \n",
    "\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"resets variables; returns dict with observations, keys are agent_ids\"\"\"\n",
    "        self.current_step = 0\n",
    "        self.blocked_agents = {}\n",
    "        self.parcels_delivered = [False for _ in range(self.MAX_NUMBER_OF_PARCELS)]\n",
    "        self.done_agents = []\n",
    "        self.all_done = False\n",
    "        \n",
    "        self.metrics = { \"step\": self.current_step,\n",
    "            \"delivered\": {},\n",
    "            \"crashed\": {},\n",
    "            \"added\": {},\n",
    "            \"optimal\": None\n",
    "        }\n",
    "\n",
    "        #Baseline\n",
    "        self.baseline_missions = {agent: [] for agent in self.agents}\n",
    "        self.o_employed = [0 for _ in range(self.NUMBER_OF_AGENTS)]\n",
    "\n",
    "        self.agents_base = None  # env.agents in the pseudocode\n",
    "        self.allowed_actions = { agent: np.array([1 for act in range(self.NUMBER_OF_ACTIONS)]) for agent in self.agents}\n",
    " \n",
    "        #Reset State\n",
    "        self.state = {'agents': {},\n",
    "                      'parcels': {}}\n",
    "        #generate initial parcels\n",
    "        for _ in range(self.INIT_NUMBER_OF_PARCELS):\n",
    "            p_id, parcel = self.generate_parcel()\n",
    "            self.state['parcels'][p_id] = parcel            \n",
    "\n",
    "        parcel_obs = self.get_parcel_obs()\n",
    "        \n",
    "        # init agents\n",
    "        self.state['agents'] = {agent: {'position': self._random_feasible_agent_position(agent),\n",
    "                                        'battery': self.MAX_BATTERY_POWER, 'has_parcel': 0} for agent in self.agents}\n",
    "        \n",
    "        if self.BASELINE_FLAG:\n",
    "            for parcel in self.state['parcels']:\n",
    "                self.compute_central_delivery(parcel, debug_log=False)\n",
    "                \n",
    "                # TODO really return something here??\n",
    "                self.metrics['optimal'] = self.compute_optimality_baseline(parcel, extra_charge=self.BASELINE_OPT_CONSTANT, debug_log=False)\n",
    "        \n",
    "        # compute allowed actions per agent --> move to function\n",
    "        for agent in self.agents:\n",
    "            self.update_allowed_actions_nodes(agent)\n",
    "            self.update_allowed_actions_parcels(agent)    \n",
    "            \n",
    "        agent_obs = {agent: {'obs': {'state':  {'position': state['position'], 'battery': state['battery'],\n",
    "                                        'has_parcel': state['has_parcel'],'current_step': self.current_step},\n",
    "                            'parcels': parcel_obs\n",
    "                            },\n",
    "                            'allowed_actions': self.allowed_actions[agent]\n",
    "                            } for agent, state in self.state['agents'].items()\n",
    "                    } \n",
    "        \n",
    "        return {**agent_obs}\n",
    "    \n",
    "    \n",
    "    def _random_feasible_agent_position(self, agent_id):\n",
    "        \"\"\"Needed to avoid car agents being initialized at nodes only reachable by drones\n",
    "        and thus trapped from the beginning. Ensures that car agents start at a node of type 'road' or 'parking'.\n",
    "        \"\"\"\n",
    "        position = random.randrange(self.NUMBER_OF_NODES)\n",
    "        if agent_id[0] == 'c':    # agent is car \n",
    "            while self.graph.nodes[position]['type'] == 'air':      # position not reachable by car\n",
    "                position = random.randrange(self.NUMBER_OF_NODES)\n",
    "        return position\n",
    "    \n",
    "    \n",
    "    def update_allowed_actions_nodes(self, agent):\n",
    "        new_pos = self.state['agents'][agent]['position']\n",
    "        next_steps = list(self.graph.neighbors(new_pos))\n",
    "        agent_type = 'air' if agent[0]=='d' else 'road'\n",
    "        \n",
    "        allowed_nodes = np.zeros(self.NUMBER_OF_NODES)\n",
    "        for neighbor in next_steps:\n",
    "            if self.graph[new_pos][neighbor]['type'] in [agent_type, 'both']:\n",
    "                allowed_nodes[neighbor] = 1\n",
    "        self.allowed_actions[agent][1:self.NUMBER_OF_NODES+1] = np.array(allowed_nodes).astype(bool)\n",
    "                 \n",
    "            \n",
    "    def update_allowed_actions_parcels(self, agent):\n",
    "        \"\"\" Allow only the Dropoff or Pickup actions, depending on the has_parcel value of the agent.\n",
    "        Pickup is not concerned with the parcel actually being at the agents current location, only with free capacity\n",
    "        and the parcel already being added to the ENV\"\"\"\n",
    "        num_parcels = len(self.state['parcels'])\n",
    "                          \n",
    "        allowed_parcels = np.zeros(self.MAX_NUMBER_OF_PARCELS)\n",
    "        dropoff = 1\n",
    "        if self.state['agents'][agent]['has_parcel'] == 0:\n",
    "            dropoff = 0\n",
    "            allowed_parcels = np.concatenate([np.ones(num_parcels), np.zeros(self.MAX_NUMBER_OF_PARCELS - num_parcels)])\n",
    "                          \n",
    "        self.allowed_actions[agent][self.NUMBER_OF_NODES+1:] = np.array([dropoff, *allowed_parcels]).astype(bool)\n",
    "    \n",
    "    \n",
    "    def get_parcel_obs(self):\n",
    "        parcel_obs = [{'id':pid, 'location': parcel[0], 'destination':parcel[1]} for (pid, parcel) in self.state['parcels'].items()]\n",
    "        return parcel_obs\n",
    "    \n",
    "    def generate_parcel(self):\n",
    "        \"\"\"generate new parcel id and new parcel with random nodes for location and destination.\n",
    "            p_ids (int) start at 1, later nodes for parcel will be sampled to avoid parcels that already spawn at their destination\"\"\"\n",
    "        p_id = len(self.state['parcels']) + 1\n",
    "\n",
    "        parcel = random.sample(range(self.NUMBER_OF_NODES), 2)  # => initial location != destination\n",
    "        self.metrics['added'].update({p_id: self.current_step})\n",
    "        \n",
    "        return p_id, parcel\n",
    "    \n",
    "    def prepare_global_reward(self, _episode_success=False):\n",
    "        \"\"\" computes a global reward for all active agents still in the environment.\n",
    "            If _episode_success is set to True, all parcels have been delivered an the ALL_DELIVERED reward is granted.\n",
    "            :Returns: list of active agents and the reward value\n",
    "        \"\"\"\n",
    "        agents_alive = list(set(self.agents).difference(set(self.done_agents)))\n",
    "        if self.DEBUG_LOG: assert len(agents_alive) > 0\n",
    "        reward = self.PARCEL_DELIVERED * (self.NUMBER_STEPS_PER_EPISODE - self.current_step) / self.NUMBER_STEPS_PER_EPISODE if _episode_success else self.PARCEL_DELIVERED / len(agents_alive)\n",
    "\n",
    "        return agents_alive, reward\n",
    "    \n",
    "    \n",
    "    #------ BASELINE related methods ------###         \n",
    "    def compute_optimality_baseline(self, parcel_id, extra_charge=2.5, debug_log=False):\n",
    "        \"\"\"Used in the optimality baseline\n",
    "            Input: parcel_id, (extra_charge)\n",
    "            Output: new total delivery rounds needed for all parcels\n",
    "        \"\"\"\n",
    "        parcel = self.state['parcels'][parcel_id]\n",
    "        path_time = 2 + shortest_path_length(self.graph, parcel[0], parcel[1], 'weight')\n",
    "        \n",
    "        _time = math.ceil(path_time * extra_charge) # round to next higher integer\n",
    "        \n",
    "        min_index = self.o_employed.index(min(self.o_employed))\n",
    "        self.o_employed[min_index] += _time\n",
    "                                    \n",
    "        return max(self.o_employed)\n",
    "        \n",
    "    \n",
    "    def compute_central_delivery(self, p_id, debug_log = False):\n",
    "        \"\"\"Used in the central baseline, iteratively tries to find a good delivery route \n",
    "        with the available agents in the time specified in BASELINE_TIME_CONSTRAINT\n",
    "            Input: parcel_id\n",
    "            Output: Dict: {agent_id: [actions], ...}  --> update that dict! (merge in this function with prev actions!) \n",
    "        \"\"\"\n",
    "\n",
    "        if self.agents_base is None:\n",
    "            self.agents_base = {a_id: (a['position'], 0) for (a_id, a) in self.state[\"agents\"].items()}  # last instructed pos + its step count\n",
    "\n",
    "        min_time = None\n",
    "        new_missions = {}   # key: agent, value: [actions]\n",
    "\n",
    "        source = self.state[\"parcels\"][p_id][0]\n",
    "        target = self.state[\"parcels\"][p_id][1]\n",
    "        shortest_paths_generator = nx.shortest_simple_paths(self.graph, source, target, weight='weight')\n",
    "\n",
    "        running = BoolTimer(self.BASELINE_TIME_CONSTRAINT)\n",
    "        running.start()             \n",
    "        \n",
    "        while running:\n",
    "            # Default --> assign full route to nearest drone\n",
    "            if min_time is None:   \n",
    "\n",
    "                air_route = nx.shortest_path(self.graph, source= source, target= target, weight=\"air\")\n",
    "                air_route_time = nx.shortest_path_length(self.graph, source= source, target= target, weight=\"air\")\n",
    "                air_route.pop(0)  # remove source node from path\n",
    "               \n",
    "                # Assign most suitable Drone\n",
    "                best_drone = None\n",
    "                for (a_id, a_tp) in self.agents_base.items():\n",
    "                    if a_id[0] != 'd':                      # filter for correct agent type\n",
    "                        continue\n",
    "                        \n",
    "                    journey_time = nx.shortest_path_length(self.graph, source=a_tp[0], target=source, weight=\"air\") + a_tp[1]\n",
    "                    if min_time is None or journey_time < min_time:\n",
    "                        min_time = journey_time\n",
    "                        best_drone = (a_id, a_tp)\n",
    "  \n",
    "                # construct path for agent\n",
    "                drone_route = nx.shortest_path(self.graph, source= best_drone[1][0], target= source, weight=\"air\")\n",
    "\n",
    "                drone_route_actions = [x+1 for x in drone_route[1:]] + [(self.ACTION_DROPOFF + p_id, 0)] + [x+1 for x in air_route[1:]] + [self.ACTION_DROPOFF]   # increment node_ids by one to get corresponding action\n",
    "                min_time += air_route_time + 2 # add 2 steps for pick & drop\n",
    "                \n",
    "                self._add_charging_stops_to_route(drone_route_actions, debug_log=debug_log)              \n",
    "                \n",
    "                new_missions[best_drone[0]] = (drone_route_actions, min_time) \n",
    "                \n",
    "            else:   # try to improve the existing base mission  \n",
    "                try:\n",
    "                    shortest_route = next(shortest_paths_generator)\n",
    "                except StopIteration:\n",
    "                    break               # all existing shortest paths already tried\n",
    "                    \n",
    "                subroutes = self._path_to_subroutes(shortest_route, debug_log=debug_log)\n",
    "                duration, min_agents = self._find_best_agents(subroutes, min_time, debug_log=debug_log)\n",
    "\n",
    "                if duration < min_time:\n",
    "                    # faster delivery route found!          \n",
    "                    assert duration < min_time, \"Central Baseline prefered a longer route...\"\n",
    "                    \n",
    "                    # update min_time\n",
    "                    min_time = duration                    \n",
    "                    new_missions = self._build_missions(min_agents, subroutes, p_id, debug_log=debug_log) \n",
    "\n",
    "        #---- end while = Timer\n",
    "        # now save the best mission found in the ENV\n",
    "        for agent in new_missions.keys():\n",
    "            # retrieve target node from mission, depends on charging stops and case no move necessary    \n",
    "            target = None     \n",
    "            if isinstance(new_missions[agent][0][-2], int):    \n",
    "                target = new_missions[agent][0][-2] \n",
    "                if target == 0: target = new_missions[agent][0][-3]    # charging action was added before dropoff\n",
    "                target -= 1                                            # action-1 = node_id\n",
    "            else:  # handle case no move necessary -> pick action before dropoff\n",
    "                target = self.agents_base[agent][0]\n",
    "\n",
    "            self.agents_base[agent] = (target, self.agents_base[agent][1] + new_missions[agent][1])  \n",
    "            self.baseline_missions[agent].extend(new_missions[agent][0])\n",
    "            \n",
    "        return new_missions\n",
    "        \n",
    "        \n",
    "    def _find_best_agents(self, subroutes, min_time, debug_log=False):\n",
    "        \"\"\" For use in centrality_baseline. Finds best available agents for traversing a set of subroutes \n",
    "            and returns these with the total duration for doing so.\n",
    "            Input: subroutes = [ edge_type, [actions]]\n",
    "            \"\"\"\n",
    "        \n",
    "        min_agents = {}\n",
    "        temp_agents_base = {k:v for k,v in self.agents_base.items()}  # deep copy for temporary planning transfer or stick with agent??\n",
    "\n",
    "        for i,r in enumerate(subroutes):\n",
    "\n",
    "            # init some helper vars\n",
    "            a_type = \"d\" if r[0] == 'air' else 'c' \n",
    "            min_time_sub = None    # best time (min) for this subroute (closest agent)\n",
    "            best_agent_sub = None\n",
    "\n",
    "            # iterate over agents of correct type!  --> later: Busy / unbusy\n",
    "            for (a_id, a_tp) in temp_agents_base.items():\n",
    "                #reminder: a_tp is tuple of latest future position (node, timestep)\n",
    "\n",
    "                # filter for correct agent type - even if type is 'both' one agent can still take only its edge type\n",
    "                weight_agent = r[0]\n",
    "                if r[0] == 'both':\n",
    "                    weight_agent = 'road' if a_id[0] == 'c' else 'air'\n",
    "                else:                     # wrong agent type\n",
    "                    if a_id[0] != a_type:  # todo replace with parameter variable in function!\n",
    "                        continue \n",
    "                        \n",
    "                journey_time = nx.shortest_path_length(self.graph, source=a_tp[0], target=r[1][0], weight=weight_agent) + a_tp[1] # earliest time agent can be there\n",
    "                if min_time_sub is None or journey_time < min_time_sub:\n",
    "                    min_time_sub = journey_time\n",
    "                    best_agent_sub = (a_id, a_tp)    # a_id, a_tp = (latest location, timestep)\n",
    "\n",
    "            # closest available agent found        \n",
    "            best_agent_weight = weight_agent = 'road' if best_agent_sub[0] == 'c' else 'air'\n",
    "            duration_sub = min_time_sub + nx.shortest_path_length(self.graph, source=r[1][0], target=r[1][-1], weight=best_agent_weight) + 1\n",
    "\n",
    "            # update agent state in temporary planning\n",
    "            temp_agents_base[best_agent_sub[0]] = (r[1][-1], duration_sub)\n",
    "\n",
    "            # agent_tuple, duration_subroutes_until_then\n",
    "            min_agents[i] = (best_agent_sub, duration_sub)\n",
    "            \n",
    "            if debug_log: assert duration_sub < sys.maxsize, \"Non existent edge taken somewhere...\"\n",
    "\n",
    "            # check if current subroute already longer than the min one\n",
    "            if duration_sub > min_time:\n",
    "                break # already worse, check next simple path\n",
    "                \n",
    "        return duration_sub, min_agents\n",
    "    \n",
    "    \n",
    "    def _build_missions(self, min_agents, subroutes, parcel_id, debug_log = False):\n",
    "        \"\"\"For use in centrality_baseline. Computes list of actions for delivery of parcel parcel_id \n",
    "        and necessary duration for execution from list of agents, subroutes\"\"\"\n",
    "        new_mission = {}\n",
    "\n",
    "        for i, s in enumerate(subroutes):\n",
    "            best_agent_pos = min_agents[i][0][1][0]\n",
    "            #earliest time to start that action\n",
    "            time_pickup = min_agents[i-1][1] if i > 0 else 0 # First subroute pickup as soon as possible\n",
    "            \n",
    "            # construct the actual delivery path to pickup\n",
    "            delivery_route = nx.shortest_path(self.graph, source=best_agent_pos, target=s[1][0], weight=s[0])\n",
    "            delivery_route_actions = [x+1 for x in delivery_route[1:]] + [(self.ACTION_DROPOFF + parcel_id, time_pickup)] + [x+1 for x in s[1][1:]] + [self.ACTION_DROPOFF]\n",
    "\n",
    "            if debug_log: assert min_agents[i][0] not in new_mission  #I see no preferable case where agent picks-up 1 parcel 2 times --> holding always better than following\n",
    "\n",
    "            self._add_charging_stops_to_route(delivery_route_actions, debug_log=debug_log)\n",
    "            new_mission[min_agents[i][0][0]] = (delivery_route_actions, min_agents[i][1])\n",
    "            \n",
    "        return new_mission\n",
    "    \n",
    "    \n",
    "    def _add_charging_stops_to_route(self, route_actions, debug_log=False):\n",
    "        \"\"\"For use in centrality_baseline. Iterates through a list of actions and inserts a charging action\n",
    "        after every move action to a node with a charging station. \n",
    "        Tuples representing Dropoff actions with minimal executions time are updated to account for eventual delays. \"\"\"\n",
    "        \n",
    "        delay = 0\n",
    "        for i, n in enumerate(route_actions):\n",
    "            if type(n) is tuple:\n",
    "                if delay > 0: route_actions[i] = (n[0], n[1] + delay)\n",
    "            else: \n",
    "                if n-1 in self.CHARGING_STATION_NODES:\n",
    "                    delay += 1\n",
    "                    route_actions.insert(i+1, 0)\n",
    "\n",
    "\n",
    "    def _path_to_subroutes(self, path, debug_log= False):\n",
    "        \"\"\"For use in centrality_baseline. Takes path in the graph as input and returns list of subroutes \n",
    "        split at changes of the edge types with the type\"\"\"\n",
    "\n",
    "        # get subroutes by their edge_types\n",
    "        e_type_prev = None\n",
    "        e_type_changes = [] # save indices of source nodes before new edge type\n",
    "        subroutes = []\n",
    "        _subroute = [path[0]] \n",
    "\n",
    "        if len(path) > 1:\n",
    "            e_type_prev = self.graph.edges[path[0], path[1]]['type']\n",
    "    \n",
    "            for i, node in enumerate(path[1:-1], start=1):\n",
    "    \n",
    "                e_type_next = self.graph.edges[node, path[i+1]]['type']\n",
    "    \n",
    "                _subroute.append(node)\n",
    "                if e_type_next != e_type_prev:   \n",
    "                    subroutes.append((e_type_prev, _subroute))\n",
    "                    _subroute = [node]\n",
    "                    e_type_prev = e_type_next\n",
    "    \n",
    "            _subroute.append(path[-1])\n",
    "        subroutes.append((e_type_prev, _subroute))   # don't forget last subroute           \n",
    "    \n",
    "        return subroutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6698c6",
   "metadata": {},
   "source": [
    "## Agent Model and Experiment Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14b96755",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete, Box, Tuple, MultiDiscrete, Dict, MultiBinary\n",
    "#from ray.rllib.utils.spaces.space_utils import flatten_space\n",
    "#from ray.rllib.models.preprocessors import DictFlatteningPreprocessor\n",
    "\n",
    "# Parametric-action agent model  --> apply Action Masking!\n",
    "class ParametricAgentModel(TFModelV2):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name, *args, **kwargs):\n",
    "        super(ParametricAgentModel, self).__init__(obs_space, action_space, num_outputs, model_config, name, *args, **kwargs)\n",
    "\n",
    "        assert isinstance(action_space, Discrete), f'action_space is a {type(action_space)}, but should be Discrete!'\n",
    "        \n",
    "        # Adjust for number of agents/parcels/Nodes!! -> Simply copy found shape from the thrown exception\n",
    "        true_obs_shape = (1750, )\n",
    "        action_embed_size = action_space.n\n",
    "        \n",
    "        self.action_embed_model = FullyConnectedNetwork(\n",
    "            Box(0, 1, shape=true_obs_shape),\n",
    "            action_space,\n",
    "            action_embed_size,\n",
    "            model_config,\n",
    "            name + '_action_embedding')\n",
    "        \n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        \n",
    "        action_mask = input_dict['obs']['allowed_actions']\n",
    "        action_embedding, _ = self.action_embed_model.forward({'obs_flat': input_dict[\"obs_flat\"]}, state, seq_lens)\n",
    "        intent_vector = tf.expand_dims(action_embedding, 1)\n",
    "        action_logits = tf.reduce_sum(intent_vector, axis=1)\n",
    "        inf_mask = tf.maximum(tf.math.log(action_mask), tf.float32.min)\n",
    "\n",
    "        return action_logits + inf_mask, state\n",
    "    \n",
    "    def value_function(self):\n",
    "        return self.action_embed_model.value_function()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2017a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Proposed way to train / evaluate MARL policy from github Issues: --> https://github.com/ray-project/ray/issues/9123 and  https://github.com/ray-project/ray/issues/9208\n",
    "\n",
    "def train(config, name, save_dir, stop_criteria, num_samples, verbosity=1):\n",
    "    \"\"\"\n",
    "    Train an RLlib PPO agent using tune until any of the configured stopping criteria is met.\n",
    "    :param stop_criteria: Dict with stopping criteria.\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run\n",
    "    :return: Return the path to the saved agent (checkpoint) and tune's ExperimentAnalysis object\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/analysis.html#experimentanalysis-tune-experimentanalysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Start training\")\n",
    "    analysis = ray.tune.run(PPOTrainer, verbose=verbosity, config=config, local_dir=save_dir, \n",
    "                            stop=stop_criteria, name=name, num_samples=num_samples,\n",
    "                            checkpoint_at_end=True, resume=True)\n",
    "                            #restore=\"/work-ceph/albecker/Exp_thesis/Exp_environment/small_no_cont/PPOTrainer_Map_Environment_46063_00001_1_2022-02-22_13-02-38/checkpoint_006000/checkpoint-6000\")\n",
    "                                                                                            \n",
    "    # list of lists: one list per checkpoint; each checkpoint list contains 1st the path, 2nd the metric value\n",
    "    checkpoints = analysis.get_trial_checkpoints_paths(trial=analysis.get_best_trial('episode_reward_mean', mode='max'),\n",
    "                                                       metric='episode_reward_mean')\n",
    "    # retriev the checkpoint path; we only have a single checkpoint, so take the first one\n",
    "    checkpoint_path = checkpoints[0][0]\n",
    "    print(f\"Saved trained model in checkpoint {checkpoint_path} - achieved episode_reward_mean: {checkpoints[0][1]}\")\n",
    "    return checkpoint_path, analysis\n",
    "\n",
    "\n",
    "def load(config, path):\n",
    "    \"\"\"\n",
    "    Load a trained RLlib agent from the specified path. Call this before testing the trained agent.\n",
    "    \"\"\"\n",
    "    agent = PPOTrainer(config=config) #, env=env_class)\n",
    "    agent.restore(path)\n",
    "    return agent\n",
    "    \n",
    "    \n",
    "def test(env_class, env_config, policy_mapping_fcn, agent):\n",
    "    \"\"\"Test trained agent for a single episode. Return the retrieved env metrics for this episode and the episode reward\"\"\"\n",
    "    # instantiate env class\n",
    "    env = env_class(env_config)\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    \n",
    "    while not done: # run until episode ends\n",
    "        actions = {}\n",
    "        for agent_id, agent_obs in obs.items():\n",
    "            # Here: policy_id == agent_id - added this to avoid confusion for other policy mappings \n",
    "            policy_id = policy_mapping_fcn(agent_id, episode=None, worker=None) \n",
    "            actions[agent_id] = agent.compute_action(agent_obs, policy_id=policy_id)\n",
    "        obs, reward, done, info = env.step(actions)\n",
    "        done = done['__all__']\n",
    "        \n",
    "        # sum up reward for all agents\n",
    "        episode_reward += sum(reward.values())\n",
    "        \n",
    "    # Retrieve custom metrics from ENV\n",
    "    return env.metrics, episode_reward \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14c5ec6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_and_test_scenarios(config, seeds=None):\n",
    "    \"\"\" Trains for a single scenario indicated by a seed \"\"\"\n",
    "    # TODO how to distinguish between the different algos ? \n",
    "    print(\"Starte: run_function_trainer!\")\n",
    "    \n",
    "    # prepare the config dicts\n",
    "    NAME = config['NAME']\n",
    "    SAVE_DIR = config['SAVE_DIR']\n",
    "    ENVIRONMENT = config['ENV']\n",
    "    \n",
    "    # Simulations\n",
    "    NUMBER_STEPS_PER_EPISODE = config['NUMBER_STEPS_PER_EPISODE']\n",
    "    STOP_CRITERIA = config['STOP_CRITERIA']\n",
    "    NUMBER_OF_SAMPLES = config['NUMBER_OF_SAMPLES']\n",
    "    \n",
    "    #MAP / ENV\n",
    "    NUMBER_OF_DRONES = config['NUMBER_OF_DRONES']\n",
    "    NUMBER_OF_CARS = config['NUMBER_OF_CARS']\n",
    "    NUMBER_OF_AGENTS = NUMBER_OF_DRONES + NUMBER_OF_CARS\n",
    "    MAX_NUMBER_OF_PARCELS = config['MAX_NUMBER_OF_PARCELS']\n",
    "    \n",
    "    # TESTING\n",
    "    SEEDS = config['SEEDS']\n",
    "    \n",
    "    env_config = {\n",
    "            'DEBUG_LOGS':False,\n",
    "            'TOPOLOGY': config['TOPOLOGY'],\n",
    "            # Simulation config\n",
    "            'NUMBER_STEPS_PER_EPISODE': NUMBER_STEPS_PER_EPISODE,\n",
    "            #'NUMBER_OF_TIMESTEPS': NUMBER_OF_TIMESTEPS,\n",
    "            'RANDOM_SEED': None, # 42\n",
    "            # Map\n",
    "            'CHARGING_STATION_NODES': config['CHARGING_STATION_NODES'],\n",
    "            # Entities\n",
    "            'NUMBER_OF_DRONES': NUMBER_OF_DRONES,\n",
    "            'NUMBER_OF_CARS': NUMBER_OF_CARS,\n",
    "            'MAX_BATTERY_POWER': config['MAX_BATTERY_POWER'],  # TODO split this for drone and car??\n",
    "            'INIT_NUMBER_OF_PARCELS': config['INIT_NUMBER_OF_PARCELS'],\n",
    "            'MAX_NUMBER_OF_PARCELS': config['MAX_NUMBER_OF_PARCELS'],\n",
    "            'THRESHOLD_ADD_NEW_PARCEL': config['THRESHOLD_ADD_NEW_PARCEL'],\n",
    "            # Baseline settings\n",
    "            'BASELINE_FLAG': False,  # is set True in the test function when needed\n",
    "            'BASELINE_OPT_CONSTANT': config['BASELINE_OPT_CONSTANT'],\n",
    "            'BASELINE_TIME_CONSTRAINT': config['BASELINE_TIME_CONSTRAINT'],\n",
    "            # TODO \n",
    "            #Rewards\n",
    "            'REWARDS': config['REWARDS'] \n",
    "        }\n",
    "        \n",
    "    run_config = {\n",
    "        'num_gpus': config['NUM_GPUS'],\n",
    "        'num_workers': config['NUM_WORKERS'],\n",
    "        'env': ENVIRONMENT,\n",
    "        'env_config': env_config,\n",
    "        'multiagent': {\n",
    "            'policies': {\n",
    "                # tuple values: policy, obs_space, action_space, config\n",
    "                **{a: (None, None, None, { 'model': {'custom_model': ParametricAgentModel }, 'framework': 'tf'}) for a in ['d_'+ str(j) for j in range(NUMBER_OF_DRONES)] + ['c_'+ str(i) for i in range(NUMBER_OF_DRONES, NUMBER_OF_CARS + NUMBER_OF_DRONES)]}\n",
    "            },\n",
    "            'policy_mapping_fn': policy_mapping_fn,\n",
    "            'policies_to_train': ['d_'+ str(i) for i in range(NUMBER_OF_DRONES)] + ['c_'+ str(i) for i in range(NUMBER_OF_DRONES, NUMBER_OF_CARS + NUMBER_OF_DRONES)]\n",
    "        },\n",
    "        #'log_level': \"INFO\",\n",
    "        #\"hiddens\": [],     # For DQN\n",
    "        #\"dueling\": False,  # For DQN\n",
    "    }\n",
    "    \n",
    "    # Train and Evaluate the agents !\n",
    "    #checkpoint, analysis = train(run_config, NAME, SAVE_DIR, STOP_CRITERIA, NUMBER_OF_SAMPLES)\n",
    "    #print(\"Training finished - Checkpoint: \", checkpoint)\n",
    "    \n",
    "    checkpoint = \"/work-ceph/albecker/Exp_thesis/Exp_environment/small_no_cont/PPOTrainer_Map_Environment_90679_00001_1_2022-02-25_13-50-52/checkpoint_015000/checkpoint-15000\"\n",
    "    \n",
    "    env_class = ENVIRONMENT\n",
    "    # Restore trained policies for evaluation\n",
    "    agent = load(run_config, checkpoint)\n",
    "    print(\"Agent loaded - Agent: \", agent)\n",
    "    \n",
    "    # Run the test cases for the specified seeds\n",
    "    runs = {'max_steps': NUMBER_STEPS_PER_EPISODE, 'max_parcels': MAX_NUMBER_OF_PARCELS, 'max_agents': NUMBER_OF_AGENTS}\n",
    "    \n",
    "    for seed in SEEDS:\n",
    "        env_config['RANDOM_SEED'] = seed\n",
    "        # TODO check if \n",
    "        assert run_config['env_config']['RANDOM_SEED'] == seed\n",
    "        result = test_scenario(run_config, agent)\n",
    "        runs.update(result)\n",
    "        \n",
    "    return runs\n",
    "\n",
    "\n",
    "def test_scenario(config, agent):\n",
    "    \"\"\"\n",
    "    Loads a pretrained agent, initializes an environment from the seed \n",
    "    and then evaluates it over one episode with the Marl agents and the central baseline.\n",
    "\n",
    "    Returns: dict with results for graph creation for both evaluation / inference runs\n",
    "    \"\"\"\n",
    "    #TODO unterscheidung run_config > env_config\n",
    "    # Wie die beiden Runs abspeichern --> {Seed + [marl / base] : result }\n",
    "\n",
    "    env_class = config['env']  \n",
    "    env_config = config['env_config']\n",
    "    seed = env_config['RANDOM_SEED']\n",
    "    policy_mapping_fn = config['multiagent']['policy_mapping_fn']\n",
    "    \n",
    "    # Test with MARL\n",
    "\n",
    "    metrics_marl, reward_marl = test(env_class, env_config, policy_mapping_fn, agent)\n",
    "    \n",
    "    # Test with CentralBase\n",
    "    env_config['BASELINE_FLAG'] = True\n",
    "    metrics_central, reward_central = test(env_class, env_config, policy_mapping_fn, agent)\n",
    "    env_config['BASELINE_FLAG'] = False\n",
    "    \n",
    "    # ASSERT that both optimal values are equal\n",
    "    #assert metrics_marl['optimal'] == metrics_central['optimal']\n",
    "    \n",
    "    return {\"M_\" + str(seed): metrics_marl, \"C_\" + str(seed): metrics_central}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d86f07bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-01 21:36:07,046\tWARNING ppo.py:223 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=32 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test evaluation functions\n",
      "Starte: run_function_trainer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534067)\u001b[0m 2022-03-01 21:36:27,441\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534046)\u001b[0m 2022-03-01 21:36:27,658\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534010)\u001b[0m 2022-03-01 21:36:27,634\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534006)\u001b[0m 2022-03-01 21:36:27,953\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534058)\u001b[0m 2022-03-01 21:36:28,175\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534064)\u001b[0m 2022-03-01 21:36:28,580\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534070)\u001b[0m 2022-03-01 21:36:28,689\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534076)\u001b[0m 2022-03-01 21:36:28,870\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534043)\u001b[0m 2022-03-01 21:36:28,889\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534017)\u001b[0m 2022-03-01 21:36:29,045\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534040)\u001b[0m 2022-03-01 21:36:29,380\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534039)\u001b[0m 2022-03-01 21:36:31,508\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534024)\u001b[0m 2022-03-01 21:36:31,732\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534021)\u001b[0m 2022-03-01 21:36:31,887\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534048)\u001b[0m 2022-03-01 21:36:32,103\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534041)\u001b[0m 2022-03-01 21:36:32,429\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534052)\u001b[0m 2022-03-01 21:36:33,315\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534036)\u001b[0m 2022-03-01 21:36:33,293\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534033)\u001b[0m 2022-03-01 21:36:33,246\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534079)\u001b[0m 2022-03-01 21:36:33,431\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534042)\u001b[0m 2022-03-01 21:36:33,693\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534055)\u001b[0m 2022-03-01 21:36:33,751\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534081)\u001b[0m 2022-03-01 21:36:33,828\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534066)\u001b[0m 2022-03-01 21:36:33,962\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534059)\u001b[0m 2022-03-01 21:36:34,085\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534061)\u001b[0m 2022-03-01 21:36:34,034\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534054)\u001b[0m 2022-03-01 21:36:34,126\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534092)\u001b[0m 2022-03-01 21:36:34,164\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534026)\u001b[0m 2022-03-01 21:36:34,228\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534029)\u001b[0m 2022-03-01 21:36:34,410\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534022)\u001b[0m 2022-03-01 21:36:34,536\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1534065)\u001b[0m 2022-03-01 21:36:34,671\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-01 21:36:50,373\tINFO trainable.py:125 -- Trainable.setup took 43.333 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-03-01 21:36:50,377\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-03-01 21:36:51,104\tINFO trainable.py:472 -- Restored on 134.155.89.136 from checkpoint: /work-ceph/albecker/Exp_thesis/Exp_environment/small_no_cont/PPOTrainer_Map_Environment_90679_00001_1_2022-02-25_13-50-52/checkpoint_015000/checkpoint-15000\n",
      "2022-03-01 21:36:51,105\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 15000, '_timesteps_total': 60000000, '_time_total': 302358.0182814598, '_episodes_total': 50165}\n",
      "2022-03-01 21:36:51,114\tWARNING deprecation.py:45 -- DeprecationWarning: `compute_action` has been deprecated. Use `Trainer.compute_single_action()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent loaded - Agent:  PPOTrainer\n",
      "Test evaluation finished ;)\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    return agent_id\n",
    "\n",
    "basic_config = {\n",
    "        # experiment\n",
    "        'NAME': 'small_no_cont',\n",
    "        'SAVE_DIR': 'Exp_environment',\n",
    "        'ALGO': PPOTrainer,\n",
    "        'ENV': Map_Environment,\n",
    "        'DEBUG_LOGS':False,\n",
    "        'NUM_GPUS': 2,\n",
    "        'NUM_WORKERS': 32,\n",
    "        'NUMBER_OF_SAMPLES': 1,\n",
    "        # Simulation config\n",
    "        'NUMBER_STEPS_PER_EPISODE': 1200,\n",
    "        'RANDOM_SEED': None, # 42\n",
    "        # Map\n",
    "        'TOPOLOGY': topology,\n",
    "        'CHARGING_STATION_NODES': [0,1,2,3,4,6,9],\n",
    "        # Entities\n",
    "        'NUMBER_OF_DRONES': 2,\n",
    "        'NUMBER_OF_CARS': 2,\n",
    "        'INIT_NUMBER_OF_PARCELS': 10,\n",
    "        'MAX_NUMBER_OF_PARCELS': 10,\n",
    "        'THRESHOLD_ADD_NEW_PARCEL': 0.1, # 10% chance\n",
    "        'MAX_BATTERY_POWER': 100, \n",
    "        #Baseline\n",
    "        'BASELINE_TIME_CONSTRAINT': 10,\n",
    "        'BASELINE_OPT_CONSTANT': 2.5,\n",
    "        #TESTING\n",
    "        'SEEDS': [72, 21, 44, 66, 86, 14],\n",
    "        #Rewards\n",
    "        'REWARDS': {\n",
    "            'PARCEL_DELIVERED': 200,\n",
    "            'STEP_PENALTY': -0.1,\n",
    "        }, \n",
    "        'STOP_CRITERIA': {\n",
    "            'timesteps_total': 60_000_000,\n",
    "        }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "print(\"Test evaluation functions\")\n",
    "experiment_results = train_and_test_scenarios(basic_config)\n",
    "print(\"Test evaluation finished ;)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c123aef",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_steps': 1200,\n",
       " 'max_parcels': 10,\n",
       " 'max_agents': 4,\n",
       " 'M_72': {'step': 1200,\n",
       "  'delivered': {'p_10': 125, 'p_3': 128, 'p_2': 221, 'p_9': 237, 'p_4': 797},\n",
       "  'crashed': {'d_0': 91, 'c_3': 828},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': None},\n",
       " 'C_72': {'step': 1200,\n",
       "  'delivered': {'p_4': 49,\n",
       "   'p_1': 67,\n",
       "   'p_3': 67,\n",
       "   'p_7': 95,\n",
       "   'p_5': 126,\n",
       "   'p_10': 224,\n",
       "   'p_9': 403},\n",
       "  'crashed': {},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': 75},\n",
       " 'M_21': {'step': 1200,\n",
       "  'delivered': {'p_7': 501},\n",
       "  'crashed': {'c_3': 227, 'd_1': 1146},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': None},\n",
       " 'C_21': {'step': 1200,\n",
       "  'delivered': {'p_1': 47,\n",
       "   'p_6': 76,\n",
       "   'p_3': 135,\n",
       "   'p_9': 215,\n",
       "   'p_7': 350,\n",
       "   'p_8': 539},\n",
       "  'crashed': {},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': 105},\n",
       " 'M_44': {'step': 1200,\n",
       "  'delivered': {'p_7': 140, 'p_8': 169, 'p_3': 617, 'p_6': 919},\n",
       "  'crashed': {},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': None},\n",
       " 'C_44': {'step': 1200,\n",
       "  'delivered': {'p_7': 253, 'p_8': 258},\n",
       "  'crashed': {},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': 103},\n",
       " 'M_66': {'step': 1200,\n",
       "  'delivered': {'p_3': 46},\n",
       "  'crashed': {'c_2': 79, 'c_3': 353, 'd_1': 589},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': None},\n",
       " 'C_66': {'step': 1200,\n",
       "  'delivered': {'p_1': 16, 'p_3': 18, 'p_5': 70},\n",
       "  'crashed': {},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': 85},\n",
       " 'M_86': {'step': 1200,\n",
       "  'delivered': {},\n",
       "  'crashed': {'c_3': 73, 'c_2': 147},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': None},\n",
       " 'C_86': {'step': 1200,\n",
       "  'delivered': {'p_8': 66,\n",
       "   'p_1': 90,\n",
       "   'p_9': 110,\n",
       "   'p_4': 112,\n",
       "   'p_7': 168,\n",
       "   'p_2': 185,\n",
       "   'p_6': 282},\n",
       "  'crashed': {},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': 90},\n",
       " 'M_14': {'step': 1200,\n",
       "  'delivered': {'p_3': 33, 'p_10': 506, 'p_8': 611},\n",
       "  'crashed': {'d_0': 499, 'd_1': 1095},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': None},\n",
       " 'C_14': {'step': 1200,\n",
       "  'delivered': {'p_1': 12, 'p_3': 12, 'p_6': 66},\n",
       "  'crashed': {},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': 108}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_results = {'max_steps': 1200,\n",
    " 'max_parcels': 10,\n",
    " 'max_agents': 4,\n",
    " 'M_72': {'step': 1200,\n",
    "  'delivered': {'p_10': 125, 'p_3': 128, 'p_2': 221, 'p_9': 237, 'p_4': 797},\n",
    "  'crashed': {'d_0': 91, 'c_3': 828},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': None},\n",
    " 'C_72': {'step': 1200,\n",
    "  'delivered': {'p_4': 49,\n",
    "   'p_1': 67,\n",
    "   'p_3': 67,\n",
    "   'p_7': 95,\n",
    "   'p_5': 126,\n",
    "   'p_10': 224,\n",
    "   'p_9': 403},\n",
    "  'crashed': {},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': 75},\n",
    " 'M_21': {'step': 1200,\n",
    "  'delivered': {'p_7': 501},\n",
    "  'crashed': {'c_3': 227, 'd_1': 1146},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': None},\n",
    " 'C_21': {'step': 1200,\n",
    "  'delivered': {'p_1': 47,\n",
    "   'p_6': 76,\n",
    "   'p_3': 135,\n",
    "   'p_9': 215,\n",
    "   'p_7': 350,\n",
    "   'p_8': 539},\n",
    "  'crashed': {},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': 105},\n",
    " 'M_44': {'step': 1200,\n",
    "  'delivered': {'p_7': 140, 'p_8': 169, 'p_3': 617, 'p_6': 919},\n",
    "  'crashed': {},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': None},\n",
    " 'C_44': {'step': 1200,\n",
    "  'delivered': {'p_7': 253, 'p_8': 258},\n",
    "  'crashed': {},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': 103},\n",
    " 'M_66': {'step': 1200,\n",
    "  'delivered': {'p_3': 46},\n",
    "  'crashed': {'c_2': 79, 'c_3': 353, 'd_1': 589},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': None},\n",
    " 'C_66': {'step': 1200,\n",
    "  'delivered': {'p_1': 16, 'p_3': 18, 'p_5': 70},\n",
    "  'crashed': {},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': 85},\n",
    " 'M_86': {'step': 1200,\n",
    "  'delivered': {},\n",
    "  'crashed': {'c_3': 73, 'c_2': 147},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': None},\n",
    " 'C_86': {'step': 1200,\n",
    "  'delivered': {'p_8': 66,\n",
    "   'p_1': 90,\n",
    "   'p_9': 110,\n",
    "   'p_4': 112,\n",
    "   'p_7': 168,\n",
    "   'p_2': 185,\n",
    "   'p_6': 282},\n",
    "  'crashed': {},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': 90},\n",
    " 'M_14': {'step': 1200,\n",
    "  'delivered': {'p_3': 33, 'p_10': 506, 'p_8': 611},\n",
    "  'crashed': {'d_0': 499, 'd_1': 1095},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': None},\n",
    " 'C_14': {'step': 1200,\n",
    "  'delivered': {'p_1': 12, 'p_3': 12, 'p_6': 66},\n",
    "  'crashed': {},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': 108}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "015fb616",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "def create_chart_bars(results_dict):\n",
    "    \"\"\" Function that plots a bar graph with the duration of one episode \n",
    "        run with MARL agents/ Centrality Baseline/ Optimality Baseline, recorded in the :param results_dict:.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Design choices    \n",
    "    colors = {\n",
    "        \"marl\": 'blue',\n",
    "        \"central\": 'green',\n",
    "        \"optimal\": 'red'\n",
    "    }\n",
    "    \n",
    "    # Retrieve settings values from results dict\n",
    "    max_steps = results_dict['max_steps']\n",
    "    max_parcels = results_dict['max_parcels'] \n",
    "    max_agents = results_dict['max_agents']\n",
    "\n",
    "    # Deep copy for further computations\n",
    "    scenario_results = {k:v for k,v in results_dict.items() if k[0:3] != 'max'}\n",
    "    \n",
    "    # filter\n",
    "    runs = scenario_results.keys()\n",
    "    values = scenario_results.values()\n",
    "    \n",
    "    # TODO statt all dead => einfach not all delivered marker\n",
    "    merged = {}  # key is seed as str, value is a dict with [marl, central, optimal, all_dead_marl, all_dead_cent]\n",
    "  \n",
    "    # Retrieve the data\n",
    "    for run_id, res in scenario_results.items():\n",
    "        \n",
    "        split_id = run_id.split('_')  # --> type, seed\n",
    "        key_type, key_seed = split_id[0], split_id[1]\n",
    "        # merge data from the runs with same seed (marl + baselines)\n",
    "        \n",
    "        # add new dict if seed not encountered yet\n",
    "        if key_seed not in merged:\n",
    "            merged[key_seed] = {} \n",
    "        \n",
    "        _key_delivered = 'marl'\n",
    "        #_key_crashed = 'all_dead_marl'\n",
    "        if key_type == 'C':\n",
    "            # Baseline run\n",
    "            merged[key_seed]['optimal'] = res['optimal']\n",
    "            _key_delivered = 'central'\n",
    "            #_key_crashed = 'all_dead_central'\n",
    "\n",
    "        # Retrieve number of steps in run\n",
    "        last_step = res['step']\n",
    " \n",
    "        # old code for plotting the number of steps needed in the episode\n",
    "        merged[key_seed][_key_delivered] = last_step \n",
    "        merged[key_seed][_key_delivered + '_all'] = len(res['delivered'])\n",
    "        \n",
    "#        all_parcels_delivered = len(res['delivered']) == max_parcels     # were all parcels delivered\n",
    "#        merged[key_seed][_key_delivered + '_all'] = all_parcels_delivered\n",
    "#        if not all_parcels_delivered:\n",
    "#            merged[key_seed][_key_delivered] = max_steps \n",
    "            \n",
    "    print(\"Merged: \", merged)\n",
    "    \n",
    "    \n",
    "    #  example data = [[30, 25, 50, 20],\n",
    "#    [40, 23, 51, 17],\n",
    "#    [35, 22, 45, 19]]\n",
    "\n",
    "    data = [[],[],[],[], []]\n",
    "    labels = []\n",
    "    \n",
    "    # split data into type of run    \n",
    "    for seed,values in merged.items():\n",
    "        labels.append('S_'+seed)\n",
    "        data[0].append(values['marl'])\n",
    "        data[1].append(values['central'])\n",
    "        data[2].append(values['optimal'])\n",
    "        data[3].append(values['marl_all'])\n",
    "        data[4].append(values['central_all'])\n",
    "    \n",
    "    print(\"Data: \", data)\n",
    "    \n",
    "    X = np.arange(len(labels))\n",
    "    #print(X)\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "    #ax.bar(X + 0.00, data[2], color = colors['optimal'], label=\"Optimality Baseline\", width = 0.25)\n",
    "    ax.bar(X + 0.25, data[3], color = colors['marl'], label=\"MARL System\", width = 0.25, alpha=0.8)\n",
    "    ax.bar(X + 0.50, data[4], color = colors['central'], label=\"Centrality Baseline\", width = 0.25)\n",
    "    \n",
    "\n",
    "    plt.xlabel(\"Experiments\")\n",
    "    plt.ylabel(\"Parcels_delivered\")\n",
    "    plt.ylim(bottom=0, top=max_parcels)\n",
    "    \n",
    "    # y axis as percentage\n",
    "    yticks = mtick.PercentFormatter(max_parcels)\n",
    "    ax.yaxis.set_major_formatter(yticks)\n",
    "    \n",
    "    # Add experiment identifiers x-Axis\n",
    "    plt.xticks(X + 0.37, labels)\n",
    "    # Add legend\n",
    "    ax.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5be69355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged:  {'72': {'marl': 1200, 'marl_all': 5, 'optimal': 75, 'central': 1200, 'central_all': 7}, '21': {'marl': 1200, 'marl_all': 1, 'optimal': 105, 'central': 1200, 'central_all': 6}, '44': {'marl': 1200, 'marl_all': 4, 'optimal': 103, 'central': 1200, 'central_all': 2}, '66': {'marl': 1200, 'marl_all': 1, 'optimal': 85, 'central': 1200, 'central_all': 3}, '86': {'marl': 1200, 'marl_all': 0, 'optimal': 90, 'central': 1200, 'central_all': 7}, '14': {'marl': 1200, 'marl_all': 3, 'optimal': 108, 'central': 1200, 'central_all': 3}}\n",
      "Data:  [[1200, 1200, 1200, 1200, 1200, 1200], [1200, 1200, 1200, 1200, 1200, 1200], [75, 105, 103, 85, 90, 108], [5, 1, 4, 1, 0, 3], [7, 6, 2, 3, 7, 3]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot duration bar graphs from the results\n",
    "create_chart_bars(experiment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3f82cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "\n",
    "def create_chart_episode_events(results_dict, draw_crashes = False):\n",
    "    \"\"\" Function that plots a graph with either the deliveries of parcels or the crashes of agents\n",
    "        over the course of an episode, recorded in the :param results_dict:.\n",
    "        Set the :param draw_crashes: flag for plotting crashes, default are deliveries.\n",
    "    \"\"\"\n",
    "    # TODO Graphen auch abspeichern oder nur hier anzeigen ??\n",
    "    # TODO - Parcel additions ??\n",
    "    # TODO - overthink fill with max_value for mean computation \n",
    "    \n",
    "    # Design choices    \n",
    "    colors = {\n",
    "        \"marl\": 'blue',\n",
    "        \"central\": 'green',\n",
    "        \"optimal\": 'red'\n",
    "    }\n",
    "    alpha = 0.6  # Opacity of individual value lines\n",
    "    alpha_opt = 0.2\n",
    "    opt_marker_size = 15\n",
    "    line_width_mean = 5\n",
    "    line_width_optimal = 2\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # Retrieve settings values from results dict\n",
    "    max_steps = results_dict['max_steps']\n",
    "    max_parcels = results_dict['max_parcels'] \n",
    "    max_agents = results_dict['max_agents']\n",
    "    \n",
    "    _len = max_agents if draw_crashes else max_parcels\n",
    "    _len += 1     # start plot at origin\n",
    "\n",
    "    _key = 'crashed' if draw_crashes else 'delivered'\n",
    "    \n",
    "    # Deep copy for further computations\n",
    "    scenario_results = {k:v for k,v in results_dict.items() if k[0:3] != 'max'}\n",
    "    \n",
    "    # for computation of mean\n",
    "    m_values, c_values, o_values = [], [], []\n",
    "    Y = [str(i) for i in range(0, _len)]\n",
    "\n",
    "    # iterate over configs\n",
    "    for scenario, results in scenario_results.items():\n",
    "\n",
    "        # Default settings -> MARL run\n",
    "        color = colors[\"marl\"]\n",
    "        _type = m_values\n",
    "        label= \"marl_system\"\n",
    "\n",
    "        if scenario[0] == 'C':\n",
    "            # Baseline run\n",
    "            color = colors[\"central\"]\n",
    "            _type = c_values\n",
    "            label = \"centrality_baseline\"\n",
    "            \n",
    "            # Retrieve and plot optimality baseline\n",
    "            optimal_time = results['optimal']\n",
    "            assert optimal_time is not None\n",
    "            if not draw_crashes: ax.plot(optimal_time, 0, \"*\", color = colors[\"optimal\"], label=\"optimality_baseline\", markersize=opt_marker_size, alpha= alpha_opt, clip_on=False)   \n",
    "            o_values.append(optimal_time)\n",
    "            \n",
    "        _num_steps = results['step']\n",
    "        \n",
    "        X = [0] + list(results[_key].values())\n",
    "        \n",
    "                \n",
    "        X = X + [max_steps]*(_len - len(X))           # Fill X up with max_step values for not delivered parcels / not crashed agents\n",
    " \n",
    "        _type.append(X)  # add X to the respective mean list\n",
    "        #Y = [str(i) for i in range(0, len(results[_key].values())+1)]\n",
    "        \n",
    "        #print(\"Data: \", results[_key].values())\n",
    "        #print(\"new X: \", X)\n",
    "        #print(\"new Y: \", Y)\n",
    "    \n",
    "        ax.step(X, Y, label=label, where='post', color=color, alpha=alpha)\n",
    "        \n",
    "        \n",
    "        # Attempt to improve the filling mess in the plot...         \n",
    "        #X = X + [max_steps]*(_len - len(X))           # Fill X up with max_step values for not delivered parcels / not crashed agents\n",
    " \n",
    "        #_type.append(X)  # add X to the respective mean list\n",
    "\n",
    "        \n",
    "    # compute mean values\n",
    "    m_mean = np.mean(np.array(m_values), axis=0)\n",
    "    c_mean = np.mean(np.array(c_values), axis=0)\n",
    "    o_mean = np.mean(np.array(o_values), axis=0)\n",
    "\n",
    "    \n",
    "    ax.step(m_mean, Y, label=\"marl_system\", where='post', color=colors[\"marl\"], linewidth=line_width_mean)\n",
    "    ax.step(c_mean, Y, label=\"centrality_baseline\", where='post', color=colors[\"central\"], linewidth=line_width_mean)\n",
    "\n",
    "    # star for opt:    if not draw_crashes: plt.plot(o_mean, 0, \"*\", label=\"optimality_baseline\", color=\"r\", markersize=opt_marker_size, clip_on=False)\n",
    "    # better?: vertical line for opt\n",
    "    if not draw_crashes: ax.axvline(o_mean, label=\"optimality_baseline\", color=colors[\"optimal\"], linewidth=2, alpha=alpha_opt+0.3)\n",
    "    \n",
    "    # y axis as percentage\n",
    "    max_percent = max_agents if draw_crashes else max_parcels\n",
    "    xticks = mtick.PercentFormatter(max_percent)\n",
    "    ax.yaxis.set_major_formatter(xticks)\n",
    "    \n",
    "    \n",
    "    # Lables and Legend\n",
    "    plt.xlabel(\"steps\")\n",
    "    ylabel = \"% of crashed agents\" if draw_crashes else \"% of delivered parcels\"\n",
    "    plt.ylabel(ylabel)\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys())\n",
    "    \n",
    "    # Margins\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.xlim()\n",
    "    plt.margins(x=0, y=0)\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9b300cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot delivery graphs from the results\n",
    "create_chart_episode_events(experiment_results, draw_crashes=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd51171e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot crash graphs from the results\n",
    "create_chart_episode_events(experiment_results, draw_crashes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e29c509",
   "metadata": {},
   "source": [
    "### Manual Actions for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efebf2d2",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "##################\n",
    "env_config = {\n",
    "            'DEBUG_LOGS':False,\n",
    "            'TOPOLOGY': topology,\n",
    "            # Simulation config\n",
    "            'NUMBER_STEPS_PER_EPISODE': 1000,\n",
    "            #'NUMBER_OF_TIMESTEPS': NUMBER_OF_TIMESTEPS,\n",
    "            'RANDOM_SEED': None, # 42\n",
    "            # Map\n",
    "            'CHARGING_STATION_NODES': [0,1,2,3,4],\n",
    "            # Entities\n",
    "            'NUMBER_OF_DRONES': 2,\n",
    "            'NUMBER_OF_CARS': 2,\n",
    "            'MAX_BATTERY_POWER': 100,  # TODO split this for drone and car??\n",
    "            'INIT_NUMBER_OF_PARCELS': 3,\n",
    "            'MAX_NUMBER_OF_PARCELS': 3,\n",
    "            'THRESHOLD_ADD_NEW_PARCEL': 0.01,\n",
    "            # Baseline settings\n",
    "            'BASELINE_FLAG': False,  # is set True in the test function when needed\n",
    "            'BASELINE_OPT_CONSTANT': 2.5,\n",
    "            'BASELINE_TIME_CONSTRAINT': 5,\n",
    "            # TODO \n",
    "            #Rewards\n",
    "            'REWARDS': {\n",
    "                'PARCEL_DELIVERED': 200,\n",
    "                'STEP_PENALTY': -0.1,\n",
    "            },  \n",
    "        }\n",
    "        \n",
    "\n",
    "\n",
    "env = Map_Environment(env_config)\n",
    "env.state\n",
    "#env.ACTION_DROPOFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5165d3bd",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO select actions to give agent 0 reward!!\n",
    "#print(env.action_space)\n",
    "\n",
    "\n",
    "actions_1 = {'d_0': 6, 'd_1': 2, 'c_2': 3, 'c_3': 4}\n",
    "#actions_1 = {'d_0': 0, 'd_1': 0, 'c_2': 5, 'c_3': 5}\n",
    "actions_2 = {'d_0': 2, 'd_1': 8, 'c_2': 7, 'c_3':0}\n",
    "actions_3 = {'d_0': 5, 'd_1': 5, 'c_2':2 }\n",
    "\n",
    "new_obs, rewards, dones, infos = env.step(actions_1)\n",
    "print(infos)\n",
    "print(f\"New Obs are: {new_obs}\")\n",
    "print(rewards)\n",
    "print(\"------------------\")\n",
    "new_obs2, rewards2, dones2, infos2 = env.step(actions_2)\n",
    "print(f\"New Obs are: {new_obs2}\")\n",
    "print(rewards2)\n",
    "print(\"------------------\")\n",
    "new_obs3, rewards3, dones3, infos3 = env.step(actions_3)\n",
    "print(f\"New Obs are: {new_obs3}\")\n",
    "print(rewards3)\n",
    "\n",
    "actions_4 = {'d_0': 0, 'd_1': 0, 'c_2': 1, 'c_3': 0}\n",
    "actions_5 = {'d_0': 0, 'd_1': 0, 'c_2': 0, 'c_3': 0}\n",
    "actions_6 = {'d_0': 0, 'd_1': 0, 'c_2': 5, 'c_3': 0}\n",
    "\n",
    "new_obs, rewards, dones, infos = env.step(actions_4)\n",
    "print(f\"New Obs are: {new_obs}\")\n",
    "print(rewards)\n",
    "print(\"------------------\")\n",
    "new_obs2, rewards2, dones2, infos2 = env.step(actions_5)\n",
    "print(f\"New Obs are: {new_obs2}\")\n",
    "print(rewards2)\n",
    "print(\"------------------\")\n",
    "new_obs3, rewards3, dones3, infos3 = env.step(actions_6)\n",
    "print(f\"New Obs are: {new_obs3}\")\n",
    "print(rewards3)\n",
    "\n",
    "print(\"------------------\")\n",
    "print(dones3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a143594",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TENSORBOARD\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "#Start tensorboard below the notebook\n",
    "#%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
