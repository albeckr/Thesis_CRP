{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f838d2e",
   "metadata": {},
   "source": [
    "# MultiAgentEnvironment: simple Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a170d6",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful\n"
     ]
    }
   ],
   "source": [
    "#%autosave 30\n",
    "import glob\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms.shortest_paths.generic import shortest_path_length\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "#from ray.tune.logger import pretty_print\n",
    "from ray.rllib.policy.policy import Policy, PolicySpec\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.fcnet import FullyConnectedNetwork\n",
    "\n",
    "#from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.env import MultiAgentEnv\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "\n",
    "from gym.spaces import Discrete, Box, Tuple, MultiDiscrete, Dict, MultiBinary\n",
    "from ray.rllib.utils.spaces.repeated import Repeated\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf1, tf, tfv = try_import_tf()  # prefered TF import for Ray.io\n",
    "\n",
    "from threading import Thread, Event\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10fdf817",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "######## Utility Classes ########\n",
    "class BoolTimer(Thread):\n",
    "    \"\"\"A boolean value that toggles after a specified number of seconds:\n",
    "    \n",
    "    Example:\n",
    "        bt = BoolTimer(30.0, False)\n",
    "        bt.start()\n",
    "\n",
    "    Used in the Centrality Baseline to limit the computation time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, interval, initial_state=True):\n",
    "        Thread.__init__(self)\n",
    "        self.interval = interval\n",
    "        self.state = initial_state\n",
    "        self.finished = Event()\n",
    "\n",
    "    def __bool__(self):\n",
    "        return bool(self.state)\n",
    "\n",
    "    def run(self):\n",
    "        self.finished.wait(self.interval)\n",
    "        if not self.finished.is_set():\n",
    "            self.state = not self.state\n",
    "        self.finished.set()\n",
    "\n",
    "\n",
    "######## Static helper functions ########\n",
    "def shuffle_actions(action_dict, check_space = False):\n",
    "    \"\"\"\n",
    "        Used to shuffle the action dict to ensure that agents with a lower id are not always preferred\n",
    "        when picking up parcels over other agents that chose the same action.\n",
    "        For debugging: Can also be used to check if all actions are in the action_space.\n",
    "    \"\"\"\n",
    "    keys = list(action_dict)\n",
    "    random.shuffle(keys)\n",
    "    shuffled = {}\n",
    "\n",
    "    for agent in keys:\n",
    "        if check_space:  #assert actions are in action space -> disable for later trainig, extra check while development\n",
    "            assert self.action_space.contains(action_dict[agent]),f\"Action {action_dict[agent]} taken by agent {agent} not in action space\"\n",
    "        shuffled[agent] = action_dict[agent]\n",
    "\n",
    "    return shuffled\n",
    "\n",
    "\n",
    "\n",
    "def load_graph(data):\n",
    "    \"\"\"Loads topology (map) from json file into a networkX Graph and returns the graph\"\"\"\n",
    "\n",
    "    nodes = data[\"nodes\"]\n",
    "    edges = data[\"edges\"]\n",
    "\n",
    "    g = nx.DiGraph()  # directed graph\n",
    "    g.add_nodes_from(nodes)\n",
    "    g.edges(data=True)\n",
    "\n",
    "   \n",
    "    for node in nodes:  # add attribute values\n",
    "        g.nodes[node][\"id\"] = nodes[node][\"id\"]\n",
    "        g.nodes[node][\"type\"] = nodes[node][\"type\"]\n",
    "\n",
    "    for edge in edges:  # add edges with attributes\n",
    "        f = edges[edge][\"from\"]\n",
    "        t = edges[edge][\"to\"]\n",
    "        \n",
    "        weight_road, weight_air, _type = sys.maxsize, sys.maxsize, None\n",
    "        \n",
    "        if edges[edge][\"road\"] >= 0:\n",
    "            weight_road = edges[edge][\"road\"]\n",
    "            _type = 'road'\n",
    "        if edges[edge][\"air\"] >= 0:\n",
    "            weight_air = edges[edge][\"air\"]\n",
    "            _type = 'both' if _type == 'road' else 'air'\n",
    "\n",
    "        weight = min(weight_road, weight_air)  # needed for optimality baseline\n",
    "        \n",
    "        g.add_edge(f, t, type=_type, road= weight_road, air=weight_air, weight=weight)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663b334",
   "metadata": {},
   "source": [
    "## Environment Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f835bd0",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Map definition\n",
    "topology = {\n",
    "    'nodes': {\n",
    "            0: {'id': 1, 'type': 'parking'},\n",
    "            1: {'id': 1, 'type': 'parking'},\n",
    "            2: {'id': 2, 'type': 'parking'},\n",
    "            3: {'id': 3, 'type': 'parking'},\n",
    "            4: {'id': 4, 'type': 'parking'},\n",
    "            5: {'id': 5, 'type': 'air'},\n",
    "            6: {'id': 6, 'type': 'parking'},\n",
    "            7: {'id': 7, 'type': 'air'},\n",
    "            8: {'id': 8, 'type': 'parking'},\n",
    "            9: {'id': 9, 'type': 'parking'},\n",
    "            10: {'id': 10, 'type': 'parking'},\n",
    "            11: {'id': 11, 'type': 'air'}\n",
    "    },\n",
    "    'edges': {\n",
    "    ## Outer Ring    --> Road much (!) faster than drones\n",
    "    \"e01\":{\"from\": 0,\"to\": 1,\"road\": 10, \"air\": 40},\n",
    "    \"e02\":{\"from\": 1,\"to\": 0,\"road\": 10, \"air\": 40},\n",
    "    \"e03\":{\"from\": 1,\"to\": 4,\"road\": 8, \"air\": 6},        \n",
    "    \"e04\":{\"from\": 4,\"to\": 1,\"road\": 8, \"air\": 6},\n",
    "    \"e03\":{\"from\": 4,\"to\": 2,\"road\": 8, \"air\": 6},        \n",
    "    \"e04\":{\"from\": 2,\"to\": 4,\"road\": 8, \"air\": 6},\n",
    "    \"e05\":{\"from\": 2,\"to\": 3,\"road\": 10, \"air\": 30},\n",
    "    \"e06\":{\"from\": 3,\"to\": 2,\"road\": 10, \"air\": 30},\n",
    "    \"e07\":{\"from\": 3,\"to\": 0,\"road\": 10, \"air\": 40},\n",
    "    \"e08\":{\"from\": 0,\"to\": 3,\"road\": 8, \"air\": 35}, \n",
    "    ## Inner Nodes  --> Reinfahren langsamer als rausfahren\n",
    "    ##              --> \n",
    "    \"e11\":{\"from\": 4,\"to\": 6,\"road\": 2, \"air\": 5},        \n",
    "    \"e12\":{\"from\": 6,\"to\": 4,\"road\": 2, \"air\": 5},\n",
    "    \"e13\":{\"from\": 0,\"to\": 6,\"road\": 6, \"air\": -1},\n",
    "    \"e14\":{\"from\": 6,\"to\": 0,\"road\": 6, \"air\": -1},\n",
    "    \"e15\":{\"from\": 3,\"to\": 6,\"road\": 6, \"air\": -1},\n",
    "    \"e16\":{\"from\": 6,\"to\": 3,\"road\": 6, \"air\": -1},   \n",
    "    ## Outliers   --> Distance about equal if both exist!\n",
    "    \"e17\":{\"from\": 4,\"to\": 5,\"road\": 2, \"air\": 2},        \n",
    "    \"e18\":{\"from\": 5,\"to\": 4,\"road\": 2, \"air\": 2},\n",
    "    \"e19\":{\"from\": 6,\"to\": 7,\"road\": -1, \"air\": 2},\n",
    "    \"e20\":{\"from\": 7,\"to\": 6,\"road\": -1, \"air\": 2},\n",
    "    \"e21\":{\"from\": 3,\"to\": 11,\"road\": -1, \"air\": 3},\n",
    "    \"e22\":{\"from\": 11,\"to\": 3,\"road\": -1, \"air\": 3},     \n",
    "    ## Upper left square  --> Hier besser mit den Drohnen agieren, Falls Sie da sind!!\n",
    "    \"e23\":{\"from\": 0,\"to\": 8,\"road\": -1, \"air\": 4},        \n",
    "    \"e24\":{\"from\": 8,\"to\": 0,\"road\": -1, \"air\": 4},\n",
    "    \"e25\":{\"from\": 8,\"to\": 9,\"road\": 3, \"air\": 3},\n",
    "    \"e26\":{\"from\": 9,\"to\": 8,\"road\": 3, \"air\": 3},\n",
    "    \"e27\":{\"from\": 9,\"to\": 10,\"road\": 3, \"air\": 3},\n",
    "    \"e28\":{\"from\": 10,\"to\": 9,\"road\": 3, \"air\": 3},\n",
    "    \"e29\":{\"from\": 0,\"to\": 10,\"road\": 3, \"air\": 3},\n",
    "    \"e30\":{\"from\": 10,\"to\": 0,\"road\": 3, \"air\": 3}  \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6ccbe98",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class Map_Environment(MultiAgentEnv):\n",
    "    \n",
    "    def __init__(self, env_config: dict = {}):\n",
    "        # ensure config file includes all necessary settings\n",
    "        assert 'NUMBER_STEPS_PER_EPISODE' in env_config\n",
    "        assert 'NUMBER_OF_DRONES' in env_config\n",
    "        assert 'NUMBER_OF_CARS' in env_config\n",
    "        assert 'INIT_NUMBER_OF_PARCELS' in env_config\n",
    "        assert 'TOPOLOGY' in env_config\n",
    "        assert 'MAX_NUMBER_OF_PARCELS' in env_config\n",
    "        assert 'THRESHOLD_ADD_NEW_PARCEL' in env_config\n",
    "        assert 'BASELINE_FLAG' in env_config\n",
    "        assert 'BASELINE_TIME_CONSTRAINT' in env_config\n",
    "        assert 'BASELINE_OPT_CONSTANT' in env_config\n",
    "        assert 'CHARGING_STATION_NODES' in env_config\n",
    "        assert 'REWARDS' in env_config\n",
    "        \n",
    "        self.graph = load_graph(topology)\n",
    "        \n",
    "\n",
    "        # Map config\n",
    "        self.NUMBER_OF_DRONES = env_config['NUMBER_OF_DRONES']\n",
    "        self.NUMBER_OF_CARS = env_config['NUMBER_OF_CARS']\n",
    "        self.NUMBER_OF_EDGES = self.graph.number_of_edges()\n",
    "        self.NUMBER_OF_NODES = self.graph.number_of_nodes()\n",
    "        self.CHARGING_STATION_NODES = env_config['CHARGING_STATION_NODES']           \n",
    "        self.MAX_BATTERY_POWER = env_config['MAX_BATTERY_POWER']\n",
    "       \n",
    "\n",
    "        # Simulation config\n",
    "        self.NUMBER_STEPS_PER_EPISODE = env_config['NUMBER_STEPS_PER_EPISODE']\n",
    "        self.INIT_NUMBER_OF_PARCELS = env_config['INIT_NUMBER_OF_PARCELS']\n",
    "        self.RANDOM_SEED = env_config.get('RANDOM_SEED', None)\n",
    "        self.MAX_NUMBER_OF_PARCELS = env_config['MAX_NUMBER_OF_PARCELS']\n",
    "        self.THRESHOLD_ADD_NEW_PARCEL = env_config['THRESHOLD_ADD_NEW_PARCEL']\n",
    "        self.BASELINE_FLAG = env_config['BASELINE_FLAG']\n",
    "        self.BASELINE_TIME_CONSTRAINT = env_config['BASELINE_TIME_CONSTRAINT']\n",
    "        self.BASELINE_OPT_CONSTANT = env_config['BASELINE_OPT_CONSTANT']\n",
    " \n",
    "        self.DEBUG_LOG = env_config.get('DEBUG_LOGS', False)\n",
    "        # Some Sanity Checks on the settings\n",
    "        if self.DEBUG_LOG: assert self.MAX_NUMBER_OF_PARCELS >= self.INIT_NUMBER_OF_PARCELS, \"Number of initial parcels exceeds max parcel limit\"\n",
    "\n",
    "        #Reward constants\n",
    "        self.STEP_PENALTY = env_config['REWARDS']['STEP_PENALTY']\n",
    "        self.PARCEL_DELIVERED = env_config['REWARDS']['PARCEL_DELIVERED']\n",
    "        # compute other rewards\n",
    "        self.BATTERY_DIED = self.STEP_PENALTY * self.NUMBER_STEPS_PER_EPISODE\n",
    "        self.BATTERY_DIED_WITH_PARCEL = self.BATTERY_DIED * 2\n",
    "        #self.DELIVERY_CONTRIBUTION: depends on active agents --> computed when given in prepare_global_reward()\n",
    "        #self.ALL_DELIVERED_CONTRIB: depends on active agents --> computed when given in prepare_global_reward(episode_success=True)\n",
    "\n",
    "        # Computed constants\n",
    "        self.NUMBER_OF_AGENTS = self.NUMBER_OF_DRONES + self.NUMBER_OF_CARS\n",
    "        self.PARCEL_STATE_DELIVERED = self.NUMBER_OF_AGENTS + self.NUMBER_OF_NODES\n",
    "        self.NUMBER_OF_ACTIONS = 1 + self.NUMBER_OF_NODES + 1 + self.MAX_NUMBER_OF_PARCELS\n",
    "        self.ACTION_DROPOFF = 1 + self.NUMBER_OF_NODES     # First Action NOOP is 0\n",
    "    \n",
    "        # seed RNGs\n",
    "        self.seed(self.RANDOM_SEED) \n",
    "        \n",
    "        self.state = None  \n",
    "        self.current_step = None\n",
    "        self.blocked_agents = None\n",
    "        self.parcels_delivered = None\n",
    "        self.done_agents = None\n",
    "        self.all_done = None  \n",
    "        self.allowed_actions = None\n",
    "        \n",
    "        # baseline related \n",
    "        self.baseline_missions = None\n",
    "        self.agents_base = None\n",
    "        self.o_employed = None\n",
    "        \n",
    "        # metrics for the evaluation\n",
    "        self.parcel_delivered_steps = None # --> {p1: 20, p2: 240, p:140}\n",
    "        self.parcel_added_steps = None     # --> {p1: 0, p2: 0, p: 50}\n",
    "        self.agents_crashed = None         # --> {c_2: 120, d_0: 242} \n",
    "        self.metrics = None\n",
    "        \n",
    "        self.agents = [*[\"d_\" + str(i) for i in range(self.NUMBER_OF_DRONES)],*[\"c_\" + str(i) for i in range(self.NUMBER_OF_DRONES, self.NUMBER_OF_DRONES + self.NUMBER_OF_CARS)]]\n",
    "        \n",
    "        # Define observation and action spaces for individual agents\n",
    "        self.action_space = Discrete(self.NUMBER_OF_ACTIONS)\n",
    "\n",
    "        #---- Repeated Obs Space: Represents a parcel with (id, location, destination)  --> # parcel_id starts at 1 \n",
    "        parcel_space = Dict({'id': Discrete(self.MAX_NUMBER_OF_PARCELS+1),\n",
    "                             'location': Discrete(self.NUMBER_OF_NODES + self.NUMBER_OF_AGENTS + 1),\n",
    "                             'destination': Discrete(self.NUMBER_OF_NODES)\n",
    "                            })\n",
    "        self.observation_space = Dict({'obs': Dict({'state': \n",
    "                                                 Dict({ 'position': Discrete(self.NUMBER_OF_NODES),\n",
    "                                                        'battery': Discrete(self.MAX_BATTERY_POWER + 1), #[0-100]\n",
    "                                                        'has_parcel': Discrete(self.MAX_NUMBER_OF_PARCELS + 1),\n",
    "                                                        'current_step': Discrete(self.NUMBER_STEPS_PER_EPISODE + 1)}), \n",
    "                                                 'parcels': Repeated(parcel_space, max_len=self.MAX_NUMBER_OF_PARCELS)\n",
    "                                                }),\n",
    "                                        'allowed_actions': MultiBinary(self.NUMBER_OF_ACTIONS)\n",
    "                                      }) \n",
    "        #TODO: why is reset() not called by env? \n",
    "        self.reset() \n",
    "        \n",
    "    def step(self, action_dict):\n",
    "        \"\"\"conduct the state transitions caused by actions in action_dict\n",
    "        :returns:\n",
    "            - observation_dict: observations for agents that need to act in the next round\n",
    "            - rewards_dict: rewards for agents following their chosen actions\n",
    "            - done_dict: indicates end of episode if max_steps reached or all parcels delivered\n",
    "            - info_dict: pass data to custom logger\n",
    "        \"\"\"\n",
    "\n",
    "        if self.DEBUG_LOG: print(f\"Debug log flag set to {self.DEBUG_LOG}\")\n",
    "        \n",
    "        # ensure no disadvantage for agents with higher IDs if action conflicts with that taken by other agent\n",
    "        action_dict = shuffle_actions(action_dict)\n",
    "    \n",
    "        self.current_step += 1\n",
    "    \n",
    "        # grant step penalty reward    \n",
    "        agent_rewards = {agent: self.STEP_PENALTY for agent in self.agents}  \n",
    "        \n",
    "        # setting an agent done twice might cause crash when used with tune... -> https://github.com/ray-project/ray/issues/10761\n",
    "        dones = {}\n",
    "        \n",
    "        self.metrics['step'] = self.current_step\n",
    "\n",
    "        # dynamically add parcel\n",
    "        if random.random() <= self.THRESHOLD_ADD_NEW_PARCEL and len(self.state['parcels']) < self.MAX_NUMBER_OF_PARCELS:\n",
    " \n",
    "            p_id, parcel = self.generate_parcel()\n",
    "            assert p_id not in self.state['parcels'], \"Duplicate parcel ID generated\"\n",
    "            self.state['parcels'][p_id] = parcel\n",
    "\n",
    "            if self.BASELINE_FLAG:\n",
    "                self.metrics[\"optimal\"] = self.compute_optimality_baseline(p_id, extra_charge=self.BASELINE_OPT_CONSTANT)\n",
    "                \n",
    "            for agent in self.agents:\n",
    "                self.allowed_actions[agent][self.ACTION_DROPOFF + p_id] = np.array([1]).astype(bool)\n",
    "            \n",
    "        if self.BASELINE_FLAG:\n",
    "            old_actions = action_dict\n",
    "            action_dict = {}\n",
    "            # Replace actions with actions recommended by the central baseline\n",
    "            for agent, action in old_actions.items():\n",
    "                if len(self.baseline_missions[agent]) > 0:\n",
    "                    new_action = self.baseline_missions[agent][0]\n",
    "\n",
    "                    if type(new_action) is tuple: # dropoff action with minimal time\n",
    "\n",
    "                        if self.current_step >= new_action[1]:\n",
    "                            new_action = new_action[0]\n",
    "                            self.baseline_missions[agent].pop(0)\n",
    "                        else: #agent has to wait for previous subroute agent\n",
    "                            new_action = 0\n",
    "                    else: # move or pickup or charge\n",
    "                        self.baseline_missions[agent].pop(0)\n",
    "                            \n",
    "                    action_dict[agent] = new_action\n",
    "\n",
    "                else: # agent has no baseline mission -> Noop\n",
    "                    action_dict[agent] = 0\n",
    "                \n",
    "        # carry out State Transition\n",
    "        \n",
    "        # handel NOP actions: -> action == 0 \n",
    "        noop_agents =  {agent: action for agent, action in action_dict.items() if action == 0}\n",
    "        effectual_agents_items = {agent: action for agent, action in action_dict.items() if action > 0}.items()\n",
    "        \n",
    "        # now: transaction between agents modelled as pickup of just offloaded (=dropped) parcel --> handle dropoff first\n",
    "        moving_agents = {agent: action for agent, action in effectual_agents_items if 0 < action and action <= self.NUMBER_OF_NODES}\n",
    "        dropoff_agents = {agent: action for agent, action in effectual_agents_items if action == self.ACTION_DROPOFF}\n",
    "        pickup_agents = {agent: action for agent, action in effectual_agents_items if action > self.ACTION_DROPOFF}\n",
    "\n",
    "        # handle noop / charge decisions:\n",
    "        for agent, action in noop_agents.items():\n",
    "            # check if recharge is possible\n",
    "            current_pos = self.state['agents'][agent]['position']\n",
    "            if current_pos in self.CHARGING_STATION_NODES:\n",
    "                self.state['agents'][agent]['battery'] = self.MAX_BATTERY_POWER\n",
    "\n",
    "        # handle Movement actions:\n",
    "        for agent, action in moving_agents.items():\n",
    "            # get Current agent position from state\n",
    "            self.state['agents'][agent]['battery'] += -1\n",
    "            current_pos = self.state['agents'][agent]['position']\n",
    "            \n",
    "            # networkX: use node instead of edge:\n",
    "            destination = action - 1\n",
    "            \n",
    "            if self.graph.has_edge(current_pos, destination):\n",
    "                # Agent chose existing edge! -> check if type is suitable\n",
    "              \n",
    "                agent_type = 'road' if agent[:1] == 'c' else 'air'\n",
    "                if self.graph[current_pos][destination][\"type\"] in [agent_type, 'both']:\n",
    "                    # Edge has correct type\n",
    "                    self.state['agents'][agent]['position'] = destination\n",
    "                    self.state['agents'][agent]['battery'] += -(self.graph[current_pos][destination][agent_type] +1)\n",
    "                    if self.state['agents'][agent]['battery'] < 0:        # ensure negative battery value does not break obs_space\n",
    "                        #Battery below 0 --> reset to 0 (stay in obs space)\n",
    "                        self.state['agents'][agent]['battery'] = 0\n",
    "                    \n",
    "                    self.blocked_agents[agent] = self.graph[current_pos][destination][agent_type]\n",
    "                    self.update_allowed_actions_nodes(agent)\n",
    "\n",
    "                    \n",
    "        # handle Dropoff Decision: -> action == self.NUMBER_OF_NODES + 2\n",
    "        for agent, action in dropoff_agents.items():\n",
    "            self.state['agents'][agent]['battery'] += -1\n",
    "\n",
    "            if self.state['agents'][agent]['has_parcel'] > 0:   # agent has parcel\n",
    "                parcel_id = self.state['agents'][agent]['has_parcel']\n",
    "                self.state['agents'][agent]['has_parcel'] = 0\n",
    "                self.state['parcels'][parcel_id][0] = self.state['agents'][agent]['position']\n",
    "                if self.state['parcels'][parcel_id][0] ==  self.state['parcels'][parcel_id][1]:\n",
    "                    # Delivery successful\n",
    "                    \n",
    "                    agent_rewards[agent] += self.PARCEL_DELIVERED  # local reward\n",
    "                    # global contribution rewards\n",
    "                    active_agents, reward = self.prepare_global_reward()\n",
    "                    for a_id in active_agents: agent_rewards[a_id] += reward\n",
    "                    \n",
    "                    self.state['parcels'][parcel_id][0] = self.PARCEL_STATE_DELIVERED\n",
    "                    self.parcels_delivered[int(parcel_id) -1] = True                    # Parcel_ids start at 1\n",
    "\n",
    "                    self.metrics['delivered'].update({\"p_\" + str(parcel_id): self.current_step})\n",
    "\n",
    "                self.update_allowed_actions_parcels(agent)\n",
    "          \n",
    "        \n",
    "        # handle Pickup Decision:\n",
    "        for agent, action in pickup_agents.items():\n",
    "            self.state['agents'][agent]['battery'] += -1 \n",
    "    \n",
    "            # agent has free capacity\n",
    "            if self.state['agents'][agent]['has_parcel'] == 0:  #free parcel capacity\n",
    "                # convert action_id to parcel_id\n",
    "                parcel_id = action - self.ACTION_DROPOFF\n",
    "\n",
    "                if self.DEBUG_LOG: assert parcel_id in self.state['parcels'] # parcel {parcel_id} already in ENV ??\n",
    "                    \n",
    "                elif self.state['parcels'][parcel_id][0] == self.state['agents'][agent]['position']:\n",
    "                    # Successful pickup operation\n",
    "                    self.state['parcels'][parcel_id][0] = self.NUMBER_OF_NODES + int(agent[2:])\n",
    "                    self.state['agents'][agent]['has_parcel'] = int(parcel_id)\n",
    "                    \n",
    "                    self.update_allowed_actions_parcels(agent)\n",
    "                        \n",
    "        # unblock agents for next round \n",
    "        self.blocked_agents = {agent: remaining_steps -1 for agent, remaining_steps in self.blocked_agents.items() if remaining_steps > 1}\n",
    "\n",
    "        # handle dones - out of battery or max_steps or goal reached\n",
    "        for agent in action_dict.keys():\n",
    "            if agent not in self.done_agents and self.state['agents'][agent]['battery'] <= 0:\n",
    "                agent_rewards[agent] = self.BATTERY_DIED_WITH_PARCEL if self.state['agents'][agent]['has_parcel'] != 0 else self.BATTERY_DIED\n",
    "                dones[agent] = True\n",
    "                self.done_agents.append(agent)\n",
    "                \n",
    "                self.metrics['crashed'].update({agent: self.current_step})\n",
    "                \n",
    "                if len(self.done_agents) == self.NUMBER_OF_AGENTS:\n",
    "                    # all agents dead\n",
    "                    self.all_done = True\n",
    "        \n",
    "        # check if episode terminated because of goal reached or all agents crashed -> avoid setting done twice\n",
    "        if self.current_step >= self.NUMBER_STEPS_PER_EPISODE or (all(self.parcels_delivered) and len(self.parcels_delivered) == self.MAX_NUMBER_OF_PARCELS):\n",
    "            # check if episode success:\n",
    "            if self.current_step < self.NUMBER_STEPS_PER_EPISODE:\n",
    "                    # grant global reward for all parcels delivered \n",
    "                    active_agents, reward = self.prepare_global_reward(_episode_success=True)\n",
    "                    for a_id in active_agents: agent_rewards[a_id] += reward\n",
    "            \n",
    "            self.all_done = True\n",
    "\n",
    "        dones['__all__'] = self.all_done\n",
    "\n",
    "        \n",
    "        parcel_obs = self.get_parcel_obs()\n",
    "    \n",
    "        # obs \\ rewards \\ dones\\ info\n",
    "        return  {agent: { 'obs': {'state': {'position': self.state['agents'][agent]['position'], 'battery': self.state['agents'][agent]['battery'],\n",
    "                                    'has_parcel': self.state['agents'][agent]['has_parcel'],'current_step': self.current_step},\n",
    "                                    'parcels': parcel_obs},\n",
    "                         'allowed_actions': self.allowed_actions[agent]} for agent in self.agents if agent not in self.blocked_agents and agent not in self.done_agents}, \\\n",
    "                { agent: agent_rewards[agent] for agent in self.agents}, \\\n",
    "                dones, \\\n",
    "                {}\n",
    "\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)  \n",
    "\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"resets variables; returns dict with observations, keys are agent_ids\"\"\"\n",
    "        self.current_step = 0\n",
    "        self.blocked_agents = {}\n",
    "        self.parcels_delivered = [False for _ in range(self.MAX_NUMBER_OF_PARCELS)]\n",
    "        self.done_agents = []\n",
    "        self.all_done = False\n",
    "        \n",
    "        self.metrics = { \"step\": self.current_step,\n",
    "            \"delivered\": {},\n",
    "            \"crashed\": {},\n",
    "            \"added\": {},\n",
    "            \"optimal\": None\n",
    "        }\n",
    "\n",
    "        #Baseline\n",
    "        self.baseline_missions = {agent: [] for agent in self.agents}\n",
    "        self.o_employed = [0 for _ in range(self.NUMBER_OF_AGENTS)]\n",
    "\n",
    "        self.agents_base = None  # env.agents in the pseudocode\n",
    "        self.allowed_actions = { agent: np.array([1 for act in range(self.NUMBER_OF_ACTIONS)]) for agent in self.agents}\n",
    " \n",
    "        #Reset State\n",
    "        self.state = {'agents': {},\n",
    "                      'parcels': {}}\n",
    "        #generate initial parcels\n",
    "        for _ in range(self.INIT_NUMBER_OF_PARCELS):\n",
    "            p_id, parcel = self.generate_parcel()\n",
    "            self.state['parcels'][p_id] = parcel            \n",
    "\n",
    "        parcel_obs = self.get_parcel_obs()\n",
    "        \n",
    "        # init agents\n",
    "        self.state['agents'] = {agent: {'position': self._random_feasible_agent_position(agent),\n",
    "                                        'battery': self.MAX_BATTERY_POWER, 'has_parcel': 0} for agent in self.agents}\n",
    "        \n",
    "        if self.BASELINE_FLAG:\n",
    "            for parcel in self.state['parcels']:\n",
    "                self.compute_central_delivery(parcel, debug_log=False)\n",
    "                \n",
    "                # TODO really return something here??\n",
    "                self.metrics['optimal'] = self.compute_optimality_baseline(parcel, extra_charge=self.BASELINE_OPT_CONSTANT, debug_log=False)\n",
    "        \n",
    "        # compute allowed actions per agent --> move to function\n",
    "        for agent in self.agents:\n",
    "            self.update_allowed_actions_nodes(agent)\n",
    "            self.update_allowed_actions_parcels(agent)    \n",
    "            \n",
    "        agent_obs = {agent: {'obs': {'state':  {'position': state['position'], 'battery': state['battery'],\n",
    "                                        'has_parcel': state['has_parcel'],'current_step': self.current_step},\n",
    "                            'parcels': parcel_obs\n",
    "                            },\n",
    "                            'allowed_actions': self.allowed_actions[agent]\n",
    "                            } for agent, state in self.state['agents'].items()\n",
    "                    } \n",
    "        \n",
    "        return {**agent_obs}\n",
    "    \n",
    "    \n",
    "    def _random_feasible_agent_position(self, agent_id):\n",
    "        \"\"\"Needed to avoid car agents being initialized at nodes only reachable by drones\n",
    "        and thus trapped from the beginning. Ensures that car agents start at a node of type 'road' or 'parking'.\n",
    "        \"\"\"\n",
    "        position = random.randrange(self.NUMBER_OF_NODES)\n",
    "        if agent_id[0] == 'c':    # agent is car \n",
    "            while self.graph.nodes[position]['type'] == 'air':      # position not reachable by car\n",
    "                position = random.randrange(self.NUMBER_OF_NODES)\n",
    "        return position\n",
    "    \n",
    "    \n",
    "    def update_allowed_actions_nodes(self, agent):\n",
    "        new_pos = self.state['agents'][agent]['position']\n",
    "        next_steps = list(self.graph.neighbors(new_pos))\n",
    "        agent_type = 'air' if agent[0]=='d' else 'road'\n",
    "        \n",
    "        allowed_nodes = np.zeros(self.NUMBER_OF_NODES)\n",
    "        for neighbor in next_steps:\n",
    "            if self.graph[new_pos][neighbor]['type'] in [agent_type, 'both']:\n",
    "                allowed_nodes[neighbor] = 1\n",
    "        self.allowed_actions[agent][1:self.NUMBER_OF_NODES+1] = np.array(allowed_nodes).astype(bool)\n",
    "                 \n",
    "            \n",
    "    def update_allowed_actions_parcels(self, agent):\n",
    "        \"\"\" Allow only the Dropoff or Pickup actions, depending on the has_parcel value of the agent.\n",
    "        Pickup is not concerned with the parcel actually being at the agents current location, only with free capacity\n",
    "        and the parcel already being added to the ENV\"\"\"\n",
    "        num_parcels = len(self.state['parcels'])\n",
    "                          \n",
    "        allowed_parcels = np.zeros(self.MAX_NUMBER_OF_PARCELS)\n",
    "        dropoff = 1\n",
    "        if self.state['agents'][agent]['has_parcel'] == 0:\n",
    "            dropoff = 0\n",
    "            allowed_parcels = np.concatenate([np.ones(num_parcels), np.zeros(self.MAX_NUMBER_OF_PARCELS - num_parcels)])\n",
    "                          \n",
    "        self.allowed_actions[agent][self.NUMBER_OF_NODES+1:] = np.array([dropoff, *allowed_parcels]).astype(bool)\n",
    "    \n",
    "    \n",
    "    def get_parcel_obs(self):\n",
    "        parcel_obs = [{'id':pid, 'location': parcel[0], 'destination':parcel[1]} for (pid, parcel) in self.state['parcels'].items()]\n",
    "        return parcel_obs\n",
    "    \n",
    "    def generate_parcel(self):\n",
    "        \"\"\"generate new parcel id and new parcel with random nodes for location and destination.\n",
    "            p_ids (int) start at 1, later nodes for parcel will be sampled to avoid parcels that already spawn at their destination\"\"\"\n",
    "        p_id = len(self.state['parcels']) + 1\n",
    "\n",
    "        parcel = random.sample(range(self.NUMBER_OF_NODES), 2)  # => initial location != destination\n",
    "        self.metrics['added'].update({p_id: self.current_step})\n",
    "        \n",
    "        return p_id, parcel\n",
    "    \n",
    "    def prepare_global_reward(self, _episode_success=False):\n",
    "        \"\"\" computes a global reward for all active agents still in the environment.\n",
    "            If _episode_success is set to True, all parcels have been delivered an the ALL_DELIVERED reward is granted.\n",
    "            :Returns: list of active agents and the reward value\n",
    "        \"\"\"\n",
    "        agents_alive = list(set(self.agents).difference(set(self.done_agents)))\n",
    "        if self.DEBUG_LOG: assert len(agents_alive) > 0\n",
    "        reward = self.PARCEL_DELIVERED * (self.NUMBER_STEPS_PER_EPISODE - self.current_step) / self.NUMBER_STEPS_PER_EPISODE if _episode_success else self.PARCEL_DELIVERED / len(agents_alive)\n",
    "\n",
    "        return agents_alive, reward\n",
    "    \n",
    "    \n",
    "    #------ BASELINE related methods ------###         \n",
    "    def compute_optimality_baseline(self, parcel_id, extra_charge=2.5, debug_log=False):\n",
    "        \"\"\"Used in the optimality baseline\n",
    "            Input: parcel_id, (extra_charge)\n",
    "            Output: new total delivery rounds needed for all parcels\n",
    "        \"\"\"\n",
    "        parcel = self.state['parcels'][parcel_id]\n",
    "        path_time = 2 + shortest_path_length(self.graph, parcel[0], parcel[1], 'weight')\n",
    "        \n",
    "        _time = math.ceil(path_time * extra_charge) # round to next higher integer\n",
    "        \n",
    "        min_index = self.o_employed.index(min(self.o_employed))\n",
    "        self.o_employed[min_index] += _time\n",
    "                                    \n",
    "        return max(self.o_employed)\n",
    "        \n",
    "    \n",
    "    def compute_central_delivery(self, p_id, debug_log = False):\n",
    "        \"\"\"Used in the central baseline, iteratively tries to find a good delivery route \n",
    "        with the available agents in the time specified in BASELINE_TIME_CONSTRAINT\n",
    "            Input: parcel_id\n",
    "            Output: Dict: {agent_id: [actions], ...}  --> update that dict! (merge in this function with prev actions!) \n",
    "        \"\"\"\n",
    "\n",
    "        if self.agents_base is None:\n",
    "            self.agents_base = {a_id: (a['position'], 0) for (a_id, a) in self.state[\"agents\"].items()}  # last instructed pos + its step count\n",
    "\n",
    "        min_time = None\n",
    "        new_missions = {}   # key: agent, value: [actions]\n",
    "\n",
    "        source = self.state[\"parcels\"][p_id][0]\n",
    "        target = self.state[\"parcels\"][p_id][1]\n",
    "        shortest_paths_generator = nx.shortest_simple_paths(self.graph, source, target, weight='weight')\n",
    "\n",
    "        running = BoolTimer(self.BASELINE_TIME_CONSTRAINT)\n",
    "        running.start()             \n",
    "        \n",
    "        while running:\n",
    "            # Default --> assign full route to nearest drone\n",
    "            if min_time is None:   \n",
    "\n",
    "                air_route = nx.shortest_path(self.graph, source= source, target= target, weight=\"air\")\n",
    "                air_route_time = nx.shortest_path_length(self.graph, source= source, target= target, weight=\"air\")\n",
    "                air_route.pop(0)  # remove source node from path\n",
    "               \n",
    "                # Assign most suitable Drone\n",
    "                best_drone = None\n",
    "                for (a_id, a_tp) in self.agents_base.items():\n",
    "                    if a_id[0] != 'd':                      # filter for correct agent type\n",
    "                        continue\n",
    "                        \n",
    "                    journey_time = nx.shortest_path_length(self.graph, source=a_tp[0], target=source, weight=\"air\") + a_tp[1]\n",
    "                    if min_time is None or journey_time < min_time:\n",
    "                        min_time = journey_time\n",
    "                        best_drone = (a_id, a_tp)\n",
    "  \n",
    "                # construct path for agent\n",
    "                drone_route = nx.shortest_path(self.graph, source= best_drone[1][0], target= source, weight=\"air\")\n",
    "\n",
    "                drone_route_actions = [x+1 for x in drone_route[1:]] + [(self.ACTION_DROPOFF + p_id, 0)] + [x+1 for x in air_route[1:]] + [self.ACTION_DROPOFF]   # increment node_ids by one to get corresponding action\n",
    "                min_time += air_route_time + 2 # add 2 steps for pick & drop\n",
    "                \n",
    "                self._add_charging_stops_to_route(drone_route_actions, debug_log=debug_log)              \n",
    "                \n",
    "                new_missions[best_drone[0]] = (drone_route_actions, min_time) \n",
    "                \n",
    "            else:   # try to improve the existing base mission  \n",
    "                try:\n",
    "                    shortest_route = next(shortest_paths_generator)\n",
    "                except StopIteration:\n",
    "                    break               # all existing shortest paths already tried\n",
    "                    \n",
    "                subroutes = self._path_to_subroutes(shortest_route, debug_log=debug_log)\n",
    "                duration, min_agents = self._find_best_agents(subroutes, min_time, debug_log=debug_log)\n",
    "\n",
    "                if duration < min_time:\n",
    "                    # faster delivery route found!          \n",
    "                    assert duration < min_time, \"Central Baseline prefered a longer route...\"\n",
    "                    \n",
    "                    # update min_time\n",
    "                    min_time = duration                    \n",
    "                    new_missions = self._build_missions(min_agents, subroutes, p_id, debug_log=debug_log) \n",
    "\n",
    "        #---- end while = Timer\n",
    "        # now save the best mission found in the ENV\n",
    "        for agent in new_missions.keys():\n",
    "            # retrieve target node from mission, depends on charging stops and case no move necessary    \n",
    "            target = None     \n",
    "            if isinstance(new_missions[agent][0][-2], int):    \n",
    "                target = new_missions[agent][0][-2] \n",
    "                if target == 0: target = new_missions[agent][0][-3]    # charging action was added before dropoff\n",
    "                target -= 1                                            # action-1 = node_id\n",
    "            else:  # handle case no move necessary -> pick action before dropoff\n",
    "                target = self.agents_base[agent][0]\n",
    "\n",
    "            self.agents_base[agent] = (target, self.agents_base[agent][1] + new_missions[agent][1])  \n",
    "            self.baseline_missions[agent].extend(new_missions[agent][0])\n",
    "            \n",
    "        return new_missions\n",
    "        \n",
    "        \n",
    "    def _find_best_agents(self, subroutes, min_time, debug_log=False):\n",
    "        \"\"\" For use in centrality_baseline. Finds best available agents for traversing a set of subroutes \n",
    "            and returns these with the total duration for doing so.\n",
    "            Input: subroutes = [ edge_type, [actions]]\n",
    "            \"\"\"\n",
    "        \n",
    "        min_agents = {}\n",
    "        temp_agents_base = {k:v for k,v in self.agents_base.items()}  # deep copy for temporary planning transfer or stick with agent??\n",
    "\n",
    "        for i,r in enumerate(subroutes):\n",
    "\n",
    "            # init some helper vars\n",
    "            a_type = \"d\" if r[0] == 'air' else 'c' \n",
    "            min_time_sub = None    # best time (min) for this subroute (closest agent)\n",
    "            best_agent_sub = None\n",
    "\n",
    "            # iterate over agents of correct type!  --> later: Busy / unbusy\n",
    "            for (a_id, a_tp) in temp_agents_base.items():\n",
    "                #reminder: a_tp is tuple of latest future position (node, timestep)\n",
    "\n",
    "                # filter for correct agent type - even if type is 'both' one agent can still take only its edge type\n",
    "                weight_agent = r[0]\n",
    "                if r[0] == 'both':\n",
    "                    weight_agent = 'road' if a_id[0] == 'c' else 'air'\n",
    "                else:                     # wrong agent type\n",
    "                    if a_id[0] != a_type:  # todo replace with parameter variable in function!\n",
    "                        continue \n",
    "                        \n",
    "                journey_time = nx.shortest_path_length(self.graph, source=a_tp[0], target=r[1][0], weight=weight_agent) + a_tp[1] # earliest time agent can be there\n",
    "                if min_time_sub is None or journey_time < min_time_sub:\n",
    "                    min_time_sub = journey_time\n",
    "                    best_agent_sub = (a_id, a_tp)    # a_id, a_tp = (latest location, timestep)\n",
    "\n",
    "            # closest available agent found        \n",
    "            best_agent_weight = weight_agent = 'road' if best_agent_sub[0] == 'c' else 'air'\n",
    "            duration_sub = min_time_sub + nx.shortest_path_length(self.graph, source=r[1][0], target=r[1][-1], weight=best_agent_weight) + 1\n",
    "\n",
    "            # update agent state in temporary planning\n",
    "            temp_agents_base[best_agent_sub[0]] = (r[1][-1], duration_sub)\n",
    "\n",
    "            # agent_tuple, duration_subroutes_until_then\n",
    "            min_agents[i] = (best_agent_sub, duration_sub)\n",
    "            \n",
    "            if debug_log: assert duration_sub < sys.maxsize, \"Non existent edge taken somewhere...\"\n",
    "\n",
    "            # check if current subroute already longer than the min one\n",
    "            if duration_sub > min_time:\n",
    "                break # already worse, check next simple path\n",
    "                \n",
    "        return duration_sub, min_agents\n",
    "    \n",
    "    \n",
    "    def _build_missions(self, min_agents, subroutes, parcel_id, debug_log = False):\n",
    "        \"\"\"For use in centrality_baseline. Computes list of actions for delivery of parcel parcel_id \n",
    "        and necessary duration for execution from list of agents, subroutes\"\"\"\n",
    "        new_mission = {}\n",
    "\n",
    "        for i, s in enumerate(subroutes):\n",
    "            best_agent_pos = min_agents[i][0][1][0]\n",
    "            #earliest time to start that action\n",
    "            time_pickup = min_agents[i-1][1] if i > 0 else 0 # First subroute pickup as soon as possible\n",
    "            \n",
    "            # construct the actual delivery path to pickup\n",
    "            delivery_route = nx.shortest_path(self.graph, source=best_agent_pos, target=s[1][0], weight=s[0])\n",
    "            delivery_route_actions = [x+1 for x in delivery_route[1:]] + [(self.ACTION_DROPOFF + parcel_id, time_pickup)] + [x+1 for x in s[1][1:]] + [self.ACTION_DROPOFF]\n",
    "\n",
    "            if debug_log: assert min_agents[i][0] not in new_mission  #I see no preferable case where agent picks-up 1 parcel 2 times --> holding always better than following\n",
    "\n",
    "            self._add_charging_stops_to_route(delivery_route_actions, debug_log=debug_log)\n",
    "            new_mission[min_agents[i][0][0]] = (delivery_route_actions, min_agents[i][1])\n",
    "            \n",
    "        return new_mission\n",
    "    \n",
    "    \n",
    "    def _add_charging_stops_to_route(self, route_actions, debug_log=False):\n",
    "        \"\"\"For use in centrality_baseline. Iterates through a list of actions and inserts a charging action\n",
    "        after every move action to a node with a charging station. \n",
    "        Tuples representing Dropoff actions with minimal executions time are updated to account for eventual delays. \"\"\"\n",
    "        \n",
    "        delay = 0\n",
    "        for i, n in enumerate(route_actions):\n",
    "            if type(n) is tuple:\n",
    "                if delay > 0: route_actions[i] = (n[0], n[1] + delay)\n",
    "            else: \n",
    "                if n-1 in self.CHARGING_STATION_NODES:\n",
    "                    delay += 1\n",
    "                    route_actions.insert(i+1, 0)\n",
    "\n",
    "\n",
    "    def _path_to_subroutes(self, path, debug_log= False):\n",
    "        \"\"\"For use in centrality_baseline. Takes path in the graph as input and returns list of subroutes \n",
    "        split at changes of the edge types with the type\"\"\"\n",
    "\n",
    "        # get subroutes by their edge_types\n",
    "        e_type_prev = None\n",
    "        e_type_changes = [] # save indices of source nodes before new edge type\n",
    "        subroutes = []\n",
    "        _subroute = [path[0]] \n",
    "\n",
    "        if len(path) > 1:\n",
    "            e_type_prev = self.graph.edges[path[0], path[1]]['type']\n",
    "    \n",
    "            for i, node in enumerate(path[1:-1], start=1):\n",
    "    \n",
    "                e_type_next = self.graph.edges[node, path[i+1]]['type']\n",
    "    \n",
    "                _subroute.append(node)\n",
    "                if e_type_next != e_type_prev:   \n",
    "                    subroutes.append((e_type_prev, _subroute))\n",
    "                    _subroute = [node]\n",
    "                    e_type_prev = e_type_next\n",
    "    \n",
    "            _subroute.append(path[-1])\n",
    "        subroutes.append((e_type_prev, _subroute))   # don't forget last subroute           \n",
    "    \n",
    "        return subroutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6698c6",
   "metadata": {},
   "source": [
    "## Agent Model and Experiment Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14b96755",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete, Box, Tuple, MultiDiscrete, Dict, MultiBinary\n",
    "#from ray.rllib.utils.spaces.space_utils import flatten_space\n",
    "#from ray.rllib.models.preprocessors import DictFlatteningPreprocessor\n",
    "\n",
    "# Parametric-action agent model  --> apply Action Masking!\n",
    "class ParametricAgentModel(TFModelV2):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name, *args, **kwargs):\n",
    "        super(ParametricAgentModel, self).__init__(obs_space, action_space, num_outputs, model_config, name, *args, **kwargs)\n",
    "\n",
    "        assert isinstance(action_space, Discrete), f'action_space is a {type(action_space)}, but should be Discrete!'\n",
    "        \n",
    "        # Adjust for number of agents/parcels/Nodes!! -> Simply copy found shape from the thrown exception\n",
    "        true_obs_shape = (1750, )\n",
    "        action_embed_size = action_space.n\n",
    "        \n",
    "        self.action_embed_model = FullyConnectedNetwork(\n",
    "            Box(0, 1, shape=true_obs_shape),\n",
    "            action_space,\n",
    "            action_embed_size,\n",
    "            model_config,\n",
    "            name + '_action_embedding')\n",
    "        \n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        \n",
    "        action_mask = input_dict['obs']['allowed_actions']\n",
    "        action_embedding, _ = self.action_embed_model.forward({'obs_flat': input_dict[\"obs_flat\"]}, state, seq_lens)\n",
    "        intent_vector = tf.expand_dims(action_embedding, 1)\n",
    "        action_logits = tf.reduce_sum(intent_vector, axis=1)\n",
    "        inf_mask = tf.maximum(tf.math.log(action_mask), tf.float32.min)\n",
    "\n",
    "        return action_logits + inf_mask, state\n",
    "    \n",
    "    def value_function(self):\n",
    "        return self.action_embed_model.value_function()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2017a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Proposed way to train / evaluate MARL policy from github Issues: --> https://github.com/ray-project/ray/issues/9123 and  https://github.com/ray-project/ray/issues/9208\n",
    "\n",
    "def train(config, name, save_dir, stop_criteria, num_samples, verbosity=1):\n",
    "    \"\"\"\n",
    "    Train an RLlib agent using tune until any of the configured stopping criteria is met.\n",
    "    :param stop_criteria: Dict with stopping criteria.\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run\n",
    "    :return: Return the path to the saved agent (checkpoint) and tune's ExperimentAnalysis object\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/analysis.html#experimentanalysis-tune-experimentanalysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Start training\")\n",
    "    analysis = ray.tune.run(DQNTrainer, verbose=verbosity, config=config, local_dir=save_dir, \n",
    "                            stop=stop_criteria, name=name, num_samples=num_samples,\n",
    "                            checkpoint_at_end=True, resume=True)\n",
    "    \n",
    "    # list of lists: one list per checkpoint; each checkpoint list contains 1st the path, 2nd the metric value\n",
    "    checkpoints = analysis.get_trial_checkpoints_paths(trial=analysis.get_best_trial('episode_reward_mean', mode='max'),\n",
    "                                                       metric='episode_reward_mean')\n",
    "    # retriev the checkpoint path; we only have a single checkpoint, so take the first one\n",
    "    checkpoint_path = checkpoints[0][0]\n",
    "    print(f\"Saved trained model in checkpoint {checkpoint_path} - achieved episode_reward_mean: {checkpoints[0][1]}\")\n",
    "    return checkpoint_path, analysis\n",
    "\n",
    "\n",
    "def load(config, path):\n",
    "    \"\"\"\n",
    "    Load a trained RLlib agent from the specified path. Call this before testing the trained agent.\n",
    "    \"\"\"\n",
    "    agent = DQNTrainer(config=config) #, env=env_class)\n",
    "    agent.restore(path)\n",
    "    return agent\n",
    "    \n",
    "    \n",
    "def test(env_class, env_config, policy_mapping_fcn, agent):\n",
    "    \"\"\"Test trained agent for a single episode. Return the retrieved env metrics for this episode and the episode reward\"\"\"\n",
    "    # instantiate env class\n",
    "    env = env_class(env_config)\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    \n",
    "    while not done: # run until episode ends\n",
    "        actions = {}\n",
    "        for agent_id, agent_obs in obs.items():\n",
    "            # Here: policy_id == agent_id - added this to avoid confusion for other policy mappings \n",
    "            policy_id = policy_mapping_fcn(agent_id, episode=None, worker=None) \n",
    "            actions[agent_id] = agent.compute_action(agent_obs, policy_id=policy_id)\n",
    "        obs, reward, done, info = env.step(actions)\n",
    "        done = done['__all__']\n",
    "        \n",
    "        # sum up reward for all agents\n",
    "        episode_reward += sum(reward.values())\n",
    "        \n",
    "    # Retrieve custom metrics from ENV\n",
    "    return env.metrics, episode_reward \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14c5ec6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  Import the corresponding Trainer class for your algorithm of choice here - needed for the evaluation.\n",
    "from ray.rllib.agents.dqn import DQNTrainer\n",
    "\n",
    "\n",
    "def train_and_test_scenarios(config, seeds=None):\n",
    "    \"\"\" Trains for a single scenario indicated by a seed \"\"\"\n",
    "    # TODO how to distinguish between the different algos ? \n",
    "    print(\"Starte: run_function_trainer!\")\n",
    "    \n",
    "    # prepare the config dicts\n",
    "    NAME = config['NAME']\n",
    "    SAVE_DIR = config['SAVE_DIR']\n",
    "    ENVIRONMENT = config['ENV']\n",
    "    \n",
    "    # Simulations\n",
    "    NUMBER_STEPS_PER_EPISODE = config['NUMBER_STEPS_PER_EPISODE']\n",
    "    STOP_CRITERIA = config['STOP_CRITERIA']\n",
    "    NUMBER_OF_SAMPLES = config['NUMBER_OF_SAMPLES']\n",
    "    \n",
    "    #MAP / ENV\n",
    "    NUMBER_OF_DRONES = config['NUMBER_OF_DRONES']\n",
    "    NUMBER_OF_CARS = config['NUMBER_OF_CARS']\n",
    "    NUMBER_OF_AGENTS = NUMBER_OF_DRONES + NUMBER_OF_CARS\n",
    "    MAX_NUMBER_OF_PARCELS = config['MAX_NUMBER_OF_PARCELS']\n",
    "    \n",
    "    # TESTING\n",
    "    SEEDS = config['SEEDS']\n",
    "    \n",
    "    env_config = {\n",
    "            'DEBUG_LOGS':False,\n",
    "            'TOPOLOGY': config['TOPOLOGY'],\n",
    "            # Simulation config\n",
    "            'NUMBER_STEPS_PER_EPISODE': NUMBER_STEPS_PER_EPISODE,\n",
    "            #'NUMBER_OF_TIMESTEPS': NUMBER_OF_TIMESTEPS,\n",
    "            'RANDOM_SEED': None, # 42\n",
    "            # Map\n",
    "            'CHARGING_STATION_NODES': config['CHARGING_STATION_NODES'],\n",
    "            # Entities\n",
    "            'NUMBER_OF_DRONES': NUMBER_OF_DRONES,\n",
    "            'NUMBER_OF_CARS': NUMBER_OF_CARS,\n",
    "            'MAX_BATTERY_POWER': config['MAX_BATTERY_POWER'],  # TODO split this for drone and car??\n",
    "            'INIT_NUMBER_OF_PARCELS': config['INIT_NUMBER_OF_PARCELS'],\n",
    "            'MAX_NUMBER_OF_PARCELS': config['MAX_NUMBER_OF_PARCELS'],\n",
    "            'THRESHOLD_ADD_NEW_PARCEL': config['THRESHOLD_ADD_NEW_PARCEL'],\n",
    "            # Baseline settings\n",
    "            'BASELINE_FLAG': False,  # is set True in the test function when needed\n",
    "            'BASELINE_OPT_CONSTANT': config['BASELINE_OPT_CONSTANT'],\n",
    "            'BASELINE_TIME_CONSTRAINT': config['BASELINE_TIME_CONSTRAINT'],\n",
    "            # TODO \n",
    "            #Rewards\n",
    "            'REWARDS': config['REWARDS'] \n",
    "        }\n",
    "        \n",
    "    run_config = {\n",
    "        'num_gpus': config['NUM_GPUS'],\n",
    "        'num_workers': config['NUM_WORKERS'],\n",
    "        'env': ENVIRONMENT,\n",
    "        'env_config': env_config,\n",
    "        'multiagent': {\n",
    "            'policies': {\n",
    "                # tuple values: policy, obs_space, action_space, config\n",
    "                **{a: (None, None, None, { 'model': {'custom_model': ParametricAgentModel }, 'framework': 'tf'}) for a in ['d_'+ str(j) for j in range(NUMBER_OF_DRONES)] + ['c_'+ str(i) for i in range(NUMBER_OF_DRONES, NUMBER_OF_CARS + NUMBER_OF_DRONES)]}\n",
    "            },\n",
    "            'policy_mapping_fn': policy_mapping_fn,\n",
    "            'policies_to_train': ['d_'+ str(i) for i in range(NUMBER_OF_DRONES)] + ['c_'+ str(i) for i in range(NUMBER_OF_DRONES, NUMBER_OF_CARS + NUMBER_OF_DRONES)]\n",
    "        },\n",
    "        #'log_level': \"INFO\",\n",
    "        \"hiddens\": [],     # For DQN\n",
    "        \"dueling\": False,  # For DQN\n",
    "    }\n",
    "    \n",
    "    # Train and Evaluate the agents !\n",
    "    checkpoint, analysis = train(run_config, NAME, SAVE_DIR, STOP_CRITERIA, NUMBER_OF_SAMPLES)\n",
    "    print(\"Training finished - Checkpoint: \", checkpoint)\n",
    "    \n",
    "    env_class = ENVIRONMENT\n",
    "    # Restore trained policies for evaluation\n",
    "    agent = load(run_config, checkpoint)\n",
    "    print(\"Agent loaded - Agent: \", agent)\n",
    "    \n",
    "    # Run the test cases for the specified seeds\n",
    "    seeds = SEEDS#[10, 91, 32, 75, 6]\n",
    "    runs = {'max_steps': NUMBER_STEPS_PER_EPISODE, 'max_parcels': MAX_NUMBER_OF_PARCELS, 'max_agents': NUMBER_OF_AGENTS}\n",
    "    \n",
    "    for seed in seeds:\n",
    "        env_config['RANDOM_SEED'] = seed\n",
    "        # TODO check if \n",
    "        assert run_config['env_config']['RANDOM_SEED'] == seed\n",
    "        result = test_scenario(run_config, agent)\n",
    "        runs.update(result)\n",
    "        \n",
    "    return runs\n",
    "\n",
    "\n",
    "def test_scenario(config, agent):\n",
    "    \"\"\"\n",
    "    Loads a pretrained agent, initializes an environment from the seed \n",
    "    and then evaluates it over one episode with the Marl agents and the central baseline.\n",
    "\n",
    "    Returns: dict with results for graph creation for both evaluation / inference runs\n",
    "    \"\"\"\n",
    "    #TODO unterscheidung run_config > env_config\n",
    "    # Wie die beiden Runs abspeichern --> {Seed + [marl / base] : result }\n",
    "\n",
    "    env_class = config['env']  \n",
    "    env_config = config['env_config']\n",
    "    seed = env_config['RANDOM_SEED']\n",
    "    policy_mapping_fn = config['multiagent']['policy_mapping_fn']\n",
    "    \n",
    "    # Test with MARL\n",
    "\n",
    "    metrics_marl, reward_marl = test(env_class, env_config, policy_mapping_fn, agent)\n",
    "    \n",
    "    # Test with CentralBase\n",
    "    env_config['BASELINE_FLAG'] = True\n",
    "    metrics_central, reward_central = test(env_class, env_config, policy_mapping_fn, agent)\n",
    "    env_config['BASELINE_FLAG'] = False\n",
    "    \n",
    "    # ASSERT that both optimal values are equal\n",
    "    #assert metrics_marl['optimal'] == metrics_central['optimal']\n",
    "    \n",
    "    return {\"M_\" + str(seed): metrics_marl, \"C_\" + str(seed): metrics_central}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d86f07bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-26 11:36:35 (running for 00:00:00.32)<br>Memory usage on this node: 625.3/1007.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/96 CPUs, 0/8 GPUs, 0.0/278.04 GiB heap, 0.0/123.15 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /work-ceph/albecker/Exp_thesis/Exp_algorithms/dqn<br>Number of trials: 2/2 (2 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-26 11:36:35,669\tINFO tune.py:636 -- Total run time: 0.49 seconds (0.00 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model in checkpoint /work-ceph/albecker/Exp_thesis/Exp_algorithms/dqn/DQNTrainer_Map_Environment_ca46a_00000_0_2022-02-25_20-19-02/checkpoint_023437/checkpoint-23437 - achieved episode_reward_mean: -717.4399999999998\n",
      "Training finished - Checkpoint:  /work-ceph/albecker/Exp_thesis/Exp_algorithms/dqn/DQNTrainer_Map_Environment_ca46a_00000_0_2022-02-25_20-19-02/checkpoint_023437/checkpoint-23437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434656)\u001b[0m 2022-02-26 11:36:51,128\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434663)\u001b[0m 2022-02-26 11:36:51,363\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434654)\u001b[0m 2022-02-26 11:36:51,559\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434650)\u001b[0m 2022-02-26 11:36:51,936\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434674)\u001b[0m 2022-02-26 11:36:52,081\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434652)\u001b[0m 2022-02-26 11:36:52,394\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434658)\u001b[0m 2022-02-26 11:36:52,394\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434643)\u001b[0m 2022-02-26 11:36:52,394\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434653)\u001b[0m 2022-02-26 11:36:52,422\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434665)\u001b[0m 2022-02-26 11:36:52,444\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434642)\u001b[0m 2022-02-26 11:36:52,506\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434635)\u001b[0m 2022-02-26 11:36:52,502\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434649)\u001b[0m 2022-02-26 11:36:52,486\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434644)\u001b[0m 2022-02-26 11:36:52,630\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434651)\u001b[0m 2022-02-26 11:36:52,814\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434646)\u001b[0m 2022-02-26 11:36:52,995\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434641)\u001b[0m 2022-02-26 11:36:52,949\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434655)\u001b[0m 2022-02-26 11:36:53,106\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434638)\u001b[0m 2022-02-26 11:36:53,072\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434645)\u001b[0m 2022-02-26 11:36:53,072\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434640)\u001b[0m 2022-02-26 11:36:53,108\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434660)\u001b[0m 2022-02-26 11:36:53,189\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434632)\u001b[0m 2022-02-26 11:36:53,271\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434630)\u001b[0m 2022-02-26 11:36:53,313\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434647)\u001b[0m 2022-02-26 11:36:53,398\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434659)\u001b[0m 2022-02-26 11:36:53,457\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434639)\u001b[0m 2022-02-26 11:36:53,533\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434636)\u001b[0m 2022-02-26 11:36:53,541\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434637)\u001b[0m 2022-02-26 11:36:53,502\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434634)\u001b[0m 2022-02-26 11:36:53,603\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434633)\u001b[0m 2022-02-26 11:36:53,588\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434631)\u001b[0m 2022-02-26 11:36:53,701\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-26 11:37:01,346\tINFO trainable.py:125 -- Trainable.setup took 22.354 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-02-26 11:37:01,352\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-02-26 11:37:01,673\tINFO trainable.py:472 -- Restored on 134.155.89.136 from checkpoint: /work-ceph/albecker/Exp_thesis/Exp_algorithms/dqn/DQNTrainer_Map_Environment_ca46a_00000_0_2022-02-25_20-19-02/checkpoint_023437/checkpoint-23437\n",
      "2022-02-26 11:37:01,674\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 23437, '_timesteps_total': 749984, '_time_total': 38130.08373045921, '_episodes_total': 20667}\n",
      "2022-02-26 11:37:01,680\tWARNING deprecation.py:45 -- DeprecationWarning: `compute_action` has been deprecated. Use `Trainer.compute_single_action()` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434663)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434663)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434663)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434656)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434656)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434656)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434654)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434654)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434654)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434652)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434652)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434652)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434650)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434650)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434650)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434658)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434658)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434658)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434674)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434674)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434674)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434642)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434642)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434642)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434643)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434643)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434643)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434651)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434651)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434651)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434653)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434653)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434653)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434665)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434665)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434665)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434644)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434644)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434644)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434646)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434646)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434646)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434635)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434635)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434635)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434649)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434649)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434649)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434659)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434659)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434659)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434655)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434655)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434655)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434641)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434641)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434641)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434638)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434638)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434638)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434645)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434645)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434645)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434632)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434632)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434632)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434647)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434647)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434647)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434640)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434640)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434640)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434660)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434660)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434660)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434630)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434630)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434630)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434634)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434634)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434634)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434633)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434633)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434633)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434639)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434639)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434639)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434636)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434636)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434636)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434637)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434637)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434637)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434631)\u001b[0m WARNING:tensorflow:From /home/albecker/anaconda3/envs/jupyter/lib/python3.9/site-packages/ray/rllib/utils/exploration/epsilon_greedy.py:218: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434631)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3434631)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent loaded - Agent:  DQNTrainer\n",
      "Test evaluation finished ;)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    return agent_id\n",
    "\n",
    "basic_config = {\n",
    "        # experiment\n",
    "        'NAME': 'dqn',\n",
    "        'SAVE_DIR': 'Exp_algorithms',\n",
    "        'ALGO': DQNTrainer,\n",
    "        'ENV': Map_Environment,\n",
    "        'DEBUG_LOGS':False,\n",
    "        'NUM_GPUS': 2,\n",
    "        'NUM_WORKERS': 32,\n",
    "        'NUMBER_OF_SAMPLES': 2,\n",
    "        # Simulation config\n",
    "        'NUMBER_STEPS_PER_EPISODE': 1200,\n",
    "        'RANDOM_SEED': None, # 42\n",
    "        # Map\n",
    "        'TOPOLOGY': topology,\n",
    "        'CHARGING_STATION_NODES': [0,1,2,3,4,6,9],\n",
    "        # Entities\n",
    "        'NUMBER_OF_DRONES': 2,\n",
    "        'NUMBER_OF_CARS': 2,\n",
    "        'INIT_NUMBER_OF_PARCELS': 10,\n",
    "        'MAX_NUMBER_OF_PARCELS': 10,\n",
    "        'THRESHOLD_ADD_NEW_PARCEL': 0.1, # 10% chance\n",
    "        'MAX_BATTERY_POWER': 100, \n",
    "        #Baseline\n",
    "        'BASELINE_TIME_CONSTRAINT': 10,\n",
    "        'BASELINE_OPT_CONSTANT': 2.5,\n",
    "        #TESTING\n",
    "        'SEEDS': [72, 21, 44, 66, 86, 14],\n",
    "        #Rewards\n",
    "        'REWARDS': {\n",
    "            'PARCEL_DELIVERED': 200,\n",
    "            'STEP_PENALTY': -0.1,\n",
    "        }, \n",
    "        'STOP_CRITERIA': {\n",
    "            'timesteps_total': 24_000_000,\n",
    "        }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "print(\"Test evaluation functions\")\n",
    "experiment_results = train_and_test_scenarios(basic_config)\n",
    "print(\"Test evaluation finished ;)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c123aef",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_steps': 1200,\n",
       " 'max_parcels': 10,\n",
       " 'max_agents': 4,\n",
       " 'M_72': {'step': 1200,\n",
       "  'delivered': {},\n",
       "  'crashed': {'c_3': 604},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': None},\n",
       " 'C_72': {'step': 1200,\n",
       "  'delivered': {'p_4': 49,\n",
       "   'p_1': 67,\n",
       "   'p_3': 67,\n",
       "   'p_7': 95,\n",
       "   'p_5': 126,\n",
       "   'p_10': 224,\n",
       "   'p_9': 403},\n",
       "  'crashed': {},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': 75},\n",
       " 'M_21': {'step': 1200,\n",
       "  'delivered': {},\n",
       "  'crashed': {'c_3': 220},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': None},\n",
       " 'C_21': {'step': 1200,\n",
       "  'delivered': {'p_1': 47,\n",
       "   'p_6': 76,\n",
       "   'p_3': 135,\n",
       "   'p_9': 215,\n",
       "   'p_7': 350,\n",
       "   'p_8': 539},\n",
       "  'crashed': {},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': 105},\n",
       " 'M_44': {'step': 1200,\n",
       "  'delivered': {},\n",
       "  'crashed': {'d_0': 78, 'c_3': 711},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': None},\n",
       " 'C_44': {'step': 1200,\n",
       "  'delivered': {'p_7': 253, 'p_8': 258},\n",
       "  'crashed': {},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': 103},\n",
       " 'M_66': {'step': 1200,\n",
       "  'delivered': {'p_10': 1116},\n",
       "  'crashed': {'d_1': 186},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': None},\n",
       " 'C_66': {'step': 1200,\n",
       "  'delivered': {'p_1': 16, 'p_3': 18, 'p_5': 70},\n",
       "  'crashed': {},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': 85},\n",
       " 'M_86': {'step': 1200,\n",
       "  'delivered': {},\n",
       "  'crashed': {'c_3': 195},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': None},\n",
       " 'C_86': {'step': 1200,\n",
       "  'delivered': {'p_8': 66,\n",
       "   'p_1': 90,\n",
       "   'p_9': 110,\n",
       "   'p_4': 112,\n",
       "   'p_7': 168,\n",
       "   'p_2': 185,\n",
       "   'p_6': 282},\n",
       "  'crashed': {},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': 90},\n",
       " 'M_14': {'step': 1200,\n",
       "  'delivered': {},\n",
       "  'crashed': {'c_3': 926},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': None},\n",
       " 'C_14': {'step': 1200,\n",
       "  'delivered': {'p_1': 12, 'p_3': 12, 'p_6': 66},\n",
       "  'crashed': {},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': 108}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_results = {'max_steps': 1200,\n",
    " 'max_parcels': 10,\n",
    " 'max_agents': 4,\n",
    " 'M_72': {'step': 1200,\n",
    "  'delivered': {},\n",
    "  'crashed': {'c_3': 604},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': None},\n",
    " 'C_72': {'step': 1200,\n",
    "  'delivered': {'p_4': 49,\n",
    "   'p_1': 67,\n",
    "   'p_3': 67,\n",
    "   'p_7': 95,\n",
    "   'p_5': 126,\n",
    "   'p_10': 224,\n",
    "   'p_9': 403},\n",
    "  'crashed': {},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': 75},\n",
    " 'M_21': {'step': 1200,\n",
    "  'delivered': {},\n",
    "  'crashed': {'c_3': 220},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': None},\n",
    " 'C_21': {'step': 1200,\n",
    "  'delivered': {'p_1': 47,\n",
    "   'p_6': 76,\n",
    "   'p_3': 135,\n",
    "   'p_9': 215,\n",
    "   'p_7': 350,\n",
    "   'p_8': 539},\n",
    "  'crashed': {},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': 105},\n",
    " 'M_44': {'step': 1200,\n",
    "  'delivered': {},\n",
    "  'crashed': {'d_0': 78, 'c_3': 711},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': None},\n",
    " 'C_44': {'step': 1200,\n",
    "  'delivered': {'p_7': 253, 'p_8': 258},\n",
    "  'crashed': {},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': 103},\n",
    " 'M_66': {'step': 1200,\n",
    "  'delivered': {'p_10': 1116},\n",
    "  'crashed': {'d_1': 186},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': None},\n",
    " 'C_66': {'step': 1200,\n",
    "  'delivered': {'p_1': 16, 'p_3': 18, 'p_5': 70},\n",
    "  'crashed': {},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': 85},\n",
    " 'M_86': {'step': 1200,\n",
    "  'delivered': {},\n",
    "  'crashed': {'c_3': 195},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': None},\n",
    " 'C_86': {'step': 1200,\n",
    "  'delivered': {'p_8': 66,\n",
    "   'p_1': 90,\n",
    "   'p_9': 110,\n",
    "   'p_4': 112,\n",
    "   'p_7': 168,\n",
    "   'p_2': 185,\n",
    "   'p_6': 282},\n",
    "  'crashed': {},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': 90},\n",
    " 'M_14': {'step': 1200,\n",
    "  'delivered': {},\n",
    "  'crashed': {'c_3': 926},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': None},\n",
    " 'C_14': {'step': 1200,\n",
    "  'delivered': {'p_1': 12, 'p_3': 12, 'p_6': 66},\n",
    "  'crashed': {},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': 108}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015fb616",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "   \n",
    "def create_chart_bars(results_dict):\n",
    "    \"\"\" Function that plots a bar graph with the duration of one episode \n",
    "        run with MARL agents/ Centrality Baseline/ Optimality Baseline, recorded in the :param results_dict:.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Design choices    \n",
    "    colors = {\n",
    "        \"marl\": 'blue',\n",
    "        \"central\": 'green',\n",
    "        \"optimal\": 'red'\n",
    "    }\n",
    "    \n",
    "    # Retrieve settings values from results dict\n",
    "    max_steps = results_dict['max_steps']\n",
    "    max_parcels = results_dict['max_parcels'] \n",
    "    max_agents = results_dict['max_agents']\n",
    "\n",
    "    # Deep copy for further computations\n",
    "    scenario_results = {k:v for k,v in results_dict.items() if k[0:3] != 'max'}\n",
    "    \n",
    "    # filter\n",
    "    runs = scenario_results.keys()\n",
    "    values = scenario_results.values()\n",
    "    \n",
    "    # TODO statt all dead => einfach not all delivered marker\n",
    "    merged = {}  # key is seed as str, value is a dict with [marl, central, optimal, all_dead_marl, all_dead_cent]\n",
    "   \n",
    "    # Retrieve the data\n",
    "    for run_id, res in scenario_results.items():\n",
    "        \n",
    "        split_id = run_id.split('_')  # --> type, seed\n",
    "        key_type, key_seed = split_id[0], split_id[1]\n",
    "        # merge data from the runs with same seed (marl + baselines)\n",
    "        \n",
    "        # add new dict if seed not encountered yet\n",
    "        if key_seed not in merged:\n",
    "            merged[key_seed] = {} \n",
    "        \n",
    "        _key_delivered = 'marl'\n",
    "        #_key_crashed = 'all_dead_marl'\n",
    "        if key_type == 'C':\n",
    "            # Baseline run\n",
    "            merged[key_seed]['optimal'] = res['optimal']\n",
    "            _key_delivered = 'central'\n",
    "            #_key_crashed = 'all_dead_central'\n",
    "\n",
    "        # Retrieve number of steps in run\n",
    "        last_step = res['step']\n",
    " \n",
    "        # old code for plotting the number of steps needed in the episode\n",
    "        merged[key_seed][_key_delivered] = last_step \n",
    "        merged[key_seed][_key_delivered + '_all'] = len(res['delivered'])\n",
    "        \n",
    "#        all_parcels_delivered = len(res['delivered']) == max_parcels     # were all parcels delivered\n",
    "#        merged[key_seed][_key_delivered + '_all'] = all_parcels_delivered\n",
    "#        if not all_parcels_delivered:\n",
    "#            merged[key_seed][_key_delivered] = max_steps \n",
    "            \n",
    "    print(\"Merged: \", merged)\n",
    "    \n",
    "    \n",
    "    #  example data = [[30, 25, 50, 20],\n",
    "#    [40, 23, 51, 17],\n",
    "#    [35, 22, 45, 19]]\n",
    "\n",
    "    data = [[],[],[],[], []]\n",
    "    labels = []\n",
    "    \n",
    "    # split data into type of run    \n",
    "    for seed,values in merged.items():\n",
    "        labels.append('S_'+seed)\n",
    "        data[0].append(values['marl'])\n",
    "        data[1].append(values['central'])\n",
    "        data[2].append(values['optimal'])\n",
    "        data[3].append(values['marl_all'])\n",
    "        data[4].append(values['central_all'])\n",
    "    \n",
    "    print(\"Data: \", data)\n",
    "    \n",
    "    X = np.arange(len(labels))\n",
    "    #print(X)\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "    #ax.bar(X + 0.00, data[2], color = colors['optimal'], label=\"Optimality Baseline\", width = 0.25)\n",
    "    ax.bar(X + 0.25, data[3], color = colors['marl'], label=\"MARL System\", width = 0.25, alpha=0.8)\n",
    "    ax.bar(X + 0.50, data[4], color = colors['central'], label=\"Centrality Baseline\", width = 0.25)\n",
    "    \n",
    "\n",
    "    plt.xlabel(\"Experiments\")\n",
    "    plt.ylabel(\"Parcels_delivered\")\n",
    "    plt.ylim(bottom=0, top=max_parcels)\n",
    "    \n",
    "    # y axis as percentage\n",
    "    yticks = mtick.PercentFormatter(max_parcels)\n",
    "    ax.yaxis.set_major_formatter(yticks)\n",
    "    \n",
    "    # Add experiment identifiers x-Axis\n",
    "    plt.xticks(X + 0.37, labels)\n",
    "    # Add legend\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be69355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot duration bar graphs from the results\n",
    "create_chart_bars(experiment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f82cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "\n",
    "def create_chart_episode_events(results_dict, draw_crashes = False):\n",
    "    \"\"\" Function that plots a graph with either the deliveries of parcels or the crashes of agents\n",
    "        over the course of an episode, recorded in the :param results_dict:.\n",
    "        Set the :param draw_crashes: flag for plotting crashes, default are deliveries.\n",
    "    \"\"\"\n",
    "    # TODO Graphen auch abspeichern oder nur hier anzeigen ??\n",
    "    # TODO - Parcel additions ??\n",
    "    # TODO - overthink fill with max_value for mean computation \n",
    "    \n",
    "    # Design choices    \n",
    "    colors = {\n",
    "        \"marl\": 'blue',\n",
    "        \"central\": 'green',\n",
    "        \"optimal\": 'red'\n",
    "    }\n",
    "    alpha = 0.6  # Opacity of individual value lines\n",
    "    alpha_opt = 0.2\n",
    "    opt_marker_size = 15\n",
    "    line_width_mean = 5\n",
    "    line_width_optimal = 2\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # Retrieve settings values from results dict\n",
    "    max_steps = results_dict['max_steps']\n",
    "    max_parcels = results_dict['max_parcels'] \n",
    "    max_agents = results_dict['max_agents']\n",
    "    \n",
    "    _len = max_agents if draw_crashes else max_parcels\n",
    "    _len += 1     # start plot at origin\n",
    "\n",
    "    _key = 'crashed' if draw_crashes else 'delivered'\n",
    "    \n",
    "    # Deep copy for further computations\n",
    "    scenario_results = {k:v for k,v in results_dict.items() if k[0:3] != 'max'}\n",
    "    \n",
    "    # for computation of mean\n",
    "    m_values, c_values, o_values = [], [], []\n",
    "    Y = [str(i) for i in range(0, _len)]\n",
    "\n",
    "    # iterate over configs\n",
    "    for scenario, results in scenario_results.items():\n",
    "\n",
    "        # Default settings -> MARL run\n",
    "        color = colors[\"marl\"]\n",
    "        _type = m_values\n",
    "        label= \"marl_system\"\n",
    "\n",
    "        if scenario[0] == 'C':\n",
    "            # Baseline run\n",
    "            color = colors[\"central\"]\n",
    "            _type = c_values\n",
    "            label = \"centrality_baseline\"\n",
    "            \n",
    "            # Retrieve and plot optimality baseline\n",
    "            optimal_time = results['optimal']\n",
    "            assert optimal_time is not None\n",
    "            if not draw_crashes: ax.plot(optimal_time, 0, \"*\", color = colors[\"optimal\"], label=\"optimality_baseline\", markersize=opt_marker_size, alpha= alpha_opt, clip_on=False)   \n",
    "            o_values.append(optimal_time)\n",
    "            \n",
    "        _num_steps = results['step']\n",
    "        \n",
    "        X = [0] + list(results[_key].values())\n",
    "        \n",
    "                \n",
    "        X = X + [max_steps]*(_len - len(X))           # Fill X up with max_step values for not delivered parcels / not crashed agents\n",
    " \n",
    "        _type.append(X)  # add X to the respective mean list\n",
    "        #Y = [str(i) for i in range(0, len(results[_key].values())+1)]\n",
    "        \n",
    "        #print(\"Data: \", results[_key].values())\n",
    "        #print(\"new X: \", X)\n",
    "        #print(\"new Y: \", Y)\n",
    "    \n",
    "        ax.step(X, Y, label=label, where='post', color=color, alpha=alpha)\n",
    "        \n",
    "        \n",
    "        # Attempt to improve the filling mess in the plot...         \n",
    "        #X = X + [max_steps]*(_len - len(X))           # Fill X up with max_step values for not delivered parcels / not crashed agents\n",
    " \n",
    "        #_type.append(X)  # add X to the respective mean list\n",
    "\n",
    "        \n",
    "    # compute mean values\n",
    "    m_mean = np.mean(np.array(m_values), axis=0)\n",
    "    c_mean = np.mean(np.array(c_values), axis=0)\n",
    "    o_mean = np.mean(np.array(o_values), axis=0)\n",
    "\n",
    "    \n",
    "    ax.step(m_mean, Y, label=\"marl_system\", where='post', color=colors[\"marl\"], linewidth=line_width_mean)\n",
    "    ax.step(c_mean, Y, label=\"centrality_baseline\", where='post', color=colors[\"central\"], linewidth=line_width_mean)\n",
    "\n",
    "    # star for opt:    if not draw_crashes: plt.plot(o_mean, 0, \"*\", label=\"optimality_baseline\", color=\"r\", markersize=opt_marker_size, clip_on=False)\n",
    "    # better?: vertical line for opt\n",
    "    if not draw_crashes: ax.axvline(o_mean, label=\"optimality_baseline\", color=colors[\"optimal\"], linewidth=2, alpha=alpha_opt+0.3)\n",
    "    \n",
    "    # y axis as percentage\n",
    "    max_percent = max_agents if draw_crashes else max_parcels\n",
    "    xticks = mtick.PercentFormatter(max_percent)\n",
    "    ax.yaxis.set_major_formatter(xticks)\n",
    "    \n",
    "    \n",
    "    # Lables and Legend\n",
    "    plt.xlabel(\"steps\")\n",
    "    ylabel = \"% of crashed agents\" if draw_crashes else \"% of delivered parcels\"\n",
    "    plt.ylabel(ylabel)\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys())\n",
    "    \n",
    "    # Margins\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.xlim()\n",
    "    plt.margins(x=0, y=0)\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b300cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot delivery graphs from the results\n",
    "create_chart_episode_events(experiment_results, draw_crashes=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd51171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot crash graphs from the results\n",
    "create_chart_episode_events(experiment_results, draw_crashes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e29c509",
   "metadata": {},
   "source": [
    "### Manual Actions for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efebf2d2",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "##################\n",
    "env_config = {\n",
    "            'DEBUG_LOGS':False,\n",
    "            'TOPOLOGY': topology,\n",
    "            # Simulation config\n",
    "            'NUMBER_STEPS_PER_EPISODE': 1000,\n",
    "            #'NUMBER_OF_TIMESTEPS': NUMBER_OF_TIMESTEPS,\n",
    "            'RANDOM_SEED': None, # 42\n",
    "            # Map\n",
    "            'CHARGING_STATION_NODES': [0,1,2,3,4],\n",
    "            # Entities\n",
    "            'NUMBER_OF_DRONES': 2,\n",
    "            'NUMBER_OF_CARS': 2,\n",
    "            'MAX_BATTERY_POWER': 100,  # TODO split this for drone and car??\n",
    "            'INIT_NUMBER_OF_PARCELS': 3,\n",
    "            'MAX_NUMBER_OF_PARCELS': 3,\n",
    "            'THRESHOLD_ADD_NEW_PARCEL': 0.01,\n",
    "            # Baseline settings\n",
    "            'BASELINE_FLAG': False,  # is set True in the test function when needed\n",
    "            'BASELINE_OPT_CONSTANT': 2.5,\n",
    "            'BASELINE_TIME_CONSTRAINT': 5,\n",
    "            # TODO \n",
    "            #Rewards\n",
    "            'REWARDS': {\n",
    "                'PARCEL_DELIVERED': 200,\n",
    "                'STEP_PENALTY': -0.1,\n",
    "            },  \n",
    "        }\n",
    "        \n",
    "\n",
    "\n",
    "env = Map_Environment(env_config)\n",
    "env.state\n",
    "#env.ACTION_DROPOFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5165d3bd",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO select actions to give agent 0 reward!!\n",
    "#print(env.action_space)\n",
    "\n",
    "\n",
    "actions_1 = {'d_0': 6, 'd_1': 2, 'c_2': 3, 'c_3': 4}\n",
    "#actions_1 = {'d_0': 0, 'd_1': 0, 'c_2': 5, 'c_3': 5}\n",
    "actions_2 = {'d_0': 2, 'd_1': 8, 'c_2': 7, 'c_3':0}\n",
    "actions_3 = {'d_0': 5, 'd_1': 5, 'c_2':2 }\n",
    "\n",
    "new_obs, rewards, dones, infos = env.step(actions_1)\n",
    "print(infos)\n",
    "print(f\"New Obs are: {new_obs}\")\n",
    "print(rewards)\n",
    "print(\"------------------\")\n",
    "new_obs2, rewards2, dones2, infos2 = env.step(actions_2)\n",
    "print(f\"New Obs are: {new_obs2}\")\n",
    "print(rewards2)\n",
    "print(\"------------------\")\n",
    "new_obs3, rewards3, dones3, infos3 = env.step(actions_3)\n",
    "print(f\"New Obs are: {new_obs3}\")\n",
    "print(rewards3)\n",
    "\n",
    "actions_4 = {'d_0': 0, 'd_1': 0, 'c_2': 1, 'c_3': 0}\n",
    "actions_5 = {'d_0': 0, 'd_1': 0, 'c_2': 0, 'c_3': 0}\n",
    "actions_6 = {'d_0': 0, 'd_1': 0, 'c_2': 5, 'c_3': 0}\n",
    "\n",
    "new_obs, rewards, dones, infos = env.step(actions_4)\n",
    "print(f\"New Obs are: {new_obs}\")\n",
    "print(rewards)\n",
    "print(\"------------------\")\n",
    "new_obs2, rewards2, dones2, infos2 = env.step(actions_5)\n",
    "print(f\"New Obs are: {new_obs2}\")\n",
    "print(rewards2)\n",
    "print(\"------------------\")\n",
    "new_obs3, rewards3, dones3, infos3 = env.step(actions_6)\n",
    "print(f\"New Obs are: {new_obs3}\")\n",
    "print(rewards3)\n",
    "\n",
    "print(\"------------------\")\n",
    "print(dones3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a143594",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TENSORBOARD\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "#Start tensorboard below the notebook\n",
    "#%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
