{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f838d2e",
   "metadata": {},
   "source": [
    "# MultiAgentEnvironment: simple Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a170d6",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful\n"
     ]
    }
   ],
   "source": [
    "#%autosave 30\n",
    "import glob\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms.shortest_paths.generic import shortest_path_length\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "#from ray.tune.logger import pretty_print\n",
    "from ray.rllib.policy.policy import Policy, PolicySpec\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.fcnet import FullyConnectedNetwork\n",
    "\n",
    "#from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.env import MultiAgentEnv\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "\n",
    "from gym.spaces import Discrete, Box, Tuple, MultiDiscrete, Dict, MultiBinary\n",
    "from ray.rllib.utils.spaces.repeated import Repeated\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf1, tf, tfv = try_import_tf()  # prefered TF import for Ray.io\n",
    "\n",
    "from threading import Thread, Event\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10fdf817",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "######## Utility Classes ########\n",
    "class BoolTimer(Thread):\n",
    "    \"\"\"A boolean value that toggles after a specified number of seconds:\n",
    "    \n",
    "    Example:\n",
    "        bt = BoolTimer(30.0, False)\n",
    "        bt.start()\n",
    "\n",
    "    Used in the Centrality Baseline to limit the computation time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, interval, initial_state=True):\n",
    "        Thread.__init__(self)\n",
    "        self.interval = interval\n",
    "        self.state = initial_state\n",
    "        self.finished = Event()\n",
    "\n",
    "    def __bool__(self):\n",
    "        return bool(self.state)\n",
    "\n",
    "    def run(self):\n",
    "        self.finished.wait(self.interval)\n",
    "        if not self.finished.is_set():\n",
    "            self.state = not self.state\n",
    "        self.finished.set()\n",
    "\n",
    "\n",
    "######## Static helper functions ########\n",
    "def shuffle_actions(action_dict, check_space = False):\n",
    "    \"\"\"\n",
    "        Used to shuffle the action dict to ensure that agents with a lower id are not always preferred\n",
    "        when picking up parcels over other agents that chose the same action.\n",
    "        For debugging: Can also be used to check if all actions are in the action_space.\n",
    "    \"\"\"\n",
    "    keys = list(action_dict)\n",
    "    random.shuffle(keys)\n",
    "    shuffled = {}\n",
    "\n",
    "    for agent in keys:\n",
    "        if check_space:  #assert actions are in action space -> disable for later trainig, extra check while development\n",
    "            assert self.action_space.contains(action_dict[agent]),f\"Action {action_dict[agent]} taken by agent {agent} not in action space\"\n",
    "        shuffled[agent] = action_dict[agent]\n",
    "\n",
    "    return shuffled\n",
    "\n",
    "\n",
    "\n",
    "def load_graph(data):\n",
    "    \"\"\"Loads topology (map) from json file into a networkX Graph and returns the graph\"\"\"\n",
    "\n",
    "    nodes = data[\"nodes\"]\n",
    "    edges = data[\"edges\"]\n",
    "\n",
    "    g = nx.DiGraph()  # directed graph\n",
    "    g.add_nodes_from(nodes)\n",
    "    g.edges(data=True)\n",
    "\n",
    "   \n",
    "    for node in nodes:  # add attribute values\n",
    "        g.nodes[node][\"id\"] = nodes[node][\"id\"]\n",
    "        g.nodes[node][\"type\"] = nodes[node][\"type\"]\n",
    "\n",
    "    for edge in edges:  # add edges with attributes\n",
    "        f = edges[edge][\"from\"]\n",
    "        t = edges[edge][\"to\"]\n",
    "        \n",
    "        weight_road, weight_air, _type = sys.maxsize, sys.maxsize, None\n",
    "        \n",
    "        if edges[edge][\"road\"] >= 0:\n",
    "            weight_road = edges[edge][\"road\"]\n",
    "            _type = 'road'\n",
    "        if edges[edge][\"air\"] >= 0:\n",
    "            weight_air = edges[edge][\"air\"]\n",
    "            _type = 'both' if _type == 'road' else 'air'\n",
    "\n",
    "        weight = min(weight_road, weight_air)  # needed for optimality baseline\n",
    "        \n",
    "        g.add_edge(f, t, type=_type, road= weight_road, air=weight_air, weight=weight)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663b334",
   "metadata": {},
   "source": [
    "## Environment Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f835bd0",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Map definition\n",
    "topology = {\n",
    "    'nodes': {\n",
    "            0: {'id': 1, 'type': 'parking'},\n",
    "            1: {'id': 1, 'type': 'parking'},\n",
    "            2: {'id': 2, 'type': 'parking'},\n",
    "            3: {'id': 3, 'type': 'parking'},\n",
    "            4: {'id': 4, 'type': 'parking'},\n",
    "            5: {'id': 5, 'type': 'air'},\n",
    "            6: {'id': 6, 'type': 'parking'},\n",
    "            7: {'id': 7, 'type': 'air'},\n",
    "            8: {'id': 8, 'type': 'parking'},\n",
    "            9: {'id': 9, 'type': 'parking'},\n",
    "            10: {'id': 10, 'type': 'parking'},\n",
    "            11: {'id': 11, 'type': 'air'}\n",
    "    },\n",
    "    'edges': {\n",
    "    ## Outer Ring    --> Road much (!) faster than drones\n",
    "    \"e01\":{\"from\": 0,\"to\": 1,\"road\": 10, \"air\": 40},\n",
    "    \"e02\":{\"from\": 1,\"to\": 0,\"road\": 10, \"air\": 40},\n",
    "    \"e03\":{\"from\": 1,\"to\": 4,\"road\": 8, \"air\": 6},        \n",
    "    \"e04\":{\"from\": 4,\"to\": 1,\"road\": 8, \"air\": 6},\n",
    "    \"e03\":{\"from\": 4,\"to\": 2,\"road\": 8, \"air\": 6},        \n",
    "    \"e04\":{\"from\": 2,\"to\": 4,\"road\": 8, \"air\": 6},\n",
    "    \"e05\":{\"from\": 2,\"to\": 3,\"road\": 10, \"air\": 30},\n",
    "    \"e06\":{\"from\": 3,\"to\": 2,\"road\": 10, \"air\": 30},\n",
    "    \"e07\":{\"from\": 3,\"to\": 0,\"road\": 10, \"air\": 40},\n",
    "    \"e08\":{\"from\": 0,\"to\": 3,\"road\": 8, \"air\": 35}, \n",
    "    ## Inner Nodes  --> Reinfahren langsamer als rausfahren\n",
    "    ##              --> \n",
    "    \"e11\":{\"from\": 4,\"to\": 6,\"road\": 2, \"air\": 5},        \n",
    "    \"e12\":{\"from\": 6,\"to\": 4,\"road\": 2, \"air\": 5},\n",
    "    \"e13\":{\"from\": 0,\"to\": 6,\"road\": 6, \"air\": -1},\n",
    "    \"e14\":{\"from\": 6,\"to\": 0,\"road\": 6, \"air\": -1},\n",
    "    \"e15\":{\"from\": 3,\"to\": 6,\"road\": 6, \"air\": -1},\n",
    "    \"e16\":{\"from\": 6,\"to\": 3,\"road\": 6, \"air\": -1},   \n",
    "    ## Outliers   --> Distance about equal if both exist!\n",
    "    \"e17\":{\"from\": 4,\"to\": 5,\"road\": 2, \"air\": 2},        \n",
    "    \"e18\":{\"from\": 5,\"to\": 4,\"road\": 2, \"air\": 2},\n",
    "    \"e19\":{\"from\": 6,\"to\": 7,\"road\": -1, \"air\": 2},\n",
    "    \"e20\":{\"from\": 7,\"to\": 6,\"road\": -1, \"air\": 2},\n",
    "    \"e21\":{\"from\": 3,\"to\": 11,\"road\": -1, \"air\": 3},\n",
    "    \"e22\":{\"from\": 11,\"to\": 3,\"road\": -1, \"air\": 3},     \n",
    "    ## Upper left square  --> Hier besser mit den Drohnen agieren, Falls Sie da sind!!\n",
    "    \"e23\":{\"from\": 0,\"to\": 8,\"road\": -1, \"air\": 4},        \n",
    "    \"e24\":{\"from\": 8,\"to\": 0,\"road\": -1, \"air\": 4},\n",
    "    \"e25\":{\"from\": 8,\"to\": 9,\"road\": 3, \"air\": 3},\n",
    "    \"e26\":{\"from\": 9,\"to\": 8,\"road\": 3, \"air\": 3},\n",
    "    \"e27\":{\"from\": 9,\"to\": 10,\"road\": 3, \"air\": 3},\n",
    "    \"e28\":{\"from\": 10,\"to\": 9,\"road\": 3, \"air\": 3},\n",
    "    \"e29\":{\"from\": 0,\"to\": 10,\"road\": 3, \"air\": 3},\n",
    "    \"e30\":{\"from\": 10,\"to\": 0,\"road\": 3, \"air\": 3}  \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6ccbe98",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class Map_Environment(MultiAgentEnv):\n",
    "    \n",
    "    def __init__(self, env_config: dict = {}):\n",
    "        # ensure config file includes all necessary settings\n",
    "        assert 'NUMBER_STEPS_PER_EPISODE' in env_config\n",
    "        assert 'NUMBER_OF_DRONES' in env_config\n",
    "        assert 'NUMBER_OF_CARS' in env_config\n",
    "        assert 'INIT_NUMBER_OF_PARCELS' in env_config\n",
    "        assert 'TOPOLOGY' in env_config\n",
    "        assert 'MAX_NUMBER_OF_PARCELS' in env_config\n",
    "        assert 'THRESHOLD_ADD_NEW_PARCEL' in env_config\n",
    "        assert 'BASELINE_FLAG' in env_config\n",
    "        assert 'BASELINE_TIME_CONSTRAINT' in env_config\n",
    "        assert 'BASELINE_OPT_CONSTANT' in env_config\n",
    "        assert 'CHARGING_STATION_NODES' in env_config\n",
    "        assert 'REWARDS' in env_config\n",
    "        \n",
    "        self.graph = load_graph(topology)\n",
    "        \n",
    "\n",
    "        # Map config\n",
    "        self.NUMBER_OF_DRONES = env_config['NUMBER_OF_DRONES']\n",
    "        self.NUMBER_OF_CARS = env_config['NUMBER_OF_CARS']\n",
    "        self.NUMBER_OF_EDGES = self.graph.number_of_edges()\n",
    "        self.NUMBER_OF_NODES = self.graph.number_of_nodes()\n",
    "        self.CHARGING_STATION_NODES = env_config['CHARGING_STATION_NODES']           \n",
    "        self.MAX_BATTERY_POWER = env_config['MAX_BATTERY_POWER']\n",
    "       \n",
    "\n",
    "        # Simulation config\n",
    "        self.NUMBER_STEPS_PER_EPISODE = env_config['NUMBER_STEPS_PER_EPISODE']\n",
    "        self.INIT_NUMBER_OF_PARCELS = env_config['INIT_NUMBER_OF_PARCELS']\n",
    "        self.RANDOM_SEED = env_config.get('RANDOM_SEED', None)\n",
    "        self.MAX_NUMBER_OF_PARCELS = env_config['MAX_NUMBER_OF_PARCELS']\n",
    "        self.THRESHOLD_ADD_NEW_PARCEL = env_config['THRESHOLD_ADD_NEW_PARCEL']\n",
    "        self.BASELINE_FLAG = env_config['BASELINE_FLAG']\n",
    "        self.BASELINE_TIME_CONSTRAINT = env_config['BASELINE_TIME_CONSTRAINT']\n",
    "        self.BASELINE_OPT_CONSTANT = env_config['BASELINE_OPT_CONSTANT']\n",
    " \n",
    "        self.DEBUG_LOG = env_config.get('DEBUG_LOGS', False)\n",
    "        # Some Sanity Checks on the settings\n",
    "        if self.DEBUG_LOG: assert self.MAX_NUMBER_OF_PARCELS >= self.INIT_NUMBER_OF_PARCELS, \"Number of initial parcels exceeds max parcel limit\"\n",
    "\n",
    "        #Reward constants\n",
    "        self.STEP_PENALTY = env_config['REWARDS']['STEP_PENALTY']\n",
    "        self.PARCEL_DELIVERED = env_config['REWARDS']['PARCEL_DELIVERED']\n",
    "        # compute other rewards\n",
    "        self.BATTERY_DIED = self.STEP_PENALTY * self.NUMBER_STEPS_PER_EPISODE\n",
    "        self.BATTERY_DIED_WITH_PARCEL = self.BATTERY_DIED * 2\n",
    "        #self.DELIVERY_CONTRIBUTION: depends on active agents --> computed when given in prepare_global_reward()\n",
    "        #self.ALL_DELIVERED_CONTRIB: depends on active agents --> computed when given in prepare_global_reward(episode_success=True)\n",
    "\n",
    "        # Computed constants\n",
    "        self.NUMBER_OF_AGENTS = self.NUMBER_OF_DRONES + self.NUMBER_OF_CARS\n",
    "        self.PARCEL_STATE_DELIVERED = self.NUMBER_OF_AGENTS + self.NUMBER_OF_NODES\n",
    "        self.NUMBER_OF_ACTIONS = 1 + self.NUMBER_OF_NODES + 1 + self.MAX_NUMBER_OF_PARCELS\n",
    "        self.ACTION_DROPOFF = 1 + self.NUMBER_OF_NODES     # First Action NOOP is 0\n",
    "    \n",
    "        # seed RNGs\n",
    "        self.seed(self.RANDOM_SEED) \n",
    "        \n",
    "        self.state = None  \n",
    "        self.current_step = None\n",
    "        self.blocked_agents = None\n",
    "        self.parcels_delivered = None\n",
    "        self.done_agents = None\n",
    "        self.all_done = None  \n",
    "        self.allowed_actions = None\n",
    "        \n",
    "        # baseline related \n",
    "        self.baseline_missions = None\n",
    "        self.agents_base = None\n",
    "        self.o_employed = None\n",
    "        \n",
    "        # metrics for the evaluation\n",
    "        self.parcel_delivered_steps = None # --> {p1: 20, p2: 240, p:140}\n",
    "        self.parcel_added_steps = None     # --> {p1: 0, p2: 0, p: 50}\n",
    "        self.agents_crashed = None         # --> {c_2: 120, d_0: 242} \n",
    "        self.metrics = None\n",
    "        \n",
    "        self.agents = [*[\"d_\" + str(i) for i in range(self.NUMBER_OF_DRONES)],*[\"c_\" + str(i) for i in range(self.NUMBER_OF_DRONES, self.NUMBER_OF_DRONES + self.NUMBER_OF_CARS)]]\n",
    "        \n",
    "        # Define observation and action spaces for individual agents\n",
    "        self.action_space = Discrete(self.NUMBER_OF_ACTIONS)\n",
    "\n",
    "        #---- Repeated Obs Space: Represents a parcel with (id, location, destination)  --> # parcel_id starts at 1 \n",
    "        parcel_space = Dict({'id': Discrete(self.MAX_NUMBER_OF_PARCELS+1),\n",
    "                             'location': Discrete(self.NUMBER_OF_NODES + self.NUMBER_OF_AGENTS + 1),\n",
    "                             'destination': Discrete(self.NUMBER_OF_NODES)\n",
    "                            })\n",
    "        self.observation_space = Dict({'obs': Dict({'state': \n",
    "                                                 Dict({ 'position': Discrete(self.NUMBER_OF_NODES),\n",
    "                                                        'battery': Discrete(self.MAX_BATTERY_POWER + 1), #[0-100]\n",
    "                                                        'has_parcel': Discrete(self.MAX_NUMBER_OF_PARCELS + 1),\n",
    "                                                        'current_step': Discrete(self.NUMBER_STEPS_PER_EPISODE + 1)}), \n",
    "                                                 'parcels': Repeated(parcel_space, max_len=self.MAX_NUMBER_OF_PARCELS)\n",
    "                                                }),\n",
    "                                        'allowed_actions': MultiBinary(self.NUMBER_OF_ACTIONS)\n",
    "                                      }) \n",
    "        #TODO: why is reset() not called by env? \n",
    "        self.reset() \n",
    "        \n",
    "    def step(self, action_dict):\n",
    "        \"\"\"conduct the state transitions caused by actions in action_dict\n",
    "        :returns:\n",
    "            - observation_dict: observations for agents that need to act in the next round\n",
    "            - rewards_dict: rewards for agents following their chosen actions\n",
    "            - done_dict: indicates end of episode if max_steps reached or all parcels delivered\n",
    "            - info_dict: pass data to custom logger\n",
    "        \"\"\"\n",
    "\n",
    "        if self.DEBUG_LOG: print(f\"Debug log flag set to {self.DEBUG_LOG}\")\n",
    "        \n",
    "        # ensure no disadvantage for agents with higher IDs if action conflicts with that taken by other agent\n",
    "        action_dict = shuffle_actions(action_dict)\n",
    "    \n",
    "        self.current_step += 1\n",
    "    \n",
    "        # grant step penalty reward    \n",
    "        agent_rewards = {agent: self.STEP_PENALTY for agent in self.agents}  \n",
    "        \n",
    "        # setting an agent done twice might cause crash when used with tune... -> https://github.com/ray-project/ray/issues/10761\n",
    "        dones = {}\n",
    "        \n",
    "        self.metrics['step'] = self.current_step\n",
    "\n",
    "        # dynamically add parcel\n",
    "        if random.random() <= self.THRESHOLD_ADD_NEW_PARCEL and len(self.state['parcels']) < self.MAX_NUMBER_OF_PARCELS:\n",
    " \n",
    "            p_id, parcel = self.generate_parcel()\n",
    "            assert p_id not in self.state['parcels'], \"Duplicate parcel ID generated\"\n",
    "            self.state['parcels'][p_id] = parcel\n",
    "\n",
    "            if self.BASELINE_FLAG:\n",
    "                self.metrics[\"optimal\"] = self.compute_optimality_baseline(p_id, extra_charge=self.BASELINE_OPT_CONSTANT)\n",
    "                \n",
    "            for agent in self.agents:\n",
    "                self.allowed_actions[agent][self.ACTION_DROPOFF + p_id] = np.array([1]).astype(bool)\n",
    "            \n",
    "        if self.BASELINE_FLAG:\n",
    "            old_actions = action_dict\n",
    "            action_dict = {}\n",
    "            # Replace actions with actions recommended by the central baseline\n",
    "            for agent, action in old_actions.items():\n",
    "                if len(self.baseline_missions[agent]) > 0:\n",
    "                    new_action = self.baseline_missions[agent][0]\n",
    "\n",
    "                    if type(new_action) is tuple: # dropoff action with minimal time\n",
    "\n",
    "                        if self.current_step >= new_action[1]:\n",
    "                            new_action = new_action[0]\n",
    "                            self.baseline_missions[agent].pop(0)\n",
    "                        else: #agent has to wait for previous subroute agent\n",
    "                            new_action = 0\n",
    "                    else: # move or pickup or charge\n",
    "                        self.baseline_missions[agent].pop(0)\n",
    "                            \n",
    "                    action_dict[agent] = new_action\n",
    "\n",
    "                else: # agent has no baseline mission -> Noop\n",
    "                    action_dict[agent] = 0\n",
    "                \n",
    "        # carry out State Transition\n",
    "        \n",
    "        # handel NOP actions: -> action == 0 \n",
    "        noop_agents =  {agent: action for agent, action in action_dict.items() if action == 0}\n",
    "        effectual_agents_items = {agent: action for agent, action in action_dict.items() if action > 0}.items()\n",
    "        \n",
    "        # now: transaction between agents modelled as pickup of just offloaded (=dropped) parcel --> handle dropoff first\n",
    "        moving_agents = {agent: action for agent, action in effectual_agents_items if 0 < action and action <= self.NUMBER_OF_NODES}\n",
    "        dropoff_agents = {agent: action for agent, action in effectual_agents_items if action == self.ACTION_DROPOFF}\n",
    "        pickup_agents = {agent: action for agent, action in effectual_agents_items if action > self.ACTION_DROPOFF}\n",
    "\n",
    "        # handle noop / charge decisions:\n",
    "        for agent, action in noop_agents.items():\n",
    "            # check if recharge is possible\n",
    "            current_pos = self.state['agents'][agent]['position']\n",
    "            if current_pos in self.CHARGING_STATION_NODES:\n",
    "                self.state['agents'][agent]['battery'] = self.MAX_BATTERY_POWER\n",
    "\n",
    "        # handle Movement actions:\n",
    "        for agent, action in moving_agents.items():\n",
    "            # get Current agent position from state\n",
    "            self.state['agents'][agent]['battery'] += -1\n",
    "            current_pos = self.state['agents'][agent]['position']\n",
    "            \n",
    "            # networkX: use node instead of edge:\n",
    "            destination = action - 1\n",
    "            \n",
    "            if self.graph.has_edge(current_pos, destination):\n",
    "                # Agent chose existing edge! -> check if type is suitable\n",
    "              \n",
    "                agent_type = 'road' if agent[:1] == 'c' else 'air'\n",
    "                if self.graph[current_pos][destination][\"type\"] in [agent_type, 'both']:\n",
    "                    # Edge has correct type\n",
    "                    self.state['agents'][agent]['position'] = destination\n",
    "                    self.state['agents'][agent]['battery'] += -(self.graph[current_pos][destination][agent_type] +1)\n",
    "                    if self.state['agents'][agent]['battery'] < 0:        # ensure negative battery value does not break obs_space\n",
    "                        #Battery below 0 --> reset to 0 (stay in obs space)\n",
    "                        self.state['agents'][agent]['battery'] = 0\n",
    "                    \n",
    "                    self.blocked_agents[agent] = self.graph[current_pos][destination][agent_type]\n",
    "                    self.update_allowed_actions_nodes(agent)\n",
    "\n",
    "                    \n",
    "        # handle Dropoff Decision: -> action == self.NUMBER_OF_NODES + 2\n",
    "        for agent, action in dropoff_agents.items():\n",
    "            self.state['agents'][agent]['battery'] += -1\n",
    "\n",
    "            if self.state['agents'][agent]['has_parcel'] > 0:   # agent has parcel\n",
    "                parcel_id = self.state['agents'][agent]['has_parcel']\n",
    "                self.state['agents'][agent]['has_parcel'] = 0\n",
    "                self.state['parcels'][parcel_id][0] = self.state['agents'][agent]['position']\n",
    "                if self.state['parcels'][parcel_id][0] ==  self.state['parcels'][parcel_id][1]:\n",
    "                    # Delivery successful\n",
    "                    \n",
    "                    agent_rewards[agent] += self.PARCEL_DELIVERED  # local reward\n",
    "                    # global contribution rewards\n",
    "                    active_agents, reward = self.prepare_global_reward()\n",
    "                    for a_id in active_agents: agent_rewards[a_id] += reward\n",
    "                    \n",
    "                    self.state['parcels'][parcel_id][0] = self.PARCEL_STATE_DELIVERED\n",
    "                    self.parcels_delivered[int(parcel_id) -1] = True                    # Parcel_ids start at 1\n",
    "\n",
    "                    self.metrics['delivered'].update({\"p_\" + str(parcel_id): self.current_step})\n",
    "\n",
    "                self.update_allowed_actions_parcels(agent)\n",
    "          \n",
    "        \n",
    "        # handle Pickup Decision:\n",
    "        for agent, action in pickup_agents.items():\n",
    "            self.state['agents'][agent]['battery'] += -1 \n",
    "    \n",
    "            # agent has free capacity\n",
    "            if self.state['agents'][agent]['has_parcel'] == 0:  #free parcel capacity\n",
    "                # convert action_id to parcel_id\n",
    "                parcel_id = action - self.ACTION_DROPOFF\n",
    "\n",
    "                if self.DEBUG_LOG: assert parcel_id in self.state['parcels'] # parcel {parcel_id} already in ENV ??\n",
    "                    \n",
    "                elif self.state['parcels'][parcel_id][0] == self.state['agents'][agent]['position']:\n",
    "                    # Successful pickup operation\n",
    "                    self.state['parcels'][parcel_id][0] = self.NUMBER_OF_NODES + int(agent[2:])\n",
    "                    self.state['agents'][agent]['has_parcel'] = int(parcel_id)\n",
    "                    \n",
    "                    self.update_allowed_actions_parcels(agent)\n",
    "                        \n",
    "        # unblock agents for next round \n",
    "        self.blocked_agents = {agent: remaining_steps -1 for agent, remaining_steps in self.blocked_agents.items() if remaining_steps > 1}\n",
    "\n",
    "        # handle dones - out of battery or max_steps or goal reached\n",
    "        for agent in action_dict.keys():\n",
    "            if agent not in self.done_agents and self.state['agents'][agent]['battery'] <= 0:\n",
    "                agent_rewards[agent] = self.BATTERY_DIED_WITH_PARCEL if self.state['agents'][agent]['has_parcel'] != 0 else self.BATTERY_DIED\n",
    "                dones[agent] = True\n",
    "                self.done_agents.append(agent)\n",
    "                \n",
    "                self.metrics['crashed'].update({agent: self.current_step})\n",
    "                \n",
    "                if len(self.done_agents) == self.NUMBER_OF_AGENTS:\n",
    "                    # all agents dead\n",
    "                    self.all_done = True\n",
    "        \n",
    "        # check if episode terminated because of goal reached or all agents crashed -> avoid setting done twice\n",
    "        if self.current_step >= self.NUMBER_STEPS_PER_EPISODE or (all(self.parcels_delivered) and len(self.parcels_delivered) == self.MAX_NUMBER_OF_PARCELS):\n",
    "            # check if episode success:\n",
    "            if self.current_step < self.NUMBER_STEPS_PER_EPISODE:\n",
    "                    # grant global reward for all parcels delivered \n",
    "                    active_agents, reward = self.prepare_global_reward(_episode_success=True)\n",
    "                    for a_id in active_agents: agent_rewards[a_id] += reward\n",
    "            \n",
    "            self.all_done = True\n",
    "\n",
    "        dones['__all__'] = self.all_done\n",
    "\n",
    "        \n",
    "        parcel_obs = self.get_parcel_obs()\n",
    "    \n",
    "        # obs \\ rewards \\ dones\\ info\n",
    "        return  {agent: { 'obs': {'state': {'position': self.state['agents'][agent]['position'], 'battery': self.state['agents'][agent]['battery'],\n",
    "                                    'has_parcel': self.state['agents'][agent]['has_parcel'],'current_step': self.current_step},\n",
    "                                    'parcels': parcel_obs},\n",
    "                         'allowed_actions': self.allowed_actions[agent]} for agent in self.agents if agent not in self.blocked_agents and agent not in self.done_agents}, \\\n",
    "                { agent: agent_rewards[agent] for agent in self.agents}, \\\n",
    "                dones, \\\n",
    "                {}\n",
    "\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)  \n",
    "\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"resets variables; returns dict with observations, keys are agent_ids\"\"\"\n",
    "        self.current_step = 0\n",
    "        self.blocked_agents = {}\n",
    "        self.parcels_delivered = [False for _ in range(self.MAX_NUMBER_OF_PARCELS)]\n",
    "        self.done_agents = []\n",
    "        self.all_done = False\n",
    "        \n",
    "        self.metrics = { \"step\": self.current_step,\n",
    "            \"delivered\": {},\n",
    "            \"crashed\": {},\n",
    "            \"added\": {},\n",
    "            \"optimal\": None\n",
    "        }\n",
    "\n",
    "        #Baseline\n",
    "        self.baseline_missions = {agent: [] for agent in self.agents}\n",
    "        self.o_employed = [0 for _ in range(self.NUMBER_OF_AGENTS)]\n",
    "\n",
    "        self.agents_base = None  # env.agents in the pseudocode\n",
    "        self.allowed_actions = { agent: np.array([1 for act in range(self.NUMBER_OF_ACTIONS)]) for agent in self.agents}\n",
    " \n",
    "        #Reset State\n",
    "        self.state = {'agents': {},\n",
    "                      'parcels': {}}\n",
    "        #generate initial parcels\n",
    "        for _ in range(self.INIT_NUMBER_OF_PARCELS):\n",
    "            p_id, parcel = self.generate_parcel()\n",
    "            self.state['parcels'][p_id] = parcel            \n",
    "\n",
    "        parcel_obs = self.get_parcel_obs()\n",
    "        \n",
    "        # init agents\n",
    "        self.state['agents'] = {agent: {'position': self._random_feasible_agent_position(agent),\n",
    "                                        'battery': self.MAX_BATTERY_POWER, 'has_parcel': 0} for agent in self.agents}\n",
    "        \n",
    "        if self.BASELINE_FLAG:\n",
    "            for parcel in self.state['parcels']:\n",
    "                self.compute_central_delivery(parcel, debug_log=False)\n",
    "                \n",
    "                # TODO really return something here??\n",
    "                self.metrics['optimal'] = self.compute_optimality_baseline(parcel, extra_charge=self.BASELINE_OPT_CONSTANT, debug_log=False)\n",
    "        \n",
    "        # compute allowed actions per agent --> move to function\n",
    "        for agent in self.agents:\n",
    "            self.update_allowed_actions_nodes(agent)\n",
    "            self.update_allowed_actions_parcels(agent)    \n",
    "            \n",
    "        agent_obs = {agent: {'obs': {'state':  {'position': state['position'], 'battery': state['battery'],\n",
    "                                        'has_parcel': state['has_parcel'],'current_step': self.current_step},\n",
    "                            'parcels': parcel_obs\n",
    "                            },\n",
    "                            'allowed_actions': self.allowed_actions[agent]\n",
    "                            } for agent, state in self.state['agents'].items()\n",
    "                    } \n",
    "        \n",
    "        return {**agent_obs}\n",
    "    \n",
    "    \n",
    "    def _random_feasible_agent_position(self, agent_id):\n",
    "        \"\"\"Needed to avoid car agents being initialized at nodes only reachable by drones\n",
    "        and thus trapped from the beginning. Ensures that car agents start at a node of type 'road' or 'parking'.\n",
    "        \"\"\"\n",
    "        position = random.randrange(self.NUMBER_OF_NODES)\n",
    "        if agent_id[0] == 'c':    # agent is car \n",
    "            while self.graph.nodes[position]['type'] == 'air':      # position not reachable by car\n",
    "                position = random.randrange(self.NUMBER_OF_NODES)\n",
    "        return position\n",
    "    \n",
    "    \n",
    "    def update_allowed_actions_nodes(self, agent):\n",
    "        new_pos = self.state['agents'][agent]['position']\n",
    "        next_steps = list(self.graph.neighbors(new_pos))\n",
    "        agent_type = 'air' if agent[0]=='d' else 'road'\n",
    "        \n",
    "        allowed_nodes = np.zeros(self.NUMBER_OF_NODES)\n",
    "        for neighbor in next_steps:\n",
    "            if self.graph[new_pos][neighbor]['type'] in [agent_type, 'both']:\n",
    "                allowed_nodes[neighbor] = 1\n",
    "        self.allowed_actions[agent][1:self.NUMBER_OF_NODES+1] = np.array(allowed_nodes).astype(bool)\n",
    "                 \n",
    "            \n",
    "    def update_allowed_actions_parcels(self, agent):\n",
    "        \"\"\" Allow only the Dropoff or Pickup actions, depending on the has_parcel value of the agent.\n",
    "        Pickup is not concerned with the parcel actually being at the agents current location, only with free capacity\n",
    "        and the parcel already being added to the ENV\"\"\"\n",
    "        num_parcels = len(self.state['parcels'])\n",
    "                          \n",
    "        allowed_parcels = np.zeros(self.MAX_NUMBER_OF_PARCELS)\n",
    "        dropoff = 1\n",
    "        if self.state['agents'][agent]['has_parcel'] == 0:\n",
    "            dropoff = 0\n",
    "            allowed_parcels = np.concatenate([np.ones(num_parcels), np.zeros(self.MAX_NUMBER_OF_PARCELS - num_parcels)])\n",
    "                          \n",
    "        self.allowed_actions[agent][self.NUMBER_OF_NODES+1:] = np.array([dropoff, *allowed_parcels]).astype(bool)\n",
    "    \n",
    "    \n",
    "    def get_parcel_obs(self):\n",
    "        parcel_obs = [{'id':pid, 'location': parcel[0], 'destination':parcel[1]} for (pid, parcel) in self.state['parcels'].items()]\n",
    "        return parcel_obs\n",
    "    \n",
    "    def generate_parcel(self):\n",
    "        \"\"\"generate new parcel id and new parcel with random nodes for location and destination.\n",
    "            p_ids (int) start at 1, later nodes for parcel will be sampled to avoid parcels that already spawn at their destination\"\"\"\n",
    "        p_id = len(self.state['parcels']) + 1\n",
    "\n",
    "        parcel = random.sample(range(self.NUMBER_OF_NODES), 2)  # => initial location != destination\n",
    "        self.metrics['added'].update({p_id: self.current_step})\n",
    "        \n",
    "        return p_id, parcel\n",
    "    \n",
    "    def prepare_global_reward(self, _episode_success=False):\n",
    "        \"\"\" computes a global reward for all active agents still in the environment.\n",
    "            If _episode_success is set to True, all parcels have been delivered an the ALL_DELIVERED reward is granted.\n",
    "            :Returns: list of active agents and the reward value\n",
    "        \"\"\"\n",
    "        agents_alive = list(set(self.agents).difference(set(self.done_agents)))\n",
    "        if self.DEBUG_LOG: assert len(agents_alive) > 0\n",
    "        reward = self.PARCEL_DELIVERED * (self.NUMBER_STEPS_PER_EPISODE - self.current_step) / self.NUMBER_STEPS_PER_EPISODE if _episode_success else self.PARCEL_DELIVERED / len(agents_alive)\n",
    "\n",
    "        return agents_alive, reward\n",
    "    \n",
    "    \n",
    "    #------ BASELINE related methods ------###         \n",
    "    def compute_optimality_baseline(self, parcel_id, extra_charge=2.5, debug_log=False):\n",
    "        \"\"\"Used in the optimality baseline\n",
    "            Input: parcel_id, (extra_charge)\n",
    "            Output: new total delivery rounds needed for all parcels\n",
    "        \"\"\"\n",
    "        parcel = self.state['parcels'][parcel_id]\n",
    "        path_time = 2 + shortest_path_length(self.graph, parcel[0], parcel[1], 'weight')\n",
    "        \n",
    "        _time = math.ceil(path_time * extra_charge) # round to next higher integer\n",
    "        \n",
    "        min_index = self.o_employed.index(min(self.o_employed))\n",
    "        self.o_employed[min_index] += _time\n",
    "                                    \n",
    "        return max(self.o_employed)\n",
    "        \n",
    "    \n",
    "    def compute_central_delivery(self, p_id, debug_log = False):\n",
    "        \"\"\"Used in the central baseline, iteratively tries to find a good delivery route \n",
    "        with the available agents in the time specified in BASELINE_TIME_CONSTRAINT\n",
    "            Input: parcel_id\n",
    "            Output: Dict: {agent_id: [actions], ...}  --> update that dict! (merge in this function with prev actions!) \n",
    "        \"\"\"\n",
    "\n",
    "        if self.agents_base is None:\n",
    "            self.agents_base = {a_id: (a['position'], 0) for (a_id, a) in self.state[\"agents\"].items()}  # last instructed pos + its step count\n",
    "\n",
    "        min_time = None\n",
    "        new_missions = {}   # key: agent, value: [actions]\n",
    "\n",
    "        source = self.state[\"parcels\"][p_id][0]\n",
    "        target = self.state[\"parcels\"][p_id][1]\n",
    "        shortest_paths_generator = nx.shortest_simple_paths(self.graph, source, target, weight='weight')\n",
    "\n",
    "        running = BoolTimer(self.BASELINE_TIME_CONSTRAINT)\n",
    "        running.start()             \n",
    "        \n",
    "        while running:\n",
    "            # Default --> assign full route to nearest drone\n",
    "            if min_time is None:   \n",
    "\n",
    "                air_route = nx.shortest_path(self.graph, source= source, target= target, weight=\"air\")\n",
    "                air_route_time = nx.shortest_path_length(self.graph, source= source, target= target, weight=\"air\")\n",
    "                air_route.pop(0)  # remove source node from path\n",
    "               \n",
    "                # Assign most suitable Drone\n",
    "                best_drone = None\n",
    "                for (a_id, a_tp) in self.agents_base.items():\n",
    "                    if a_id[0] != 'd':                      # filter for correct agent type\n",
    "                        continue\n",
    "                        \n",
    "                    journey_time = nx.shortest_path_length(self.graph, source=a_tp[0], target=source, weight=\"air\") + a_tp[1]\n",
    "                    if min_time is None or journey_time < min_time:\n",
    "                        min_time = journey_time\n",
    "                        best_drone = (a_id, a_tp)\n",
    "  \n",
    "                # construct path for agent\n",
    "                drone_route = nx.shortest_path(self.graph, source= best_drone[1][0], target= source, weight=\"air\")\n",
    "\n",
    "                drone_route_actions = [x+1 for x in drone_route[1:]] + [(self.ACTION_DROPOFF + p_id, 0)] + [x+1 for x in air_route[1:]] + [self.ACTION_DROPOFF]   # increment node_ids by one to get corresponding action\n",
    "                min_time += air_route_time + 2 # add 2 steps for pick & drop\n",
    "                \n",
    "                self._add_charging_stops_to_route(drone_route_actions, debug_log=debug_log)              \n",
    "                \n",
    "                new_missions[best_drone[0]] = (drone_route_actions, min_time) \n",
    "                \n",
    "            else:   # try to improve the existing base mission  \n",
    "                try:\n",
    "                    shortest_route = next(shortest_paths_generator)\n",
    "                except StopIteration:\n",
    "                    break               # all existing shortest paths already tried\n",
    "                    \n",
    "                subroutes = self._path_to_subroutes(shortest_route, debug_log=debug_log)\n",
    "                duration, min_agents = self._find_best_agents(subroutes, min_time, debug_log=debug_log)\n",
    "\n",
    "                if duration < min_time:\n",
    "                    # faster delivery route found!          \n",
    "                    assert duration < min_time, \"Central Baseline prefered a longer route...\"\n",
    "                    \n",
    "                    # update min_time\n",
    "                    min_time = duration                    \n",
    "                    new_missions = self._build_missions(min_agents, subroutes, p_id, debug_log=debug_log) \n",
    "\n",
    "        #---- end while = Timer\n",
    "        # now save the best mission found in the ENV\n",
    "        for agent in new_missions.keys():\n",
    "            # retrieve target node from mission, depends on charging stops and case no move necessary    \n",
    "            target = None     \n",
    "            if isinstance(new_missions[agent][0][-2], int):    \n",
    "                target = new_missions[agent][0][-2] \n",
    "                if target == 0: target = new_missions[agent][0][-3]    # charging action was added before dropoff\n",
    "                target -= 1                                            # action-1 = node_id\n",
    "            else:  # handle case no move necessary -> pick action before dropoff\n",
    "                target = self.agents_base[agent][0]\n",
    "\n",
    "            self.agents_base[agent] = (target, self.agents_base[agent][1] + new_missions[agent][1])  \n",
    "            self.baseline_missions[agent].extend(new_missions[agent][0])\n",
    "            \n",
    "        return new_missions\n",
    "        \n",
    "        \n",
    "    def _find_best_agents(self, subroutes, min_time, debug_log=False):\n",
    "        \"\"\" For use in centrality_baseline. Finds best available agents for traversing a set of subroutes \n",
    "            and returns these with the total duration for doing so.\n",
    "            Input: subroutes = [ edge_type, [actions]]\n",
    "            \"\"\"\n",
    "        \n",
    "        min_agents = {}\n",
    "        temp_agents_base = {k:v for k,v in self.agents_base.items()}  # deep copy for temporary planning transfer or stick with agent??\n",
    "\n",
    "        for i,r in enumerate(subroutes):\n",
    "\n",
    "            # init some helper vars\n",
    "            a_type = \"d\" if r[0] == 'air' else 'c' \n",
    "            min_time_sub = None    # best time (min) for this subroute (closest agent)\n",
    "            best_agent_sub = None\n",
    "\n",
    "            # iterate over agents of correct type!  --> later: Busy / unbusy\n",
    "            for (a_id, a_tp) in temp_agents_base.items():\n",
    "                #reminder: a_tp is tuple of latest future position (node, timestep)\n",
    "\n",
    "                # filter for correct agent type - even if type is 'both' one agent can still take only its edge type\n",
    "                weight_agent = r[0]\n",
    "                if r[0] == 'both':\n",
    "                    weight_agent = 'road' if a_id[0] == 'c' else 'air'\n",
    "                else:                     # wrong agent type\n",
    "                    if a_id[0] != a_type:  # todo replace with parameter variable in function!\n",
    "                        continue \n",
    "                        \n",
    "                journey_time = nx.shortest_path_length(self.graph, source=a_tp[0], target=r[1][0], weight=weight_agent) + a_tp[1] # earliest time agent can be there\n",
    "                if min_time_sub is None or journey_time < min_time_sub:\n",
    "                    min_time_sub = journey_time\n",
    "                    best_agent_sub = (a_id, a_tp)    # a_id, a_tp = (latest location, timestep)\n",
    "\n",
    "            # closest available agent found        \n",
    "            best_agent_weight = weight_agent = 'road' if best_agent_sub[0] == 'c' else 'air'\n",
    "            duration_sub = min_time_sub + nx.shortest_path_length(self.graph, source=r[1][0], target=r[1][-1], weight=best_agent_weight) + 1\n",
    "\n",
    "            # update agent state in temporary planning\n",
    "            temp_agents_base[best_agent_sub[0]] = (r[1][-1], duration_sub)\n",
    "\n",
    "            # agent_tuple, duration_subroutes_until_then\n",
    "            min_agents[i] = (best_agent_sub, duration_sub)\n",
    "            \n",
    "            if debug_log: assert duration_sub < sys.maxsize, \"Non existent edge taken somewhere...\"\n",
    "\n",
    "            # check if current subroute already longer than the min one\n",
    "            if duration_sub > min_time:\n",
    "                break # already worse, check next simple path\n",
    "                \n",
    "        return duration_sub, min_agents\n",
    "    \n",
    "    \n",
    "    def _build_missions(self, min_agents, subroutes, parcel_id, debug_log = False):\n",
    "        \"\"\"For use in centrality_baseline. Computes list of actions for delivery of parcel parcel_id \n",
    "        and necessary duration for execution from list of agents, subroutes\"\"\"\n",
    "        new_mission = {}\n",
    "\n",
    "        for i, s in enumerate(subroutes):\n",
    "            best_agent_pos = min_agents[i][0][1][0]\n",
    "            #earliest time to start that action\n",
    "            time_pickup = min_agents[i-1][1] if i > 0 else 0 # First subroute pickup as soon as possible\n",
    "            \n",
    "            # construct the actual delivery path to pickup\n",
    "            delivery_route = nx.shortest_path(self.graph, source=best_agent_pos, target=s[1][0], weight=s[0])\n",
    "            delivery_route_actions = [x+1 for x in delivery_route[1:]] + [(self.ACTION_DROPOFF + parcel_id, time_pickup)] + [x+1 for x in s[1][1:]] + [self.ACTION_DROPOFF]\n",
    "\n",
    "            if debug_log: assert min_agents[i][0] not in new_mission  #I see no preferable case where agent picks-up 1 parcel 2 times --> holding always better than following\n",
    "\n",
    "            self._add_charging_stops_to_route(delivery_route_actions, debug_log=debug_log)\n",
    "            new_mission[min_agents[i][0][0]] = (delivery_route_actions, min_agents[i][1])\n",
    "            \n",
    "        return new_mission\n",
    "    \n",
    "    \n",
    "    def _add_charging_stops_to_route(self, route_actions, debug_log=False):\n",
    "        \"\"\"For use in centrality_baseline. Iterates through a list of actions and inserts a charging action\n",
    "        after every move action to a node with a charging station. \n",
    "        Tuples representing Dropoff actions with minimal executions time are updated to account for eventual delays. \"\"\"\n",
    "        \n",
    "        delay = 0\n",
    "        for i, n in enumerate(route_actions):\n",
    "            if type(n) is tuple:\n",
    "                if delay > 0: route_actions[i] = (n[0], n[1] + delay)\n",
    "            else: \n",
    "                if n-1 in self.CHARGING_STATION_NODES:\n",
    "                    delay += 1\n",
    "                    route_actions.insert(i+1, 0)\n",
    "\n",
    "\n",
    "    def _path_to_subroutes(self, path, debug_log= False):\n",
    "        \"\"\"For use in centrality_baseline. Takes path in the graph as input and returns list of subroutes \n",
    "        split at changes of the edge types with the type\"\"\"\n",
    "\n",
    "        # get subroutes by their edge_types\n",
    "        e_type_prev = None\n",
    "        e_type_changes = [] # save indices of source nodes before new edge type\n",
    "        subroutes = []\n",
    "        _subroute = [path[0]] \n",
    "\n",
    "        if len(path) > 1:\n",
    "            e_type_prev = self.graph.edges[path[0], path[1]]['type']\n",
    "    \n",
    "            for i, node in enumerate(path[1:-1], start=1):\n",
    "    \n",
    "                e_type_next = self.graph.edges[node, path[i+1]]['type']\n",
    "    \n",
    "                _subroute.append(node)\n",
    "                if e_type_next != e_type_prev:   \n",
    "                    subroutes.append((e_type_prev, _subroute))\n",
    "                    _subroute = [node]\n",
    "                    e_type_prev = e_type_next\n",
    "    \n",
    "            _subroute.append(path[-1])\n",
    "        subroutes.append((e_type_prev, _subroute))   # don't forget last subroute           \n",
    "    \n",
    "        return subroutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6698c6",
   "metadata": {},
   "source": [
    "## Agent Model and Experiment Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14b96755",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from gym.spaces import Discrete, Box, Tuple, MultiDiscrete, Dict, MultiBinary\n",
    "#from ray.rllib.utils.spaces.space_utils import flatten_space\n",
    "#from ray.rllib.models.preprocessors import DictFlatteningPreprocessor\n",
    "\n",
    "# Parametric-action agent model  --> apply Action Masking!\n",
    "class ParametricAgentModel(TFModelV2):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name, *args, **kwargs):\n",
    "        super(ParametricAgentModel, self).__init__(obs_space, action_space, num_outputs, model_config, name, *args, **kwargs)\n",
    "\n",
    "        assert isinstance(action_space, Discrete), f'action_space is a {type(action_space)}, but should be Discrete!'\n",
    "        \n",
    "        # Adjust for number of agents/parcels/Nodes!! -> Simply copy found shape from the thrown exception\n",
    "        true_obs_shape = (1750, )\n",
    "        action_embed_size = action_space.n\n",
    "        \n",
    "        self.action_embed_model = FullyConnectedNetwork(\n",
    "            Box(0, 1, shape=true_obs_shape),  # TODO hier nochmal die 1 anpassen?? --> muss das hier der obs entsprechen??\n",
    "            action_space,\n",
    "            action_embed_size,\n",
    "            model_config,\n",
    "            name + '_action_embedding')\n",
    "        \n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        \n",
    "        action_mask = input_dict['obs']['allowed_actions']\n",
    "        action_embedding, _ = self.action_embed_model.forward({'obs_flat': input_dict[\"obs_flat\"]}, state, seq_lens)\n",
    "        intent_vector = tf.expand_dims(action_embedding, 1)\n",
    "        action_logits = tf.reduce_sum(intent_vector, axis=1)\n",
    "        inf_mask = tf.maximum(tf.math.log(action_mask), tf.float32.min)\n",
    "\n",
    "        return action_logits + inf_mask, state\n",
    "    \n",
    "    def value_function(self):\n",
    "        return self.action_embed_model.value_function()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2017a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Proposed way to train / evaluate MARL policy from github Issues: --> https://github.com/ray-project/ray/issues/9123 and  https://github.com/ray-project/ray/issues/9208\n",
    "\n",
    "def train(config, name, save_dir, stop_criteria, num_samples, verbosity=1):\n",
    "    \"\"\"\n",
    "    Train an RLlib PPO agent using tune until any of the configured stopping criteria is met.\n",
    "    :param stop_criteria: Dict with stopping criteria.\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run\n",
    "    :return: Return the path to the saved agent (checkpoint) and tune's ExperimentAnalysis object\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/analysis.html#experimentanalysis-tune-experimentanalysis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Start training\")\n",
    "    analysis = ray.tune.run(PPOTrainer, verbose=verbosity, config=config, local_dir=save_dir, \n",
    "                            stop=stop_criteria, name=name, num_samples=num_samples,\n",
    "                            checkpoint_at_end=True, resume=True)\n",
    "    \n",
    "    # list of lists: one list per checkpoint; each checkpoint list contains 1st the path, 2nd the metric value\n",
    "    checkpoints = analysis.get_trial_checkpoints_paths(trial=analysis.get_best_trial('episode_reward_mean', mode='max'),\n",
    "                                                       metric='episode_reward_mean')\n",
    "    # retriev the checkpoint path; we only have a single checkpoint, so take the first one\n",
    "    checkpoint_path = checkpoints[0][0]\n",
    "    print(f\"Saved trained model in checkpoint {checkpoint_path} - achieved episode_reward_mean: {checkpoints[0][1]}\")\n",
    "    return checkpoint_path, analysis\n",
    "\n",
    "\n",
    "def load(config, path):\n",
    "    \"\"\"\n",
    "    Load a trained RLlib agent from the specified path. Call this before testing the trained agent.\n",
    "    \"\"\"\n",
    "    agent = PPOTrainer(config=config) #, env=env_class)\n",
    "    agent.restore(path)\n",
    "    return agent\n",
    "    \n",
    "    \n",
    "def test(env_class, env_config, policy_mapping_fcn, agent):\n",
    "    \"\"\"Test trained agent for a single episode. Return the retrieved env metrics for this episode and the episode reward\"\"\"\n",
    "    # instantiate env class\n",
    "    env = env_class(env_config)\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    \n",
    "    while not done: # run until episode ends\n",
    "        actions = {}\n",
    "        for agent_id, agent_obs in obs.items():\n",
    "            # Here: policy_id == agent_id - added this to avoid confusion for other policy mappings \n",
    "            policy_id = policy_mapping_fcn(agent_id, episode=None, worker=None) \n",
    "            actions[agent_id] = agent.compute_action(agent_obs, policy_id=policy_id)\n",
    "        obs, reward, done, info = env.step(actions)\n",
    "        done = done['__all__']\n",
    "        \n",
    "        # sum up reward for all agents\n",
    "        episode_reward += sum(reward.values())\n",
    "        \n",
    "    # Retrieve custom metrics from ENV\n",
    "    return env.metrics, episode_reward \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14c5ec6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_and_test_scenarios(config, seeds=None):\n",
    "    \"\"\" Trains for a single scenario indicated by a seed \"\"\"\n",
    "    # TODO how to distinguish between the different algos ? \n",
    "    print(\"Starte: run_function_trainer!\")\n",
    "    \n",
    "    # prepare the config dicts\n",
    "    NAME = config['NAME']\n",
    "    SAVE_DIR = config['SAVE_DIR']\n",
    "    ENVIRONMENT = config['ENV']\n",
    "    \n",
    "    # Simulations\n",
    "    NUMBER_STEPS_PER_EPISODE = config['NUMBER_STEPS_PER_EPISODE']\n",
    "    STOP_CRITERIA = config['STOP_CRITERIA']\n",
    "    NUMBER_OF_SAMPLES = config['NUMBER_OF_SAMPLES']\n",
    "    \n",
    "    #MAP / ENV\n",
    "    NUMBER_OF_DRONES = config['NUMBER_OF_DRONES']\n",
    "    NUMBER_OF_CARS = config['NUMBER_OF_CARS']\n",
    "    NUMBER_OF_AGENTS = NUMBER_OF_DRONES + NUMBER_OF_CARS\n",
    "    MAX_NUMBER_OF_PARCELS = config['MAX_NUMBER_OF_PARCELS']\n",
    "    \n",
    "    # TESTING\n",
    "    SEEDS = config['SEEDS']\n",
    "    \n",
    "    env_config = {\n",
    "            'DEBUG_LOGS':False,\n",
    "            'TOPOLOGY': config['TOPOLOGY'],\n",
    "            # Simulation config\n",
    "            'NUMBER_STEPS_PER_EPISODE': NUMBER_STEPS_PER_EPISODE,\n",
    "            #'NUMBER_OF_TIMESTEPS': NUMBER_OF_TIMESTEPS,\n",
    "            'RANDOM_SEED': None, # 42\n",
    "            # Map\n",
    "            'CHARGING_STATION_NODES': config['CHARGING_STATION_NODES'],\n",
    "            # Entities\n",
    "            'NUMBER_OF_DRONES': NUMBER_OF_DRONES,\n",
    "            'NUMBER_OF_CARS': NUMBER_OF_CARS,\n",
    "            'MAX_BATTERY_POWER': config['MAX_BATTERY_POWER'],  # TODO split this for drone and car??\n",
    "            'INIT_NUMBER_OF_PARCELS': config['INIT_NUMBER_OF_PARCELS'],\n",
    "            'MAX_NUMBER_OF_PARCELS': config['MAX_NUMBER_OF_PARCELS'],\n",
    "            'THRESHOLD_ADD_NEW_PARCEL': config['THRESHOLD_ADD_NEW_PARCEL'],\n",
    "            # Baseline settings\n",
    "            'BASELINE_FLAG': False,  # is set True in the test function when needed\n",
    "            'BASELINE_OPT_CONSTANT': config['BASELINE_OPT_CONSTANT'],\n",
    "            'BASELINE_TIME_CONSTRAINT': config['BASELINE_TIME_CONSTRAINT'],\n",
    "            # TODO \n",
    "            #Rewards\n",
    "            'REWARDS': config['REWARDS'] \n",
    "        }\n",
    "        \n",
    "    run_config = {\n",
    "        'num_gpus': config['NUM_GPUS'],\n",
    "        'num_workers': config['NUM_WORKERS'],\n",
    "        'env': ENVIRONMENT,\n",
    "        'env_config': env_config,\n",
    "        'multiagent': {\n",
    "            'policies': {\n",
    "                # tuple values: policy, obs_space, action_space, config\n",
    "                **{a: (None, None, None, { 'model': {'custom_model': ParametricAgentModel }, 'framework': 'tf'}) for a in ['d_'+ str(j) for j in range(NUMBER_OF_DRONES)] + ['c_'+ str(i) for i in range(NUMBER_OF_DRONES, NUMBER_OF_CARS + NUMBER_OF_DRONES)]}\n",
    "            },\n",
    "            'policy_mapping_fn': policy_mapping_fn,\n",
    "            'policies_to_train': ['d_'+ str(i) for i in range(NUMBER_OF_DRONES)] + ['c_'+ str(i) for i in range(NUMBER_OF_DRONES, NUMBER_OF_CARS + NUMBER_OF_DRONES)]\n",
    "        },\n",
    "        #'log_level': \"INFO\",\n",
    "        #\"hiddens\": [],     # For DQN\n",
    "        #\"dueling\": False,  # For DQN\n",
    "    }\n",
    "    \n",
    "    # Train and Evaluate the agents !\n",
    "    checkpoint, analysis = train(run_config, NAME, SAVE_DIR, STOP_CRITERIA, NUMBER_OF_SAMPLES)\n",
    "    print(\"Training finished - Checkpoint: \", checkpoint)\n",
    "    \n",
    "    env_class = ENVIRONMENT\n",
    "    # Restore trained policies for evaluation\n",
    "    agent = load(run_config, checkpoint)\n",
    "    print(\"Agent loaded - Agent: \", agent)\n",
    "    \n",
    "    # Run the test cases for the specified seeds\n",
    "    runs = {'max_steps': NUMBER_STEPS_PER_EPISODE, 'max_parcels': MAX_NUMBER_OF_PARCELS, 'max_agents': NUMBER_OF_AGENTS}\n",
    "    \n",
    "    for seed in SEEDS:\n",
    "        env_config['RANDOM_SEED'] = seed\n",
    "        # TODO check if \n",
    "        print(seed)\n",
    "        assert run_config['env_config']['RANDOM_SEED'] == seed\n",
    "        result = test_scenario(run_config, agent)\n",
    "        runs.update(result)\n",
    "        \n",
    "    return runs\n",
    "\n",
    "\n",
    "def test_scenario(config, agent):\n",
    "    \"\"\"\n",
    "    Loads a pretrained agent, initializes an environment from the seed \n",
    "    and then evaluates it over one episode with the Marl agents and the central baseline.\n",
    "\n",
    "    Returns: dict with results for graph creation for both evaluation / inference runs\n",
    "    \"\"\"\n",
    "    #TODO unterscheidung run_config > env_config\n",
    "    # Wie die beiden Runs abspeichern --> {Seed + [marl / base] : result }\n",
    "\n",
    "    env_class = config['env']  \n",
    "    env_config = config['env_config']\n",
    "    seed = env_config['RANDOM_SEED']\n",
    "    policy_mapping_fn = config['multiagent']['policy_mapping_fn']\n",
    "    \n",
    "    # Test with MARL\n",
    "\n",
    "    metrics_marl, reward_marl = test(env_class, env_config, policy_mapping_fn, agent)\n",
    "    \n",
    "    # Test with CentralBase\n",
    "    env_config['BASELINE_FLAG'] = True\n",
    "    metrics_central, reward_central = test(env_class, env_config, policy_mapping_fn, agent)\n",
    "    env_config['BASELINE_FLAG'] = False\n",
    "    \n",
    "    # ASSERT that both optimal values are equal\n",
    "    #assert metrics_marl['optimal'] == metrics_central['optimal']\n",
    "    \n",
    "    return {\"M_\" + str(seed): metrics_marl, \"C_\" + str(seed): metrics_central}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d86f07bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-02-28 21:34:30 (running for 00:00:00.47)<br>Memory usage on this node: 678.1/1007.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/96 CPUs, 0/8 GPUs, 0.0/163.36 GiB heap, 0.0/74.0 GiB objects (0.0/1.0 accelerator_type:RTX)<br>Result logdir: /work-ceph/albecker/Exp_thesis/Exp_environment/small_no<br>Number of trials: 2/2 (2 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-28 21:34:30,659\tINFO tune.py:636 -- Total run time: 0.69 seconds (0.00 seconds for the tuning loop).\n",
      "2022-02-28 21:34:31,723\tWARNING ppo.py:223 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=32 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model in checkpoint /work-ceph/albecker/Exp_thesis/Exp_environment/small_no/PPOTrainer_Map_Environment_46063_00001_1_2022-02-22_13-02-38/checkpoint_006000/checkpoint-6000 - achieved episode_reward_mean: -134.70800000003226\n",
      "Training finished - Checkpoint:  /work-ceph/albecker/Exp_thesis/Exp_environment/small_no/PPOTrainer_Map_Environment_46063_00001_1_2022-02-22_13-02-38/checkpoint_006000/checkpoint-6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=544766)\u001b[0m 2022-02-28 21:35:03,231\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544746)\u001b[0m 2022-02-28 21:35:03,616\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544773)\u001b[0m 2022-02-28 21:35:03,630\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544757)\u001b[0m 2022-02-28 21:35:03,663\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544711)\u001b[0m 2022-02-28 21:35:03,772\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544774)\u001b[0m 2022-02-28 21:35:04,026\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544782)\u001b[0m 2022-02-28 21:35:04,169\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544627)\u001b[0m 2022-02-28 21:35:04,177\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544777)\u001b[0m 2022-02-28 21:35:04,259\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544672)\u001b[0m 2022-02-28 21:35:04,316\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544768)\u001b[0m 2022-02-28 21:35:04,314\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544786)\u001b[0m 2022-02-28 21:35:04,361\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544793)\u001b[0m 2022-02-28 21:35:04,368\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544765)\u001b[0m 2022-02-28 21:35:04,479\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544792)\u001b[0m 2022-02-28 21:35:04,439\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544775)\u001b[0m 2022-02-28 21:35:04,497\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544776)\u001b[0m 2022-02-28 21:35:04,601\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544798)\u001b[0m 2022-02-28 21:35:04,677\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544596)\u001b[0m 2022-02-28 21:35:04,682\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544772)\u001b[0m 2022-02-28 21:35:04,749\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544796)\u001b[0m 2022-02-28 21:35:04,801\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544767)\u001b[0m 2022-02-28 21:35:04,938\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544692)\u001b[0m 2022-02-28 21:35:04,966\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544753)\u001b[0m 2022-02-28 21:35:04,982\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544778)\u001b[0m 2022-02-28 21:35:05,062\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544799)\u001b[0m 2022-02-28 21:35:05,075\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544783)\u001b[0m 2022-02-28 21:35:05,238\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544784)\u001b[0m 2022-02-28 21:35:05,385\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544742)\u001b[0m 2022-02-28 21:35:05,594\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544785)\u001b[0m 2022-02-28 21:35:05,569\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=544800)\u001b[0m 2022-02-28 21:35:05,516\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=544762)\u001b[0m 2022-02-28 21:35:05,797\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-02-28 21:35:17,064\tINFO trainable.py:125 -- Trainable.setup took 45.346 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-02-28 21:35:17,075\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "2022-02-28 21:35:17,400\tINFO trainable.py:472 -- Restored on 134.155.89.136 from checkpoint: /work-ceph/albecker/Exp_thesis/Exp_environment/small_no/PPOTrainer_Map_Environment_46063_00001_1_2022-02-22_13-02-38/checkpoint_006000/checkpoint-6000\n",
      "2022-02-28 21:35:17,407\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 6000, '_timesteps_total': 24000000, '_time_total': 122822.35261631012, '_episodes_total': 20028}\n",
      "2022-02-28 21:35:17,411\tWARNING deprecation.py:45 -- DeprecationWarning: `compute_action` has been deprecated. Use `Trainer.compute_single_action()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent loaded - Agent:  PPOTrainer\n",
      "72\n",
      "21\n",
      "44\n",
      "66\n",
      "86\n",
      "14\n",
      "Test evaluation finished ;)\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    return agent_id\n",
    "\n",
    "basic_config = {\n",
    "        # experiment\n",
    "        'NAME': 'small_no',\n",
    "        'SAVE_DIR': 'Exp_environment',\n",
    "        'ALGO': PPOTrainer,\n",
    "        'ENV': Map_Environment,\n",
    "        'DEBUG_LOGS':False,\n",
    "        'NUM_GPUS': 2,\n",
    "        'NUM_WORKERS': 32,\n",
    "        'NUMBER_OF_SAMPLES': 2,\n",
    "        # Simulation config\n",
    "        'NUMBER_STEPS_PER_EPISODE': 1200,\n",
    "        'RANDOM_SEED': None, # 42\n",
    "        # Map\n",
    "        'TOPOLOGY': topology,\n",
    "        'CHARGING_STATION_NODES': [0,1,2,3,4,6,9],\n",
    "        # Entities\n",
    "        'NUMBER_OF_DRONES': 2,\n",
    "        'NUMBER_OF_CARS': 2,\n",
    "        'INIT_NUMBER_OF_PARCELS': 10,\n",
    "        'MAX_NUMBER_OF_PARCELS': 10,\n",
    "        'THRESHOLD_ADD_NEW_PARCEL': 0.1, # 10% chance\n",
    "        'MAX_BATTERY_POWER': 100, \n",
    "        #Baseline\n",
    "        'BASELINE_TIME_CONSTRAINT': 10,\n",
    "        'BASELINE_OPT_CONSTANT': 2.5,\n",
    "        #TESTING\n",
    "        'SEEDS': [72, 21, 44, 66, 86, 14],\n",
    "        #Rewards\n",
    "        'REWARDS': {\n",
    "            'PARCEL_DELIVERED': 200,\n",
    "            'STEP_PENALTY': -0.1,\n",
    "        }, \n",
    "        'STOP_CRITERIA': {\n",
    "            'timesteps_total': 24_000_000,\n",
    "        }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "print(\"Test evaluation functions\")\n",
    "experiment_results = train_and_test_scenarios(basic_config)\n",
    "print(\"Test evaluation finished ;)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c123aef",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_steps': 1200,\n",
       " 'max_parcels': 10,\n",
       " 'max_agents': 4,\n",
       " 'M_72': {'step': 1200,\n",
       "  'delivered': {'p_3': 885},\n",
       "  'crashed': {'c_3': 1152},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': None},\n",
       " 'C_72': {'step': 1200,\n",
       "  'delivered': {'p_4': 49,\n",
       "   'p_1': 67,\n",
       "   'p_3': 67,\n",
       "   'p_7': 95,\n",
       "   'p_5': 126,\n",
       "   'p_10': 224,\n",
       "   'p_9': 403},\n",
       "  'crashed': {},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': 75},\n",
       " 'M_21': {'step': 1200,\n",
       "  'delivered': {},\n",
       "  'crashed': {},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': None},\n",
       " 'C_21': {'step': 1200,\n",
       "  'delivered': {'p_1': 47,\n",
       "   'p_6': 76,\n",
       "   'p_3': 135,\n",
       "   'p_9': 215,\n",
       "   'p_7': 350,\n",
       "   'p_8': 539},\n",
       "  'crashed': {},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': 105},\n",
       " 'M_44': {'step': 1200,\n",
       "  'delivered': {'p_9': 735, 'p_7': 857},\n",
       "  'crashed': {'d_0': 1081},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': None},\n",
       " 'C_44': {'step': 1200,\n",
       "  'delivered': {'p_7': 253, 'p_8': 258},\n",
       "  'crashed': {},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': 103},\n",
       " 'M_66': {'step': 1200,\n",
       "  'delivered': {},\n",
       "  'crashed': {'d_1': 103, 'c_3': 226},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': None},\n",
       " 'C_66': {'step': 1200,\n",
       "  'delivered': {'p_1': 16, 'p_3': 18, 'p_5': 70},\n",
       "  'crashed': {},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': 85},\n",
       " 'M_86': {'step': 1200,\n",
       "  'delivered': {'p_9': 25, 'p_8': 27},\n",
       "  'crashed': {'d_1': 90, 'd_0': 1079},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': None},\n",
       " 'C_86': {'step': 1200,\n",
       "  'delivered': {'p_8': 66,\n",
       "   'p_1': 90,\n",
       "   'p_9': 110,\n",
       "   'p_4': 112,\n",
       "   'p_7': 168,\n",
       "   'p_2': 185,\n",
       "   'p_6': 282},\n",
       "  'crashed': {},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': 90},\n",
       " 'M_14': {'step': 1200,\n",
       "  'delivered': {'p_3': 477, 'p_8': 1025},\n",
       "  'crashed': {'d_1': 445, 'd_0': 1084},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': None},\n",
       " 'C_14': {'step': 1200,\n",
       "  'delivered': {'p_1': 12, 'p_3': 12, 'p_6': 66},\n",
       "  'crashed': {},\n",
       "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
       "  'optimal': 108}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_results = {'max_steps': 1200,\n",
    " 'max_parcels': 10,\n",
    " 'max_agents': 4,\n",
    " 'M_72': {'step': 1200,\n",
    "  'delivered': {'p_3': 885},\n",
    "  'crashed': {'c_3': 1152},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': None},\n",
    " 'C_72': {'step': 1200,\n",
    "  'delivered': {'p_4': 49,\n",
    "   'p_1': 67,\n",
    "   'p_3': 67,\n",
    "   'p_7': 95,\n",
    "   'p_5': 126,\n",
    "   'p_10': 224,\n",
    "   'p_9': 403},\n",
    "  'crashed': {},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': 75},\n",
    " 'M_21': {'step': 1200,\n",
    "  'delivered': {},\n",
    "  'crashed': {},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': None},\n",
    " 'C_21': {'step': 1200,\n",
    "  'delivered': {'p_1': 47,\n",
    "   'p_6': 76,\n",
    "   'p_3': 135,\n",
    "   'p_9': 215,\n",
    "   'p_7': 350,\n",
    "   'p_8': 539},\n",
    "  'crashed': {},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': 105},\n",
    " 'M_44': {'step': 1200,\n",
    "  'delivered': {'p_9': 735, 'p_7': 857},\n",
    "  'crashed': {'d_0': 1081},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': None},\n",
    " 'C_44': {'step': 1200,\n",
    "  'delivered': {'p_7': 253, 'p_8': 258},\n",
    "  'crashed': {},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': 103},\n",
    " 'M_66': {'step': 1200,\n",
    "  'delivered': {},\n",
    "  'crashed': {'d_1': 103, 'c_3': 226},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': None},\n",
    " 'C_66': {'step': 1200,\n",
    "  'delivered': {'p_1': 16, 'p_3': 18, 'p_5': 70},\n",
    "  'crashed': {},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': 85},\n",
    " 'M_86': {'step': 1200,\n",
    "  'delivered': {'p_9': 25, 'p_8': 27},\n",
    "  'crashed': {'d_1': 90, 'd_0': 1079},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': None},\n",
    " 'C_86': {'step': 1200,\n",
    "  'delivered': {'p_8': 66,\n",
    "   'p_1': 90,\n",
    "   'p_9': 110,\n",
    "   'p_4': 112,\n",
    "   'p_7': 168,\n",
    "   'p_2': 185,\n",
    "   'p_6': 282},\n",
    "  'crashed': {},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': 90},\n",
    " 'M_14': {'step': 1200,\n",
    "  'delivered': {'p_3': 477, 'p_8': 1025},\n",
    "  'crashed': {'d_1': 445, 'd_0': 1084},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': None},\n",
    " 'C_14': {'step': 1200,\n",
    "  'delivered': {'p_1': 12, 'p_3': 12, 'p_6': 66},\n",
    "  'crashed': {},\n",
    "  'added': {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0},\n",
    "  'optimal': 108}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "015fb616",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick \n",
    "    \n",
    "def create_chart_bars(results_dict):\n",
    "    \"\"\" Function that plots a bar graph with the duration of one episode \n",
    "        run with MARL agents/ Centrality Baseline/ Optimality Baseline, recorded in the :param results_dict:.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Design choices    \n",
    "    colors = {\n",
    "        \"marl\": 'blue',\n",
    "        \"central\": 'green',\n",
    "        \"optimal\": 'red'\n",
    "    }\n",
    "    \n",
    "    # Retrieve settings values from results dict\n",
    "    max_steps = results_dict['max_steps']\n",
    "    max_parcels = results_dict['max_parcels'] \n",
    "    max_agents = results_dict['max_agents']\n",
    "\n",
    "    # Deep copy for further computations\n",
    "    scenario_results = {k:v for k,v in results_dict.items() if k[0:3] != 'max'}\n",
    "    \n",
    "    # filter\n",
    "    runs = scenario_results.keys()\n",
    "    values = scenario_results.values()\n",
    "    \n",
    "    # TODO statt all dead => einfach not all delivered marker\n",
    "    merged = {}  # key is seed as str, value is a dict with [marl, central, optimal, all_dead_marl, all_dead_cent]\n",
    "  \n",
    "    # Retrieve the data\n",
    "    for run_id, res in scenario_results.items():\n",
    "        \n",
    "        split_id = run_id.split('_')  # --> type, seed\n",
    "        key_type, key_seed = split_id[0], split_id[1]\n",
    "        # merge data from the runs with same seed (marl + baselines)\n",
    "        \n",
    "        # add new dict if seed not encountered yet\n",
    "        if key_seed not in merged:\n",
    "            merged[key_seed] = {} \n",
    "        \n",
    "        _key_delivered = 'marl'\n",
    "        #_key_crashed = 'all_dead_marl'\n",
    "        if key_type == 'C':\n",
    "            # Baseline run\n",
    "            merged[key_seed]['optimal'] = res['optimal']\n",
    "            _key_delivered = 'central'\n",
    "            #_key_crashed = 'all_dead_central'\n",
    "\n",
    "        # Retrieve number of steps in run\n",
    "        last_step = res['step']\n",
    " \n",
    "        # old code for plotting the number of steps needed in the episode\n",
    "        merged[key_seed][_key_delivered] = last_step \n",
    "        merged[key_seed][_key_delivered + '_all'] = len(res['delivered'])\n",
    "        \n",
    "#        all_parcels_delivered = len(res['delivered']) == max_parcels     # were all parcels delivered\n",
    "#        merged[key_seed][_key_delivered + '_all'] = all_parcels_delivered\n",
    "#        if not all_parcels_delivered:\n",
    "#            merged[key_seed][_key_delivered] = max_steps \n",
    "            \n",
    "    print(\"Merged: \", merged)\n",
    "    \n",
    "    \n",
    "    #  example data = [[30, 25, 50, 20],\n",
    "#    [40, 23, 51, 17],\n",
    "#    [35, 22, 45, 19]]\n",
    "\n",
    "    data = [[],[],[],[], []]\n",
    "    labels = []\n",
    "    \n",
    "    # split data into type of run    \n",
    "    for seed,values in merged.items():\n",
    "        labels.append('S_'+seed)\n",
    "        data[0].append(values['marl'])\n",
    "        data[1].append(values['central'])\n",
    "        data[2].append(values['optimal'])\n",
    "        data[3].append(values['marl_all'])\n",
    "        data[4].append(values['central_all'])\n",
    "    \n",
    "    print(\"Data: \", data)\n",
    "    \n",
    "    X = np.arange(len(labels))\n",
    "    #print(X)\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "    #ax.bar(X + 0.00, data[2], color = colors['optimal'], label=\"Optimality Baseline\", width = 0.25)\n",
    "    ax.bar(X + 0.25, data[3], color = colors['marl'], label=\"MARL System\", width = 0.25, alpha=0.8)\n",
    "    ax.bar(X + 0.50, data[4], color = colors['central'], label=\"Centrality Baseline\", width = 0.25)\n",
    "    \n",
    "\n",
    "    plt.xlabel(\"Experiments\")\n",
    "    plt.ylabel(\"Parcels_delivered\")\n",
    "    plt.ylim(bottom=0, top=max_parcels)\n",
    "    \n",
    "    # y axis as percentage\n",
    "    yticks = mtick.PercentFormatter(max_parcels)\n",
    "    ax.yaxis.set_major_formatter(yticks)\n",
    "    \n",
    "    # Add experiment identifiers x-Axis\n",
    "    plt.xticks(X + 0.37, labels)\n",
    "    # Add legend\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5be69355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged:  {'72': {'marl': 1200, 'marl_all': 1, 'optimal': 75, 'central': 1200, 'central_all': 7}, '21': {'marl': 1200, 'marl_all': 0, 'optimal': 105, 'central': 1200, 'central_all': 6}, '44': {'marl': 1200, 'marl_all': 2, 'optimal': 103, 'central': 1200, 'central_all': 2}, '66': {'marl': 1200, 'marl_all': 0, 'optimal': 85, 'central': 1200, 'central_all': 3}, '86': {'marl': 1200, 'marl_all': 2, 'optimal': 90, 'central': 1200, 'central_all': 7}, '14': {'marl': 1200, 'marl_all': 2, 'optimal': 108, 'central': 1200, 'central_all': 3}}\n",
      "Data:  [[1200, 1200, 1200, 1200, 1200, 1200], [1200, 1200, 1200, 1200, 1200, 1200], [75, 105, 103, 85, 90, 108], [1, 0, 2, 0, 2, 2], [7, 6, 2, 3, 7, 3]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot duration bar graphs from the results\n",
    "create_chart_bars(experiment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b3f82cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "\n",
    "def create_chart_episode_events(results_dict, draw_crashes = False):\n",
    "    \"\"\" Function that plots a graph with either the deliveries of parcels or the crashes of agents\n",
    "        over the course of an episode, recorded in the :param results_dict:.\n",
    "        Set the :param draw_crashes: flag for plotting crashes, default are deliveries.\n",
    "    \"\"\"\n",
    "    # TODO Graphen auch abspeichern oder nur hier anzeigen ??\n",
    "    # TODO - Parcel additions ??\n",
    "    # TODO - overthink fill with max_value for mean computation \n",
    "    \n",
    "    # Design choices    \n",
    "    colors = {\n",
    "        \"marl\": 'blue',\n",
    "        \"central\": 'green',\n",
    "        \"optimal\": 'red'\n",
    "    }\n",
    "    alpha = 0.6  # Opacity of individual value lines\n",
    "    alpha_opt = 0.2\n",
    "    opt_marker_size = 15\n",
    "    line_width_mean = 5\n",
    "    line_width_optimal = 2\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # Retrieve settings values from results dict\n",
    "    max_steps = results_dict['max_steps']\n",
    "    max_parcels = results_dict['max_parcels'] \n",
    "    max_agents = results_dict['max_agents']\n",
    "    \n",
    "    _len = max_agents if draw_crashes else max_parcels\n",
    "    _len += 1     # start plot at origin\n",
    "\n",
    "    _key = 'crashed' if draw_crashes else 'delivered'\n",
    "    \n",
    "    # Deep copy for further computations\n",
    "    scenario_results = {k:v for k,v in results_dict.items() if k[0:3] != 'max'}\n",
    "    \n",
    "    # for computation of mean\n",
    "    m_values, c_values, o_values = [], [], []\n",
    "    Y = [str(i) for i in range(0, _len)]\n",
    "\n",
    "    # iterate over configs\n",
    "    for scenario, results in scenario_results.items():\n",
    "\n",
    "        # Default settings -> MARL run\n",
    "        color = colors[\"marl\"]\n",
    "        _type = m_values\n",
    "        label= \"marl_system\"\n",
    "\n",
    "        if scenario[0] == 'C':\n",
    "            # Baseline run\n",
    "            color = colors[\"central\"]\n",
    "            _type = c_values\n",
    "            label = \"centrality_baseline\"\n",
    "            \n",
    "            # Retrieve and plot optimality baseline\n",
    "            optimal_time = results['optimal']\n",
    "            assert optimal_time is not None\n",
    "            if not draw_crashes: ax.plot(optimal_time, 0, \"*\", color = colors[\"optimal\"], label=\"optimality_baseline\", markersize=opt_marker_size, alpha= alpha_opt, clip_on=False)   \n",
    "            o_values.append(optimal_time)\n",
    "            \n",
    "        _num_steps = results['step']\n",
    "        \n",
    "        X = [0] + list(results[_key].values())\n",
    "        \n",
    "                \n",
    "        X = X + [max_steps]*(_len - len(X))           # Fill X up with max_step values for not delivered parcels / not crashed agents\n",
    " \n",
    "        _type.append(X)  # add X to the respective mean list\n",
    "        #Y = [str(i) for i in range(0, len(results[_key].values())+1)]\n",
    "        \n",
    "        #print(\"Data: \", results[_key].values())\n",
    "        #print(\"new X: \", X)\n",
    "        #print(\"new Y: \", Y)\n",
    "    \n",
    "        ax.step(X, Y, label=label, where='post', color=color, alpha=alpha)\n",
    "        \n",
    "        \n",
    "        # Attempt to improve the filling mess in the plot...         \n",
    "        #X = X + [max_steps]*(_len - len(X))           # Fill X up with max_step values for not delivered parcels / not crashed agents\n",
    " \n",
    "        #_type.append(X)  # add X to the respective mean list\n",
    "\n",
    "        \n",
    "    # compute mean values\n",
    "    m_mean = np.mean(np.array(m_values), axis=0)\n",
    "    c_mean = np.mean(np.array(c_values), axis=0)\n",
    "    o_mean = np.mean(np.array(o_values), axis=0)\n",
    "\n",
    "    \n",
    "    ax.step(m_mean, Y, label=\"marl_system\", where='post', color=colors[\"marl\"], linewidth=line_width_mean)\n",
    "    ax.step(c_mean, Y, label=\"centrality_baseline\", where='post', color=colors[\"central\"], linewidth=line_width_mean)\n",
    "\n",
    "    # star for opt:    if not draw_crashes: plt.plot(o_mean, 0, \"*\", label=\"optimality_baseline\", color=\"r\", markersize=opt_marker_size, clip_on=False)\n",
    "    # better?: vertical line for opt\n",
    "    if not draw_crashes: ax.axvline(o_mean, label=\"optimality_baseline\", color=colors[\"optimal\"], linewidth=2, alpha=alpha_opt+0.3)\n",
    "    \n",
    "    # y axis as percentage\n",
    "    max_percent = max_agents if draw_crashes else max_parcels\n",
    "    yticks = mtick.PercentFormatter(max_percent)\n",
    "    ax.yaxis.set_major_formatter(yticks)\n",
    "    \n",
    "    \n",
    "    # Lables and Legend\n",
    "    plt.xlabel(\"steps\")\n",
    "    ylabel = \"% of crashed agents\" if draw_crashes else \"% of delivered parcels\"\n",
    "    plt.ylabel(ylabel)\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys())\n",
    "    \n",
    "    # Margins\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.xlim()\n",
    "    plt.margins(x=0, y=0)\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e9b300cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot delivery graphs from the results\n",
    "create_chart_episode_events(experiment_results, draw_crashes=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd51171e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot crash graphs from the results\n",
    "create_chart_episode_events(experiment_results, draw_crashes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e29c509",
   "metadata": {},
   "source": [
    "### Manual Actions for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efebf2d2",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "##################\n",
    "env_config = {\n",
    "            'DEBUG_LOGS':False,\n",
    "            'TOPOLOGY': topology,\n",
    "            # Simulation config\n",
    "            'NUMBER_STEPS_PER_EPISODE': 1000,\n",
    "            #'NUMBER_OF_TIMESTEPS': NUMBER_OF_TIMESTEPS,\n",
    "            'RANDOM_SEED': None, # 42\n",
    "            # Map\n",
    "            'CHARGING_STATION_NODES': [0,1,2,3,4],\n",
    "            # Entities\n",
    "            'NUMBER_OF_DRONES': 2,\n",
    "            'NUMBER_OF_CARS': 2,\n",
    "            'MAX_BATTERY_POWER': 100,  # TODO split this for drone and car??\n",
    "            'INIT_NUMBER_OF_PARCELS': 3,\n",
    "            'MAX_NUMBER_OF_PARCELS': 3,\n",
    "            'THRESHOLD_ADD_NEW_PARCEL': 0.01,\n",
    "            # Baseline settings\n",
    "            'BASELINE_FLAG': False,  # is set True in the test function when needed\n",
    "            'BASELINE_OPT_CONSTANT': 2.5,\n",
    "            'BASELINE_TIME_CONSTRAINT': 5,\n",
    "            # TODO \n",
    "            #Rewards\n",
    "            'REWARDS': {\n",
    "                'PARCEL_DELIVERED': 200,\n",
    "                'STEP_PENALTY': -0.1,\n",
    "            },  \n",
    "        }\n",
    "        \n",
    "\n",
    "\n",
    "env = Map_Environment(env_config)\n",
    "env.state\n",
    "#env.ACTION_DROPOFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5165d3bd",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO select actions to give agent 0 reward!!\n",
    "#print(env.action_space)\n",
    "\n",
    "\n",
    "actions_1 = {'d_0': 6, 'd_1': 2, 'c_2': 3, 'c_3': 4}\n",
    "#actions_1 = {'d_0': 0, 'd_1': 0, 'c_2': 5, 'c_3': 5}\n",
    "actions_2 = {'d_0': 2, 'd_1': 8, 'c_2': 7, 'c_3':0}\n",
    "actions_3 = {'d_0': 5, 'd_1': 5, 'c_2':2 }\n",
    "\n",
    "new_obs, rewards, dones, infos = env.step(actions_1)\n",
    "print(infos)\n",
    "print(f\"New Obs are: {new_obs}\")\n",
    "print(rewards)\n",
    "print(\"------------------\")\n",
    "new_obs2, rewards2, dones2, infos2 = env.step(actions_2)\n",
    "print(f\"New Obs are: {new_obs2}\")\n",
    "print(rewards2)\n",
    "print(\"------------------\")\n",
    "new_obs3, rewards3, dones3, infos3 = env.step(actions_3)\n",
    "print(f\"New Obs are: {new_obs3}\")\n",
    "print(rewards3)\n",
    "\n",
    "actions_4 = {'d_0': 0, 'd_1': 0, 'c_2': 1, 'c_3': 0}\n",
    "actions_5 = {'d_0': 0, 'd_1': 0, 'c_2': 0, 'c_3': 0}\n",
    "actions_6 = {'d_0': 0, 'd_1': 0, 'c_2': 5, 'c_3': 0}\n",
    "\n",
    "new_obs, rewards, dones, infos = env.step(actions_4)\n",
    "print(f\"New Obs are: {new_obs}\")\n",
    "print(rewards)\n",
    "print(\"------------------\")\n",
    "new_obs2, rewards2, dones2, infos2 = env.step(actions_5)\n",
    "print(f\"New Obs are: {new_obs2}\")\n",
    "print(rewards2)\n",
    "print(\"------------------\")\n",
    "new_obs3, rewards3, dones3, infos3 = env.step(actions_6)\n",
    "print(f\"New Obs are: {new_obs3}\")\n",
    "print(rewards3)\n",
    "\n",
    "print(\"------------------\")\n",
    "print(dones3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a143594",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# TENSORBOARD\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "#Start tensorboard below the notebook\n",
    "#%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
